{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff0b59b",
   "metadata": {},
   "source": [
    "# Tokenization in Natural Language Processing (NLP)\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking down text into smaller units called **tokens**. These tokens can be words, sentences, characters, or subwords. It's one of the fundamental preprocessing steps in NLP.\n",
    "\n",
    "### Why is Tokenization Important?\n",
    "\n",
    "1. **Text Analysis**: Makes text data easier to analyze and process\n",
    "2. **Feature Extraction**: Helps convert text into numerical features for machine learning\n",
    "3. **Pattern Recognition**: Enables identification of patterns and structures in text\n",
    "4. **Language Understanding**: Foundation for tasks like sentiment analysis, translation, etc.\n",
    "\n",
    "### Types of Tokenization:\n",
    "\n",
    "1. **Word Tokenization**: Splits text into individual words\n",
    "2. **Sentence Tokenization**: Splits text into sentences\n",
    "3. **Character Tokenization**: Splits text into individual characters\n",
    "4. **Subword Tokenization**: Splits words into meaningful subunits (advanced)\n",
    "\n",
    "Let's explore each type with practical examples using NLTK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34186f9",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2518ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Setup Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mahes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mahes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install NLTK if not already installed\n",
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"NLTK Setup Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979cd4a",
   "metadata": {},
   "source": [
    "## 2. Word Tokenization\n",
    "\n",
    "Word tokenization breaks text into individual words or tokens. NLTK provides multiple methods for word tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adccfb",
   "metadata": {},
   "source": [
    "### 2.1 Using word_tokenize() - Most Common Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb42fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Natural Language Processing is amazing! It's used in AI, ML, and many other fields.\n",
      "\n",
      "Tokens:\n",
      "['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'It', \"'s\", 'used', 'in', 'AI', ',', 'ML', ',', 'and', 'many', 'other', 'fields', '.']\n",
      "\n",
      "Total number of tokens: 19\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing is amazing! It's used in AI, ML, and many other fields.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)\n",
    "print(f\"\\nTotal number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824a2b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "is\n",
      "amazing\n",
      "!\n",
      "It\n",
      "'s\n",
      "used\n",
      "in\n",
      "AI\n",
      ",\n",
      "ML\n",
      ",\n",
      "and\n",
      "many\n",
      "other\n",
      "fields\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24be70",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "- Punctuation marks are treated as separate tokens\n",
    "- Contractions like \"It's\" are split into \"It\" and \"'s\"\n",
    "- Special characters are preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e620d7",
   "metadata": {},
   "source": [
    "### 2.2 Using WordPunctTokenizer - Alternative Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e50b9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Don't worry! Python's NLTK makes tokenization easy.\n",
      "\n",
      "Tokens using WordPunctTokenizer:\n",
      "Don\n",
      "'\n",
      "t\n",
      "worry\n",
      "!\n",
      "Python\n",
      "'\n",
      "s\n",
      "NLTK\n",
      "makes\n",
      "tokenization\n",
      "easy\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = \"Don't worry! Python's NLTK makes tokenization easy.\"\n",
    "\n",
    "# Using WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nTokens using WordPunctTokenizer:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275c6a0",
   "metadata": {},
   "source": [
    "**Note:** WordPunctTokenizer splits on punctuation but keeps them as separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb76161",
   "metadata": {},
   "source": [
    "### 2.3 Using TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f17bfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: We're learning NLP! Isn't it great?\n",
      "\n",
      "Tokens using TreebankWordTokenizer:\n",
      "We\n",
      "'re\n",
      "learning\n",
      "NLP\n",
      "!\n",
      "Is\n",
      "n't\n",
      "it\n",
      "great\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"We're learning NLP! Isn't it great?\"\n",
    "\n",
    "# Using TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nTokens using TreebankWordTokenizer:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557670ab",
   "metadata": {},
   "source": [
    "Key observations and explanations:\n",
    "- Tokenizer follows Penn Treebank conventions: it separates contractions into the main word and the contraction suffix. Examples:\n",
    "  - \"We're\" → \"We\" and \"'re\" (apostrophe attached to second token)\n",
    "  - \"Isn't\" → \"Is\" and \"n't\"\n",
    "- Punctuation is split into its own tokens: \"!\" and \"?\" are separate from words.\n",
    "- Capitalization is preserved in tokens as-is (e.g., \"We\", \"Is\", \"NLP\").\n",
    "- Abbreviations or acronyms like \"NLP\" are kept as a single token.\n",
    "- This tokenization is helpful for tasks that need explicit contraction parts (e.g., POS tagging, parsing) because the contraction pieces carry grammatical information (e.g., \"'re\" → auxiliary verb).\n",
    "- Downsides for some tasks:\n",
    "  - If you want surface-level text (exact original strings) or downstream frequency counts where contractions should be one token, this split may be undesirable.\n",
    "  - For language modeling or embeddings, you may want to normalize contractions (merge) or expand them (e.g., \"isn't\" → \"is not\") depending on your pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039ecda",
   "metadata": {},
   "source": [
    "### 2.4 Comparison of Different Word Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81af78a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: She said, \"Hello! How's everything going?\"\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. word_tokenize():\n",
      "She\n",
      "said\n",
      ",\n",
      "``\n",
      "Hello\n",
      "!\n",
      "How\n",
      "'s\n",
      "everything\n",
      "going\n",
      "?\n",
      "''\n",
      "\n",
      "2. WordPunctTokenizer:\n",
      "She\n",
      "said\n",
      ",\n",
      "\"\n",
      "Hello\n",
      "!\n",
      "How\n",
      "'\n",
      "s\n",
      "everything\n",
      "going\n",
      "?\"\n",
      "\n",
      "3. TreebankWordTokenizer:\n",
      "She\n",
      "said\n",
      ",\n",
      "``\n",
      "Hello\n",
      "!\n",
      "How\n",
      "'s\n",
      "everything\n",
      "going\n",
      "?\n",
      "''\n"
     ]
    }
   ],
   "source": [
    "text = \"She said, \\\"Hello! How's everything going?\\\"\"\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# word_tokenize\n",
    "print(\"\\n1. word_tokenize():\")\n",
    "for token in word_tokenize(text):\n",
    "    print(token)\n",
    "\n",
    "# WordPunctTokenizer\n",
    "print(\"\\n2. WordPunctTokenizer:\")\n",
    "wpt = WordPunctTokenizer()\n",
    "for token in wpt.tokenize(text):\n",
    "    print(token)\n",
    "\n",
    "# TreebankWordTokenizer\n",
    "print(\"\\n3. TreebankWordTokenizer:\")\n",
    "tbt = TreebankWordTokenizer()\n",
    "for token in tbt.tokenize(text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c54f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: She said, \"Hello! How's everything going?\"\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. word_tokenize():\n",
      "['She', 'said', ',', '``', 'Hello', '!', 'How', \"'s\", 'everything', 'going', '?', \"''\"]\n",
      "\n",
      "2. WordPunctTokenizer:\n",
      "['She', 'said', ',', '\"', 'Hello', '!', 'How', \"'\", 's', 'everything', 'going', '?\"']\n",
      "\n",
      "3. TreebankWordTokenizer:\n",
      "['She', 'said', ',', '``', 'Hello', '!', 'How', \"'s\", 'everything', 'going', '?', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "text = \"She said, \\\"Hello! How's everything going?\\\"\"\n",
    "\n",
    "print(\"Original Text:\", text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# word_tokenize\n",
    "print(\"\\n1. word_tokenize():\")\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# WordPunctTokenizer\n",
    "print(\"\\n2. WordPunctTokenizer:\")\n",
    "wpt = WordPunctTokenizer()\n",
    "print(wpt.tokenize(text))\n",
    "\n",
    "# TreebankWordTokenizer\n",
    "print(\"\\n3. TreebankWordTokenizer:\")\n",
    "tbt = TreebankWordTokenizer()\n",
    "print(tbt.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f468ab",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization\n",
    "\n",
    "Sentence tokenization splits text into individual sentences. This is crucial for many NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a9f9b",
   "metadata": {},
   "source": [
    "### 3.1 Using sent_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc519dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Natural Language Processing is fascinating. It helps computers understand human language. \n",
      "NLP has many applications like chatbots, translation, and sentiment analysis. \n",
      "Dr. Smith says, \"NLP will revolutionize AI.\" What do you think?\n",
      "\n",
      "============================================================\n",
      "\n",
      "Sentences:\n",
      "1. Natural Language Processing is fascinating.\n",
      "2. It helps computers understand human language.\n",
      "3. NLP has many applications like chatbots, translation, and sentiment analysis.\n",
      "4. Dr. Smith says, \"NLP will revolutionize AI.\"\n",
      "5. What do you think?\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"Natural Language Processing is fascinating. It helps computers understand human language. \n",
    "NLP has many applications like chatbots, translation, and sentiment analysis. \n",
    "Dr. Smith says, \"NLP will revolutionize AI.\" What do you think?\"\"\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nSentences:\")\n",
    "\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6defa",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "- Handles abbreviations like \"Dr.\" correctly\n",
    "- Recognizes sentence boundaries with periods, question marks, and exclamation marks\n",
    "- Smart enough to handle quotes and special cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f1842",
   "metadata": {},
   "source": [
    "### 3.2 Handling Complex Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54fb3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Text Sentence Tokenization:\n",
      "============================================================\n",
      "1. Mr. Johnson works at XYZ Corp.\n",
      "2. He earned $1000.50 last week.\n",
      "3. His email is john@example.com.\n",
      "4. He said, \"I love Python 3.9!\"\n",
      "5. The meeting is at 3 p.m. tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Complex text with abbreviations, decimals, etc.\n",
    "text = \"\"\"Mr. Johnson works at XYZ Corp. He earned $1000.50 last week. \n",
    "His email is john@example.com. He said, \"I love Python 3.9!\" \n",
    "The meeting is at 3 p.m. tomorrow.\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Complex Text Sentence Tokenization:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388855e",
   "metadata": {},
   "source": [
    "## 4. Whitespace Tokenization\n",
    "\n",
    "Simple tokenization based on whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb70ed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Python   is    awesome!   Let's     learn   NLP.\n",
      "\n",
      "Tokens using WhitespaceTokenizer:\n",
      "['Python', 'is', 'awesome!', \"Let's\", 'learn', 'NLP.']\n",
      "\n",
      "Number of tokens: 6\n",
      "\n",
      "============================================================\n",
      "Comparison with Python's split():\n",
      "['Python', 'is', 'awesome!', \"Let's\", 'learn', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "text = \"Python   is    awesome!   Let's     learn   NLP.\"\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nTokens using WhitespaceTokenizer:\")\n",
    "print(tokens)\n",
    "print(f\"\\nNumber of tokens: {len(tokens)}\")\n",
    "\n",
    "# Compare with simple split()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison with Python's split():\")\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3680fe9",
   "metadata": {},
   "source": [
    "## 5. Regular Expression Tokenization\n",
    "\n",
    "Using regex patterns for custom tokenization rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "494e41e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello! My email is user123@example.com and phone is 123-456-7890.\n",
      "\n",
      "1. Only words (\\w+):\n",
      "['Hello', 'My', 'email', 'is', 'user123', 'example', 'com', 'and', 'phone', 'is', '123', '456', '7890']\n",
      "\n",
      "2. Words, numbers, and special characters:\n",
      "['Hello', '!', 'My', 'email', 'is', 'user123', '@example.com', 'and', 'phone', 'is', '123', '-456-7890.']\n",
      "\n",
      "3. Extracted Emails: ['user123@example.com']\n",
      "   Extracted Phones: ['123-456-7890']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"Hello! My email is user123@example.com and phone is 123-456-7890.\"\n",
    "\n",
    "# Example 1: Only words (no punctuation)\n",
    "tokenizer1 = RegexpTokenizer(r'\\w+')\n",
    "tokens1 = tokenizer1.tokenize(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\n1. Only words (\\\\w+):\")\n",
    "print(tokens1)\n",
    "\n",
    "# Example 2: Words with hyphens\n",
    "tokenizer2 = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens2 = tokenizer2.tokenize(text)\n",
    "\n",
    "print(\"\\n2. Words, numbers, and special characters:\")\n",
    "print(tokens2)\n",
    "\n",
    "# Example 3: Extract only email and phone patterns\n",
    "import re\n",
    "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "phones = re.findall(r'\\b\\d{3}-\\d{3}-\\d{4}\\b', text)\n",
    "\n",
    "print(\"\\n3. Extracted Emails:\", emails)\n",
    "print(\"   Extracted Phones:\", phones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2ea70",
   "metadata": {},
   "source": [
    "## 6. Tokenizing Multi-line Text and Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "585182c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Tokenization\n",
      "============================================================\n",
      "\n",
      "Total Sentences: 7\n",
      "\n",
      "Total Words: 49\n",
      "\n",
      "Detailed Analysis:\n",
      "Sentence 1 (10 words): Artificial Intelligence (AI) is transforming the world.\n",
      "Words ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'transforming', 'the', 'world', '.'] \n",
      "\n",
      "\n",
      "\n",
      "Sentence 2 (8 words): Machine Learning is a subset of AI.\n",
      "Words ['Machine', 'Learning', 'is', 'a', 'subset', 'of', 'AI', '.'] \n",
      "\n",
      "\n",
      "\n",
      "Sentence 3 (8 words): Deep Learning is a subset of ML.\n",
      "Words ['Deep', 'Learning', 'is', 'a', 'subset', 'of', 'ML', '.'] \n",
      "\n",
      "\n",
      "\n",
      "Sentence 4 (6 words): Key AI Applications:\n",
      "1.\n",
      "Words ['Key', 'AI', 'Applications', ':', '1', '.'] \n",
      "\n",
      "\n",
      "\n",
      "Sentence 5 (5 words): Natural Language Processing\n",
      "2.\n",
      "Words ['Natural', 'Language', 'Processing', '2', '.'] \n",
      "\n",
      "\n",
      "\n",
      "Sentence 6 (4 words): Computer Vision\n",
      "3.\n",
      "Words ['Computer', 'Vision', '3', '.'] \n",
      "\n",
      "\n",
      "\n",
      "Sentence 7 (8 words): Robotics\n",
      "\n",
      "The future of AI looks promising!\n",
      "Words ['Robotics', 'The', 'future', 'of', 'AI', 'looks', 'promising', '!'] \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi-line document\n",
    "document = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming the world.\n",
    "Machine Learning is a subset of AI. Deep Learning is a subset of ML.\n",
    "\n",
    "Key AI Applications:\n",
    "1. Natural Language Processing\n",
    "2. Computer Vision\n",
    "3. Robotics\n",
    "\n",
    "The future of AI looks promising!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Document Tokenization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(document)\n",
    "print(f\"\\nTotal Sentences: {len(sentences)}\\n\")\n",
    "\n",
    "# Word tokenization for the entire document\n",
    "all_words = word_tokenize(document)\n",
    "print(f\"Total Words: {len(all_words)}\\n\")\n",
    "\n",
    "# Sentence and word count combined\n",
    "print(\"Detailed Analysis:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    words = word_tokenize(sentence)\n",
    "    print(f\"Sentence {i} ({len(words)} words): {sentence.strip()}\")\n",
    "    print(f\"Words {words} \\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c291d8",
   "metadata": {},
   "source": [
    "## 7. Handling Special Cases and Edge Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba91e84",
   "metadata": {},
   "source": [
    "### 7.1 URLs, Emails, and Social Media Handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10d8edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Check out https://www.python.org! Contact me at user@email.com or @john_doe #NLP #Python\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. Standard word_tokenize():\n",
      "['Check', 'out', 'https', ':', '//www.python.org', '!', 'Contact', 'me', 'at', 'user', '@', 'email.com', 'or', '@', 'john_doe', '#', 'NLP', '#', 'Python']\n",
      "\n",
      "2. TweetTokenizer (preserves hashtags, mentions, etc.):\n",
      "['Check', 'out', 'https://www.python.org', '!', 'Contact', 'me', 'at', 'user@email.com', 'or', '@john_doe', '#NLP', '#Python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Text with URLs, emails, and mentions\n",
    "text = \"Check out https://www.python.org! Contact me at user@email.com or @john_doe #NLP #Python\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Standard word tokenization\n",
    "print(\"\\n1. Standard word_tokenize():\")\n",
    "tokens1 = word_tokenize(text)\n",
    "print(tokens1)\n",
    "\n",
    "# Tweet Tokenizer (better for social media)\n",
    "print(\"\\n2. TweetTokenizer (preserves hashtags, mentions, etc.):\")\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokens2 = tweet_tokenizer.tokenize(text)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e60c6",
   "metadata": {},
   "source": [
    "### 7.2 Numbers, Dates, and Currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5976d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The price is $1,234.56. Meeting on 2024-01-15 at 3:30 PM. Call 555-123-4567.\n",
      "\n",
      "Tokens:\n",
      "['The', 'price', 'is', '$', '1,234.56', '.', 'Meeting', 'on', '2024-01-15', 'at', '3:30', 'PM', '.', 'Call', '555-123-4567', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"The price is $1,234.56. Meeting on 2024-01-15 at 3:30 PM. Call 555-123-4567.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa560182",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Currency symbols and amounts are split\n",
    "- Dates with hyphens are preserved\n",
    "- Phone numbers maintain their structure\n",
    "- Commas in numbers are separated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa31b8a",
   "metadata": {},
   "source": [
    "### 7.3 Contractions and Possessives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07702304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I'm learning NLP. It's John's book. They've, we'll, and won't study together.\n",
      "\n",
      "Tokens:\n",
      "['I', \"'m\", 'learning', 'NLP', '.', 'It', \"'s\", 'John', \"'s\", 'book', '.', 'They', \"'ve\", ',', 'we', \"'ll\", ',', 'and', 'wo', \"n't\", 'study', 'together', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm learning NLP. It's John's book. They've, we'll, and won't study together.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42e96c",
   "metadata": {},
   "source": [
    "How contractions are handled:\n",
    "\n",
    "  I'm → ['I', \"'m'\"]\n",
    "  \n",
    "  It's → ['It', \"'s'\"]\n",
    "  \n",
    "  John's → ['John', \"'s'\"]\n",
    "  \n",
    "  They've → ['They', \"'ve'\"]\n",
    "  \n",
    "  we'll → ['we', \"'ll'\"]\n",
    "  \n",
    "  won't → ['wo', \"n't'\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef61e6e",
   "metadata": {},
   "source": [
    "## 8. Practical Applications and Real-World Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7093d06",
   "metadata": {},
   "source": [
    "### 8.1 Building a Simple Text Statistics Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fd9f467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT ANALYSIS RESULTS\n",
      "============================================================\n",
      "Total Characters: 311\n",
      "Total Sentences: 5\n",
      "Total Tokens: 57\n",
      "Total Words: 48\n",
      "Unique Words: 37\n",
      "Avg Sentence Length: 11.40\n",
      "Avg Word Length: 5.25\n",
      "\n",
      "Top 5 Most Common Words:\n",
      "  'python': 4 times\n",
      "  'is': 4 times\n",
      "  'and': 4 times\n",
      "  'it': 3 times\n",
      "  'an': 1 times\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_text(text):\n",
    "    \"\"\"Analyze text and return statistics\"\"\"\n",
    "    \n",
    "    # Tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Filter only alphabetic words\n",
    "    words_only = [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'total_characters': len(text),\n",
    "        'total_sentences': len(sentences),\n",
    "        'total_tokens': len(words),\n",
    "        'total_words': len(words_only),\n",
    "        'unique_words': len(set(words_only)),\n",
    "        'avg_sentence_length': len(words) / len(sentences) if sentences else 0,\n",
    "        'avg_word_length': sum(len(word) for word in words_only) / len(words_only) if words_only else 0\n",
    "    }\n",
    "    \n",
    "    # Most common words\n",
    "    word_freq = Counter(words_only)\n",
    "    stats['top_5_words'] = word_freq.most_common(5)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"\n",
    "Python is an amazing programming language. It is versatile and powerful.\n",
    "Python is used in web development, data science, artificial intelligence, and more.\n",
    "Many developers love Python because it is easy to learn and has a great community.\n",
    "Python's simplicity makes it perfect for beginners and experts alike.\n",
    "\"\"\"\n",
    "\n",
    "stats = analyze_text(text)\n",
    "\n",
    "print(\"TEXT ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for key, value in stats.items():\n",
    "    if key != 'top_5_words':\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:.2f}\" if isinstance(value, float) else f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "\n",
    "print(\"\\nTop 5 Most Common Words:\")\n",
    "for word, count in stats['top_5_words']:\n",
    "    print(f\"  '{word}': {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2faed0",
   "metadata": {},
   "source": [
    "### 8.2 Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3177cf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Frequent Words:\n",
      "============================================================\n",
      "[('learning', 7), ('machine', 4), ('deep', 3), ('is', 2), ('a', 2), ('subset', 2), ('of', 2), ('are', 2), ('artificial', 1), ('intelligence', 1)]\n",
      "learning        | ███████ (7)\n",
      "machine         | ████ (4)\n",
      "deep            | ███ (3)\n",
      "is              | ██ (2)\n",
      "a               | ██ (2)\n",
      "subset          | ██ (2)\n",
      "of              | ██ (2)\n",
      "are             | ██ (2)\n",
      "artificial      | █ (1)\n",
      "intelligence    | █ (1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence. Machine learning \n",
    "algorithms learn from data. Deep learning is a subset of machine learning.\n",
    "Neural networks are used in deep learning. Machine learning and deep learning \n",
    "are revolutionizing technology.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and clean\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Get top 10 words\n",
    "top_words = word_freq.most_common(10)\n",
    "\n",
    "print(\"Top 10 Most Frequent Words:\")\n",
    "print(\"=\"*60)\n",
    "print(top_words)\n",
    "\n",
    "for word, count in top_words:\n",
    "    print(f\"{word:15} | {'█' * count} ({count})\")\n",
    "\n",
    "# Visualize\n",
    "word_list, counts = zip(*top_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(word_list, counts, color='steelblue')\n",
    "plt.xlabel('Words', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Top 10 Most Frequent Words', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c3c0a",
   "metadata": {},
   "source": [
    "### 8.3 N-gram Generation (Bigrams and Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5055955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGRAMS (2-word combinations):\n",
      "============================================================\n",
      "('natural', 'language')\n",
      "('language', 'processing')\n",
      "('processing', 'with')\n",
      "('with', 'python')\n",
      "('python', 'is')\n",
      "('is', 'fun')\n",
      "('fun', '.')\n",
      "('.', 'python')\n",
      "('python', 'makes')\n",
      "('makes', 'nlp')\n",
      "\n",
      "============================================================\n",
      "\n",
      "TRIGRAMS (3-word combinations):\n",
      "============================================================\n",
      "('natural', 'language', 'processing')\n",
      "('language', 'processing', 'with')\n",
      "('processing', 'with', 'python')\n",
      "('with', 'python', 'is')\n",
      "('python', 'is', 'fun')\n",
      "('is', 'fun', '.')\n",
      "('fun', '.', 'python')\n",
      "('.', 'python', 'makes')\n",
      "('python', 'makes', 'nlp')\n",
      "('makes', 'nlp', 'easy')\n",
      "\n",
      "============================================================\n",
      "\n",
      "Most Common Bigrams:\n",
      "  ('natural', 'language'): 1\n",
      "  ('language', 'processing'): 1\n",
      "  ('processing', 'with'): 1\n",
      "  ('with', 'python'): 1\n",
      "  ('python', 'is'): 1\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"Natural language processing with Python is fun. Python makes NLP easy and accessible.\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text.lower())\n",
    "\n",
    "# Generate bigrams (2-word combinations)\n",
    "bigram_list = list(bigrams(words))\n",
    "print(\"BIGRAMS (2-word combinations):\")\n",
    "print(\"=\"*60)\n",
    "for bg in bigram_list[:10]:\n",
    "    print(bg)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Generate trigrams (3-word combinations)\n",
    "trigram_list = list(trigrams(words))\n",
    "print(\"\\nTRIGRAMS (3-word combinations):\")\n",
    "print(\"=\"*60)\n",
    "for tg in trigram_list[:10]:\n",
    "    print(tg)\n",
    "\n",
    "# Most common bigrams\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "bigram_freq = Counter(bigram_list)\n",
    "print(\"\\nMost Common Bigrams:\")\n",
    "for bg, count in bigram_freq.most_common(5):\n",
    "    print(f\"  {bg}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70c689",
   "metadata": {},
   "source": [
    "## 9. Advanced Tokenization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1aac49",
   "metadata": {},
   "source": [
    "### 9.1 Tokenization with POS (Part-of-Speech) Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71f98492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mahes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\mahes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZATION WITH POS TAGS\n",
      "============================================================\n",
      "Token                POS Tag    Description                   \n",
      "------------------------------------------------------------\n",
      "Python               NNP        Other                         \n",
      "is                   VBZ        Verb, 3rd person singular     \n",
      "a                    DT         Determiner                    \n",
      "powerful             JJ         Adjective                     \n",
      "programming          JJ         Adjective                     \n",
      "language             NN         Noun, singular                \n",
      ".                    .          Punctuation                   \n",
      "Developers           NNP        Other                         \n",
      "love                 VBP        Other                         \n",
      "using                VBG        Verb, gerund                  \n",
      "Python               NNP        Other                         \n",
      ".                    .          Punctuation                   \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "text = \"Python is a powerful programming language. Developers love using Python.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(\"TOKENIZATION WITH POS TAGS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Token':<20} {'POS Tag':<10} {'Description':<30}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Common POS tag meanings\n",
    "pos_meanings = {\n",
    "    'NN': 'Noun, singular',\n",
    "    'NNS': 'Noun, plural',\n",
    "    'VB': 'Verb, base form',\n",
    "    'VBZ': 'Verb, 3rd person singular',\n",
    "    'VBG': 'Verb, gerund',\n",
    "    'JJ': 'Adjective',\n",
    "    'DT': 'Determiner',\n",
    "    'IN': 'Preposition',\n",
    "    'CC': 'Coordinating conjunction',\n",
    "    '.': 'Punctuation'\n",
    "}\n",
    "\n",
    "for token, tag in pos_tags:\n",
    "    description = pos_meanings.get(tag, 'Other')\n",
    "    print(f\"{token:<20} {tag:<10} {description:<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d8710",
   "metadata": {},
   "source": [
    "### 9.2 Custom Tokenizer Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bd810a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM TOKENIZATION WITH TYPE CLASSIFICATION\n",
      "============================================================\n",
      "Token                          Type           \n",
      "------------------------------------------------------------\n",
      "Contact                        WORD           \n",
      "us                             WORD           \n",
      "at                             WORD           \n",
      "support@company.com            EMAIL          \n",
      "or                             WORD           \n",
      "visit                          WORD           \n",
      "https://example.com.           URL            \n",
      "Price                          WORD           \n",
      "99.99                          NUMBER         \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class CustomTokenizer:\n",
    "    \"\"\"Custom tokenizer with specific rules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Preserve emails, URLs, numbers, and words\n",
    "        self.pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b|http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|\\d+\\.?\\d*|\\w+'\n",
    "        self.tokenizer = RegexpTokenizer(self.pattern)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "    \n",
    "    def tokenize_with_types(self, text):\n",
    "        \"\"\"Tokenize and classify token types\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        classified = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if re.match(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', token):\n",
    "                classified.append((token, 'EMAIL'))\n",
    "            elif re.match(r'http[s]?://', token):\n",
    "                classified.append((token, 'URL'))\n",
    "            elif re.match(r'^\\d+\\.?\\d*$', token):\n",
    "                classified.append((token, 'NUMBER'))\n",
    "            elif re.match(r'^\\w+$', token):\n",
    "                classified.append((token, 'WORD'))\n",
    "            else:\n",
    "                classified.append((token, 'OTHER'))\n",
    "        \n",
    "        return classified\n",
    "\n",
    "# Test the custom tokenizer\n",
    "text = \"Contact us at support@company.com or visit https://example.com. Price: $99.99\"\n",
    "\n",
    "tokenizer = CustomTokenizer()\n",
    "tokens = tokenizer.tokenize_with_types(text)\n",
    "\n",
    "print(\"CUSTOM TOKENIZATION WITH TYPE CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Token':<30} {'Type':<15}\")\n",
    "print(\"-\"*60)\n",
    "for token, token_type in tokens:\n",
    "    print(f\"{token:<30} {token_type:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c07aa",
   "metadata": {},
   "source": [
    "## 10. Performance Comparison of Different Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51d4509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE COMPARISON OF TOKENIZERS\n",
      "============================================================\n",
      "Text size: 61000 characters\n",
      "Tokenizer                 Time (seconds)  Tokens    \n",
      "------------------------------------------------------------\n",
      "word_tokenize             0.167330        10000     \n",
      "WordPunctTokenizer        0.005962        10000     \n",
      "TreebankWordTokenizer     0.039927        9001      \n",
      "WhitespaceTokenizer       0.005047        9000      \n",
      "Python split()            0.000998        9000      \n",
      "\n",
      "============================================================\n",
      "Fastest: Python split() (0.000998 seconds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, TreebankWordTokenizer, WhitespaceTokenizer\n",
    "\n",
    "# Generate a larger text for performance testing\n",
    "large_text = \"\"\"Natural Language Processing is a fascinating field of study. \"\"\" * 1000\n",
    "\n",
    "tokenizers = {\n",
    "    'word_tokenize': lambda text: word_tokenize(text),\n",
    "    'WordPunctTokenizer': lambda text: WordPunctTokenizer().tokenize(text),\n",
    "    'TreebankWordTokenizer': lambda text: TreebankWordTokenizer().tokenize(text),\n",
    "    'WhitespaceTokenizer': lambda text: WhitespaceTokenizer().tokenize(text),\n",
    "    'Python split()': lambda text: text.split()\n",
    "}\n",
    "\n",
    "print(\"PERFORMANCE COMPARISON OF TOKENIZERS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Text size: {len(large_text)} characters\")\n",
    "print(f\"{'Tokenizer':<25} {'Time (seconds)':<15} {'Tokens':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "results = {}\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    start_time = time.time()\n",
    "    tokens = tokenizer(large_text)\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    results[name] = (elapsed, len(tokens))\n",
    "    print(f\"{name:<25} {elapsed:<15.6f} {len(tokens):<10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "fastest = min(results.items(), key=lambda x: x[1][0])\n",
    "print(f\"Fastest: {fastest[0]} ({fastest[1][0]:.6f} seconds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb2e0b",
   "metadata": {},
   "source": [
    "## 11. Common Pitfalls and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64c8ea",
   "metadata": {},
   "source": [
    "### Common Pitfalls to Avoid:\n",
    "\n",
    "1. **Not handling edge cases**: URLs, emails, special characters\n",
    "2. **Ignoring case sensitivity**: \"Python\" vs \"python\" treated as different\n",
    "3. **Not removing stopwords**: Common words that don't add meaning\n",
    "4. **Over-tokenization**: Breaking words too much\n",
    "5. **Under-tokenization**: Not breaking enough\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "✅ **Choose the right tokenizer** for your task\n",
    "✅ **Normalize text** before tokenization (lowercase, remove special chars)\n",
    "✅ **Handle contractions** appropriately\n",
    "✅ **Consider domain-specific needs** (social media, medical, legal text)\n",
    "✅ **Test on sample data** before processing large datasets\n",
    "✅ **Document your preprocessing** steps for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb241910",
   "metadata": {},
   "source": [
    "### Practical Example: Complete Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32d59e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Hello! I'm learning NLP in 2024. It's AMAZING!!! Contact: test@email.com\n",
      "\n",
      "============================================================\n",
      "\n",
      "1. Basic tokenization (no preprocessing):\n",
      "['Hello', '!', 'I', \"'m\", 'learning', 'NLP', 'in', '2024', '.', 'It', \"'s\", 'AMAZING', '!', '!', '!', 'Contact', ':', 'test', '@', 'email.com']\n",
      "\n",
      "2. Lowercase + Remove punctuation:\n",
      "['hello', 'i', \"'m\", 'learning', 'nlp', 'in', '2024', 'it', \"'s\", 'amazing', 'contact', 'test', 'email.com']\n",
      "\n",
      "3. Lowercase + Remove punctuation + Remove numbers:\n",
      "['hello', 'i', \"'m\", 'learning', 'nlp', 'in', 'it', \"'s\", 'amazing', 'contact', 'test', 'email.com']\n",
      "\n",
      "4. Only lowercase (keep punctuation):\n",
      "['hello', '!', 'i', \"'m\", 'learning', 'nlp', 'in', '2024', '.', 'it', \"'s\", 'amazing', '!', '!', '!', 'contact', ':', 'test', '@', 'email.com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def preprocess_text(text, lowercase=True, remove_punct=True, remove_numbers=False):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to preprocess\n",
    "    lowercase : bool\n",
    "        Convert to lowercase\n",
    "    remove_punct : bool\n",
    "        Remove punctuation\n",
    "    remove_numbers : bool\n",
    "        Remove numeric tokens\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : preprocessed tokens\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    if lowercase:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punct:\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove numbers\n",
    "    if remove_numbers:\n",
    "        tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello! I'm learning NLP in 2024. It's AMAZING!!! Contact: test@email.com\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Different preprocessing options\n",
    "print(\"1. Basic tokenization (no preprocessing):\")\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print(\"\\n2. Lowercase + Remove punctuation:\")\n",
    "print(preprocess_text(text, lowercase=True, remove_punct=True, remove_numbers=False))\n",
    "\n",
    "print(\"\\n3. Lowercase + Remove punctuation + Remove numbers:\")\n",
    "print(preprocess_text(text, lowercase=True, remove_punct=True, remove_numbers=True))\n",
    "\n",
    "print(\"\\n4. Only lowercase (keep punctuation):\")\n",
    "print(preprocess_text(text, lowercase=True, remove_punct=False, remove_numbers=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c94fd",
   "metadata": {},
   "source": [
    "## 12. Real-World Use Case: Analyzing a Book Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b04ab337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK REVIEW ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "This book is absolutely fantastic! The author has done an amazing job. \n",
      "The story is captivating and keeps you engaged throughout. I couldn't put it down.\n",
      "The characters are well-developed and the plot is intricate. \n",
      "However, some parts felt a bit slow. Overall, I highly recommend this book.\n",
      "Rating: 4.5/5 stars. A must-read for fiction lovers!\n",
      "\n",
      "======================================================================\n",
      "\n",
      "📊 Total Sentences: 9\n",
      "\n",
      "📝 Total Words: 53\n",
      "🔤 Unique Words: 43\n",
      "\n",
      "🔥 Top 10 Most Common Words:\n",
      "   • the: 4\n",
      "   • is: 3\n",
      "   • this: 2\n",
      "   • book: 2\n",
      "   • and: 2\n",
      "   • i: 2\n",
      "   • a: 2\n",
      "   • absolutely: 1\n",
      "   • fantastic: 1\n",
      "   • author: 1\n",
      "\n",
      "😊 Positive words found: ['fantastic', 'amazing', 'captivating', 'engaged', 'recommend']\n",
      "😞 Negative words found: ['slow']\n",
      "\n",
      "📈 Sentiment Score: 4 (Positive: 5, Negative: 1)\n",
      "   ✅ Overall Sentiment: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Sample book review\n",
    "review = \"\"\"\n",
    "This book is absolutely fantastic! The author has done an amazing job. \n",
    "The story is captivating and keeps you engaged throughout. I couldn't put it down.\n",
    "The characters are well-developed and the plot is intricate. \n",
    "However, some parts felt a bit slow. Overall, I highly recommend this book.\n",
    "Rating: 4.5/5 stars. A must-read for fiction lovers!\n",
    "\"\"\"\n",
    "\n",
    "print(\"BOOK REVIEW ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(review)\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Sentence Analysis\n",
    "sentences = sent_tokenize(review)\n",
    "print(f\"📊 Total Sentences: {len(sentences)}\\n\")\n",
    "\n",
    "# 2. Word Analysis\n",
    "words = word_tokenize(review.lower())\n",
    "words_clean = [w for w in words if w.isalpha()]  # Only alphabetic words\n",
    "\n",
    "print(f\"📝 Total Words: {len(words_clean)}\")\n",
    "print(f\"🔤 Unique Words: {len(set(words_clean))}\\n\")\n",
    "\n",
    "# 3. Most Common Words\n",
    "word_freq = Counter(words_clean)\n",
    "print(\"🔥 Top 10 Most Common Words:\")\n",
    "for word, count in word_freq.most_common(10):\n",
    "    print(f\"   • {word}: {count}\")\n",
    "\n",
    "# 4. Sentiment Words Detection\n",
    "positive_words = ['fantastic', 'amazing', 'captivating', 'engaged', 'well-developed', \n",
    "                  'recommend', 'must-read', 'good', 'great', 'excellent']\n",
    "negative_words = ['slow', 'boring', 'bad', 'terrible', 'poor', 'disappointing']\n",
    "\n",
    "found_positive = [w for w in words_clean if w in positive_words]\n",
    "found_negative = [w for w in words_clean if w in negative_words]\n",
    "\n",
    "print(f\"\\n😊 Positive words found: {found_positive}\")\n",
    "print(f\"😞 Negative words found: {found_negative}\")\n",
    "\n",
    "# 5. Sentiment Score\n",
    "sentiment_score = len(found_positive) - len(found_negative)\n",
    "print(f\"\\n📈 Sentiment Score: {sentiment_score} (Positive: {len(found_positive)}, Negative: {len(found_negative)})\")\n",
    "\n",
    "if sentiment_score > 0:\n",
    "    print(\"   ✅ Overall Sentiment: POSITIVE\")\n",
    "elif sentiment_score < 0:\n",
    "    print(\"   ❌ Overall Sentiment: NEGATIVE\")\n",
    "else:\n",
    "    print(\"   ➖ Overall Sentiment: NEUTRAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7169117",
   "metadata": {},
   "source": [
    "## 13. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "#### 1️⃣ **Types of Tokenization**\n",
    "   - **Word Tokenization**: Breaking text into words\n",
    "   - **Sentence Tokenization**: Breaking text into sentences\n",
    "   - **Whitespace Tokenization**: Simple splitting by spaces\n",
    "   - **Regex Tokenization**: Custom patterns for specific needs\n",
    "\n",
    "#### 2️⃣ **NLTK Tokenizers Covered**\n",
    "   - `word_tokenize()` - Most commonly used\n",
    "   - `sent_tokenize()` - For sentence splitting\n",
    "   - `WordPunctTokenizer()` - Splits on punctuation\n",
    "   - `TreebankWordTokenizer()` - Penn Treebank standard\n",
    "   - `WhitespaceTokenizer()` - Splits on whitespace\n",
    "   - `RegexpTokenizer()` - Custom regex patterns\n",
    "   - `TweetTokenizer()` - For social media text\n",
    "\n",
    "#### 3️⃣ **Practical Applications**\n",
    "   - Text statistics and analysis\n",
    "   - Word frequency counting\n",
    "   - N-gram generation (bigrams, trigrams)\n",
    "   - Sentiment analysis\n",
    "   - Document preprocessing\n",
    "\n",
    "#### 4️⃣ **Best Practices**\n",
    "   - Choose tokenizer based on your specific use case\n",
    "   - Handle edge cases (URLs, emails, contractions)\n",
    "   - Consider performance for large datasets\n",
    "   - Combine with other preprocessing steps (lowercasing, removing stopwords)\n",
    "   - Test with sample data before full processing\n",
    "\n",
    "#### 5️⃣ **When to Use Which Tokenizer**\n",
    "\n",
    "| Use Case | Recommended Tokenizer |\n",
    "|----------|----------------------|\n",
    "| General NLP tasks | `word_tokenize()` |\n",
    "| Sentence splitting | `sent_tokenize()` |\n",
    "| Social media analysis | `TweetTokenizer()` |\n",
    "| Custom patterns | `RegexpTokenizer()` |\n",
    "| Simple/fast processing | `WhitespaceTokenizer()` |\n",
    "| Linguistic research | `TreebankWordTokenizer()` |\n",
    "\n",
    "### Next Steps in NLP:\n",
    "1. **Stopword Removal** - Filtering common words\n",
    "2. **Stemming & Lemmatization** - Word normalization\n",
    "3. **POS Tagging** - Part-of-speech identification\n",
    "4. **Named Entity Recognition (NER)** - Identifying entities\n",
    "5. **Text Vectorization** - Converting text to numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a64eb",
   "metadata": {},
   "source": [
    "## 14. Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c95a3c",
   "metadata": {},
   "source": [
    "### Exercise 1: Analyze Your Own Text\n",
    "Write a function that takes any text and returns:\n",
    "- Number of sentences\n",
    "- Number of words\n",
    "- Average words per sentence\n",
    "- Top 5 longest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebb0a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def analyze_my_text(text):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test with your own text\n",
    "my_text = \"Write your text here...\"\n",
    "# analyze_my_text(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9f563",
   "metadata": {},
   "source": [
    "### Exercise 2: Email Extractor\n",
    "Create a tokenizer that extracts all email addresses from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "092ba8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def extract_emails(text):\n",
    "    # TODO: Implement email extraction\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_text = \"Contact us at support@company.com or sales@company.com for more info.\"\n",
    "# print(extract_emails(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f897f2",
   "metadata": {},
   "source": [
    "### Exercise 3: Hashtag Counter\n",
    "Write a function to count all hashtags in a social media post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05cfa1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def count_hashtags(text):\n",
    "    # TODO: Implement hashtag counter\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "tweet = \"Learning #NLP is fun! #Python #MachineLearning #AI\"\n",
    "# print(count_hashtags(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eae2ce",
   "metadata": {},
   "source": [
    "## 📚 Additional Resources\n",
    "\n",
    "- **NLTK Documentation**: https://www.nltk.org/\n",
    "- **NLTK Book**: https://www.nltk.org/book/\n",
    "- **Tokenization Guide**: https://www.nltk.org/api/nltk.tokenize.html\n",
    "- **RegEx Tutorial**: https://docs.python.org/3/library/re.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
