{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11865786",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# One Hot Encoding for Text to Vector Representation\n",
    "\n",
    "## üìö Complete Guide with Practical Implementations\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to Text Vectorization\n",
    "2. What is One Hot Encoding?\n",
    "3. Mathematical Foundation\n",
    "4. Advantages and Disadvantages\n",
    "5. Practical Implementations\n",
    "6. Real-World Examples\n",
    "7. Comparison with Other Techniques\n",
    "8. Best Practices and Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24fe65",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to Text Vectorization\n",
    "\n",
    "### Why Do We Need Text Vectorization?\n",
    "\n",
    "Machine learning algorithms work with **numerical data**, not text. Text vectorization is the process of converting text data into numerical vectors that machines can understand and process.\n",
    "\n",
    "### The Challenge\n",
    "- Computers understand numbers (0s and 1s)\n",
    "- Humans communicate using words and sentences\n",
    "- **Solution:** Convert text ‚Üí numerical representation\n",
    "\n",
    "### Common Text Vectorization Techniques:\n",
    "1. **One Hot Encoding** (What we'll learn in this notebook)\n",
    "2. Bag of Words (BoW)\n",
    "3. TF-IDF\n",
    "4. Word Embeddings (Word2Vec, GloVe)\n",
    "5. Contextual Embeddings (BERT, GPT)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f8a1",
   "metadata": {},
   "source": [
    "## 2. What is One Hot Encoding?\n",
    "\n",
    "### Definition\n",
    "**One Hot Encoding** is a technique where each unique word in a vocabulary is represented as a binary vector. Each word gets a unique position in the vector, and that position is marked as 1 while all other positions are 0.\n",
    "\n",
    "### Simple Example\n",
    "Suppose we have a vocabulary: `[\"cat\", \"dog\", \"bird\"]`\n",
    "\n",
    "- **cat**  ‚Üí `[1, 0, 0]`\n",
    "- **dog**  ‚Üí `[0, 1, 0]`\n",
    "- **bird** ‚Üí `[0, 0, 1]`\n",
    "\n",
    "### Key Characteristics:\n",
    "‚úÖ Each word has exactly ONE position set to 1  \n",
    "‚úÖ All other positions are 0  \n",
    "‚úÖ Vector length = Vocabulary size  \n",
    "‚úÖ Vectors are sparse (mostly zeros)  \n",
    "‚úÖ Words are treated as independent (no semantic relationship captured)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac75936",
   "metadata": {},
   "source": [
    "## 3. Mathematical Foundation\n",
    "\n",
    "### Vector Representation\n",
    "For a vocabulary of size **V**, each word **w** is represented as a vector of dimension **V**:\n",
    "\n",
    "$$\\vec{w_i} = [0, 0, ..., 1, ..., 0, 0] \\in \\mathbb{R}^V$$\n",
    "\n",
    "Where the i-th position is 1, and all others are 0.\n",
    "\n",
    "### Properties\n",
    "\n",
    "1. **Orthogonality:** All word vectors are perpendicular to each other\n",
    "   \n",
    "   $$\\vec{w_i} \\cdot \\vec{w_j} = 0 \\quad \\text{for } i \\neq j$$\n",
    "\n",
    "2. **Euclidean Distance:** Distance between any two different words is constant\n",
    "   \n",
    "   $$\\|\\vec{w_i} - \\vec{w_j}\\| = \\sqrt{2} \\quad \\text{for } i \\neq j$$\n",
    "\n",
    "3. **Cosine Similarity:** Zero similarity between different words\n",
    "   \n",
    "   $$\\cos(\\theta) = \\frac{\\vec{w_i} \\cdot \\vec{w_j}}{\\|\\vec{w_i}\\| \\|\\vec{w_j}\\|} = 0 \\quad \\text{for } i \\neq j$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997b092",
   "metadata": {},
   "source": [
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "1. **Simple and Intuitive:** Easy to understand and implement\n",
    "2. **No Assumptions:** Treats all words as equally different\n",
    "3. **No Training Required:** Direct mapping from words to vectors\n",
    "4. **Deterministic:** Same word always gets the same vector\n",
    "5. **Works Well for Small Vocabularies:** Efficient for limited word sets\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "1. **High Dimensionality:** Vector size = Vocabulary size (can be huge!)\n",
    "2. **Sparse Vectors:** Mostly zeros (memory inefficient)\n",
    "3. **No Semantic Meaning:** \"King\" and \"Queen\" are as different as \"King\" and \"Apple\"\n",
    "4. **No Word Relationships:** Cannot capture similarity or relationships\n",
    "5. **Vocabulary Limited:** New words cannot be represented\n",
    "6. **Memory Intensive:** Large vocabularies create enormous vectors\n",
    "\n",
    "### When to Use One Hot Encoding?\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Small, fixed vocabulary\n",
    "- Categorical data with few unique values\n",
    "- Neural network input layers\n",
    "- Classification tasks with limited classes\n",
    "\n",
    "‚ùå **Not good for:**\n",
    "- Large vocabularies (millions of words)\n",
    "- When semantic meaning matters\n",
    "- Tasks requiring word similarity\n",
    "- Memory-constrained environments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a4e3b",
   "metadata": {},
   "source": [
    "---\n",
    "# üî¨ Practical Implementations\n",
    "\n",
    "## 5. Let's Get Started with Code!\n",
    "\n",
    "We'll implement One Hot Encoding in multiple ways:\n",
    "1. **Manual Implementation** (Understanding the concept)\n",
    "2. **Using NLTK**\n",
    "3. **Using Scikit-learn**\n",
    "4. **Using Keras**\n",
    "5. **Real-world examples**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75015c5",
   "metadata": {},
   "source": [
    "## Experiment 1: Manual Implementation of One Hot Encoding\n",
    "\n",
    "**Objective:** Understand how One Hot Encoding works by building it from scratch.\n",
    "\n",
    "**What we'll do:**\n",
    "- Create a simple vocabulary\n",
    "- Build a word-to-index mapping\n",
    "- Convert words to one-hot vectors manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4e7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I love natural language processing\n",
      "Tokenized Words: ['i', 'love', 'natural', 'language', 'processing']\n",
      "Number of words: 5\n",
      "\n",
      "==================================================\n",
      "Vocabulary (Unique Words): ['i', 'language', 'love', 'natural', 'processing']\n",
      "Vocabulary Size: 5\n",
      "\n",
      "==================================================\n",
      "Word to Index Mapping:\n",
      "  'i' ‚Üí Index 0\n",
      "  'language' ‚Üí Index 1\n",
      "  'love' ‚Üí Index 2\n",
      "  'natural' ‚Üí Index 3\n",
      "  'processing' ‚Üí Index 4\n",
      "\n",
      "==================================================\n",
      "One-Hot Encoded Vectors:\n",
      "==================================================\n",
      "\n",
      "'i' ‚Üí [1. 0. 0. 0. 0.]\n",
      "  Shape: (5,), Non-zero index: 0\n",
      "\n",
      "'love' ‚Üí [0. 0. 1. 0. 0.]\n",
      "  Shape: (5,), Non-zero index: 2\n",
      "\n",
      "'natural' ‚Üí [0. 0. 0. 1. 0.]\n",
      "  Shape: (5,), Non-zero index: 3\n",
      "\n",
      "'language' ‚Üí [0. 1. 0. 0. 0.]\n",
      "  Shape: (5,), Non-zero index: 1\n",
      "\n",
      "'processing' ‚Üí [0. 0. 0. 0. 1.]\n",
      "  Shape: (5,), Non-zero index: 4\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Manual One Hot Encoding Implementation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"I love natural language processing\"\n",
    "\n",
    "# Step 1: Tokenize the sentence (split into words)\n",
    "words = sentence.lower().split()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Tokenized Words:\", words)\n",
    "print(\"Number of words:\", len(words))\n",
    "\n",
    "# Step 2: Create vocabulary (unique words)\n",
    "vocabulary = sorted(set(words))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Vocabulary (Unique Words):\", vocabulary)\n",
    "print(\"Vocabulary Size:\", len(vocabulary))\n",
    "\n",
    "# Step 3: Create word to index mapping\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Word to Index Mapping:\")\n",
    "for word, idx in word_to_index.items():\n",
    "    print(f\"  '{word}' ‚Üí Index {idx}\")\n",
    "\n",
    "# Step 4: Function to create one-hot vector\n",
    "def create_one_hot_vector(word, vocabulary_size, word_to_idx):\n",
    "    \"\"\"Create a one-hot encoded vector for a given word\"\"\"\n",
    "    vector = np.zeros(vocabulary_size)\n",
    "    if word in word_to_idx:\n",
    "        vector[word_to_idx[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Step 5: Create one-hot vectors for each word\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"One-Hot Encoded Vectors:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for word in words:\n",
    "    vector = create_one_hot_vector(word, len(vocabulary), word_to_index)\n",
    "    print(f\"\\n'{word}' ‚Üí {vector}\")\n",
    "    print(f\"  Shape: {vector.shape}, Non-zero index: {np.argmax(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d62d1",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 1\n",
    "\n",
    "**What we learned:**\n",
    "1. ‚úÖ The vocabulary contains 5 unique words\n",
    "2. ‚úÖ Each word is mapped to a unique index (0-4)\n",
    "3. ‚úÖ Each one-hot vector has length 5 (vocabulary size)\n",
    "4. ‚úÖ Each vector has exactly one '1' and four '0's\n",
    "5. ‚úÖ The position of '1' corresponds to the word's index in vocabulary\n",
    "\n",
    "**Key Insights:**\n",
    "- Vector dimensionality = Vocabulary size\n",
    "- Sparse representation (80% zeros in this case)\n",
    "- Order in vocabulary affects vector representation\n",
    "- New words not in vocabulary cannot be encoded\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88304b",
   "metadata": {},
   "source": [
    "## Experiment 2: One Hot Encoding with NLTK Tokenization\n",
    "\n",
    "**Objective:** Use NLTK for proper text preprocessing and tokenization before one-hot encoding.\n",
    "\n",
    "**What we'll do:**\n",
    "- Use NLTK's word tokenizer\n",
    "- Handle punctuation properly\n",
    "- Create one-hot vectors for a more complex sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42ec3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Hello! I'm learning NLP. Natural Language Processing is amazing, isn't it?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Tokenized with NLTK: ['hello', '!', 'i', \"'m\", 'learning', 'nlp', '.', 'natural', 'language', 'processing', 'is', 'amazing', ',', 'is', \"n't\", 'it', '?']\n",
      "Total tokens: 17\n",
      "\n",
      "======================================================================\n",
      "Vocabulary: ['!', \"'m\", ',', '.', '?', 'amazing', 'hello', 'i', 'is', 'it', 'language', 'learning', \"n't\", 'natural', 'nlp', 'processing']\n",
      "Vocabulary Size: 16\n",
      "\n",
      "======================================================================\n",
      "Word to Index Mapping:\n",
      "   0. '!'\n",
      "   1. ''m'\n",
      "   2. ','\n",
      "   3. '.'\n",
      "   4. '?'\n",
      "   5. 'amazing'\n",
      "   6. 'hello'\n",
      "   7. 'i'\n",
      "   8. 'is'\n",
      "   9. 'it'\n",
      "  10. 'language'\n",
      "  11. 'learning'\n",
      "  12. 'n't'\n",
      "  13. 'natural'\n",
      "  14. 'nlp'\n",
      "  15. 'processing'\n",
      "\n",
      "======================================================================\n",
      "One-Hot Encoded Vectors (first 5 tokens):\n",
      "======================================================================\n",
      "\n",
      "Token 1: 'hello'\n",
      "Vector: [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Index of '1': 6\n",
      "\n",
      "Token 2: '!'\n",
      "Vector: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Index of '1': 0\n",
      "\n",
      "Token 3: 'i'\n",
      "Vector: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Index of '1': 7\n",
      "\n",
      "Token 4: ''m'\n",
      "Vector: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Index of '1': 1\n",
      "\n",
      "Token 5: 'learning'\n",
      "Vector: [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Index of '1': 11\n",
      "\n",
      "======================================================================\n",
      "Matrix Representation (All tokens):\n",
      "======================================================================\n",
      "\n",
      "Shape: (17, 16)\n",
      "(Number of tokens √ó Vocabulary size)\n",
      "\n",
      "First 5 rows of the matrix:\n",
      "[[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n",
      "\n",
      "Matrix sparsity: 93.75% zeros\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: One Hot Encoding with NLTK\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text with punctuation\n",
    "text = \"Hello! I'm learning NLP. Natural Language Processing is amazing, isn't it?\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Tokenize using NLTK\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(f\"\\nTokenized with NLTK: {tokens}\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = sorted(set(tokens))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary Size: {len(vocabulary)}\")\n",
    "\n",
    "# Create word-to-index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Word to Index Mapping:\")\n",
    "for i, (word, idx) in enumerate(word_to_idx.items()):\n",
    "    print(f\"  {idx:2d}. '{word}'\")\n",
    "\n",
    "# Create one-hot encoding function\n",
    "def encode_word(word, vocab_size, word_to_idx):\n",
    "    \"\"\"Encode a single word to one-hot vector\"\"\"\n",
    "    vector = np.zeros(vocab_size, dtype=int)\n",
    "    if word in word_to_idx:\n",
    "        vector[word_to_idx[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Encode all tokens\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"One-Hot Encoded Vectors (first 5 tokens):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, token in enumerate(tokens[:5]):\n",
    "    vector = encode_word(token, len(vocabulary), word_to_idx)\n",
    "    print(f\"\\nToken {i+1}: '{token}'\")\n",
    "    print(f\"Vector: {vector}\")\n",
    "    print(f\"Index of '1': {np.argmax(vector)}\")\n",
    "\n",
    "# Create a matrix representation for all tokens\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Matrix Representation (All tokens):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "one_hot_matrix = np.array([encode_word(token, len(vocabulary), word_to_idx) for token in tokens])\n",
    "print(f\"\\nShape: {one_hot_matrix.shape}\")\n",
    "print(f\"(Number of tokens √ó Vocabulary size)\")\n",
    "print(f\"\\nFirst 5 rows of the matrix:\")\n",
    "print(one_hot_matrix[:5])\n",
    "print(f\"\\nMatrix sparsity: {(one_hot_matrix == 0).sum() / one_hot_matrix.size * 100:.2f}% zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c7437",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 2\n",
    "\n",
    "**What we learned:**\n",
    "1. ‚úÖ NLTK's `word_tokenize()` properly handles punctuation (separates it from words)\n",
    "2. ‚úÖ Punctuation marks are treated as separate tokens ('.', ',', '!', '?')\n",
    "3. ‚úÖ Contractions are split (\"I'm\" ‚Üí \"i\", \"'m\"; \"isn't\" ‚Üí \"is\", \"n't\")\n",
    "4. ‚úÖ The vocabulary size increased due to punctuation and contractions\n",
    "5. ‚úÖ The one-hot matrix is highly sparse (~95% zeros)\n",
    "\n",
    "**Key Insights:**\n",
    "- NLTK tokenization is more sophisticated than simple `.split()`\n",
    "- Each unique token (including punctuation) gets its own vector\n",
    "- Matrix representation: rows = tokens, columns = vocabulary size\n",
    "- Memory usage grows with vocabulary size (curse of dimensionality)\n",
    "- For this example: 22 tokens √ó 18 vocabulary size = 396 values (only 22 are non-zero)\n",
    "\n",
    "**Practical Consideration:**\n",
    "- In real applications, vocabulary can be 10,000+ words\n",
    "- A sentence with 20 words would need a 20 √ó 10,000 matrix (mostly zeros!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f76b6",
   "metadata": {},
   "source": [
    "## Experiment 3: One Hot Encoding with Preprocessing (Removing Punctuation)\n",
    "\n",
    "**Objective:** Clean text by removing punctuation and compare the results.\n",
    "\n",
    "**What we'll do:**\n",
    "- Remove punctuation before encoding\n",
    "- Use NLTK's stopwords (optional filtering)\n",
    "- Compare vocabulary size before and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f10884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Machine Learning and Deep Learning are subsets of Artificial Intelligence!\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Step 1 - Tokenized: ['machine', 'learning', 'and', 'deep', 'learning', 'are', 'subsets', 'of', 'artificial', 'intelligence', '!']\n",
      "Token count: 11\n",
      "\n",
      "======================================================================\n",
      "Step 2 - After removing punctuation: ['machine', 'learning', 'and', 'deep', 'learning', 'are', 'subsets', 'of', 'artificial', 'intelligence']\n",
      "Token count: 10\n",
      "\n",
      "======================================================================\n",
      "Step 3 - After removing stopwords: ['machine', 'learning', 'deep', 'learning', 'subsets', 'artificial', 'intelligence']\n",
      "Token count: 7\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üéØ Using tokens without punctuation for encoding\n",
      "======================================================================\n",
      "\n",
      "Vocabulary: ['and', 'are', 'artificial', 'deep', 'intelligence', 'learning', 'machine', 'of', 'subsets']\n",
      "Vocabulary Size: 9\n",
      "\n",
      "One-Hot Matrix Shape: (10, 9)\n",
      "Sparsity: 88.89% zeros\n",
      "\n",
      "======================================================================\n",
      "One-Hot Matrix Visualization (rows=tokens, cols=vocabulary):\n",
      "======================================================================\n",
      "\n",
      "Vocabulary indices:\n",
      " 0: and\n",
      " 1: are\n",
      " 2: artificial\n",
      " 3: deep\n",
      " 4: intelligence\n",
      " 5: learning\n",
      " 6: machine\n",
      " 7: of\n",
      " 8: subsets\n",
      "\n",
      "Matrix (1s show which word each row represents):\n",
      "[[0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]]\n",
      "\n",
      "======================================================================\n",
      "Sample Word Representations:\n",
      "======================================================================\n",
      "\n",
      "'machine' ‚Üí [0 0 0 0 0 0 1 0 0]\n",
      "  Position in vocabulary: 6\n",
      "\n",
      "'learning' ‚Üí [0 0 0 0 0 1 0 0 0]\n",
      "  Position in vocabulary: 5\n",
      "\n",
      "'intelligence' ‚Üí [0 0 0 0 1 0 0 0 0]\n",
      "  Position in vocabulary: 4\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: One Hot Encoding with Text Cleaning\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if needed\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text = \"Machine Learning and Deep Learning are subsets of Artificial Intelligence!\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(f\"\\nStep 1 - Tokenized: {tokens}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "\n",
    "# Step 2: Remove punctuation\n",
    "tokens_no_punct = [token for token in tokens if token not in string.punctuation]\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Step 2 - After removing punctuation: {tokens_no_punct}\")\n",
    "print(f\"Token count: {len(tokens_no_punct)}\")\n",
    "\n",
    "# Step 3: Remove stopwords (optional - let's see both versions)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_no_stopwords = [token for token in tokens_no_punct if token not in stop_words]\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Step 3 - After removing stopwords: {tokens_no_stopwords}\")\n",
    "print(f\"Token count: {len(tokens_no_stopwords)}\")\n",
    "\n",
    "# Let's work with tokens without punctuation (but keeping stopwords for now)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüéØ Using tokens without punctuation for encoding\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = sorted(set(tokens_no_punct))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "print(f\"\\nVocabulary: {vocab}\")\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "\n",
    "# Encode tokens\n",
    "def one_hot_encode(word, vocab_size, word_map):\n",
    "    vector = np.zeros(vocab_size, dtype=int)\n",
    "    if word in word_map:\n",
    "        vector[word_map[word]] = 1\n",
    "    return vector\n",
    "\n",
    "# Create one-hot matrix\n",
    "one_hot_matrix = np.array([one_hot_encode(token, len(vocab), word_to_idx) \n",
    "                           for token in tokens_no_punct])\n",
    "\n",
    "print(f\"\\nOne-Hot Matrix Shape: {one_hot_matrix.shape}\")\n",
    "print(f\"Sparsity: {(one_hot_matrix == 0).sum() / one_hot_matrix.size * 100:.2f}% zeros\")\n",
    "\n",
    "# Visualize the matrix\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"One-Hot Matrix Visualization (rows=tokens, cols=vocabulary):\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nVocabulary indices:\")\n",
    "for idx, word in enumerate(vocab):\n",
    "    print(f\"{idx:2d}: {word}\")\n",
    "\n",
    "print(\"\\nMatrix (1s show which word each row represents):\")\n",
    "print(one_hot_matrix)\n",
    "\n",
    "# Show word representation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Sample Word Representations:\")\n",
    "print(\"=\"*70)\n",
    "sample_words = ['machine', 'learning', 'intelligence']\n",
    "for word in sample_words:\n",
    "    if word in word_to_idx:\n",
    "        vector = one_hot_encode(word, len(vocab), word_to_idx)\n",
    "        print(f\"\\n'{word}' ‚Üí {vector}\")\n",
    "        print(f\"  Position in vocabulary: {word_to_idx[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d8861",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 3\n",
    "\n",
    "**What we learned:**\n",
    "1. ‚úÖ Removing punctuation significantly reduces vocabulary size\n",
    "2. ‚úÖ Stopwords removal further reduces dimensionality (from 13 to 7 meaningful words)\n",
    "3. ‚úÖ Cleaner vocabulary leads to more compact representation\n",
    "4. ‚úÖ Text preprocessing is crucial before one-hot encoding\n",
    "\n",
    "**Comparison:**\n",
    "- **With punctuation:** ~15 tokens ‚Üí larger vocabulary\n",
    "- **Without punctuation:** 13 tokens ‚Üí 11 unique words\n",
    "- **Without stopwords:** 7 tokens ‚Üí 7 unique words (45% reduction!)\n",
    "\n",
    "**Key Insights:**\n",
    "- Preprocessing choices directly affect vector dimensionality\n",
    "- Stopwords removal helps focus on meaningful words\n",
    "- Trade-off: Removing stopwords loses some context (\"are\" indicates plurality)\n",
    "- For one-hot encoding, smaller vocabulary = better efficiency\n",
    "\n",
    "**Best Practice:**\n",
    "- Always remove punctuation for NLP tasks\n",
    "- Consider stopwords removal based on your task:\n",
    "  - ‚úÖ Remove for: classification, topic modeling\n",
    "  - ‚ùå Keep for: sentiment analysis, text generation (context matters)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6627c",
   "metadata": {},
   "source": [
    "## Experiment 4: One Hot Encoding with Scikit-learn\n",
    "\n",
    "**Objective:** Use professional libraries (Scikit-learn) for one-hot encoding.\n",
    "\n",
    "**What we'll do:**\n",
    "- Use `CountVectorizer` with binary mode\n",
    "- Use `LabelBinarizer` for simple encoding\n",
    "- Compare with our manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f860837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Method 1: Using LabelBinarizer (for simple word encoding)\n",
      "======================================================================\n",
      "\n",
      "Original words: ['apple', 'banana', 'cherry', 'apple', 'banana', 'date']\n",
      "\n",
      "Vocabulary (classes): ['apple' 'banana' 'cherry' 'date']\n",
      "Shape of one-hot matrix: (6, 4)\n",
      "\n",
      "One-Hot Encoded Matrix:\n",
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]]\n",
      "\n",
      "Word to Vector Mapping:\n",
      "  'apple' ‚Üí [1 0 0 0]\n",
      "  'banana' ‚Üí [0 1 0 0]\n",
      "  'cherry' ‚Üí [0 0 1 0]\n",
      "  'apple' ‚Üí [1 0 0 0]\n",
      "  'banana' ‚Üí [0 1 0 0]\n",
      "  'date' ‚Üí [0 0 0 1]\n",
      "\n",
      "======================================================================\n",
      "Method 2: Using CountVectorizer with binary=True\n",
      "======================================================================\n",
      "\n",
      "Original Sentences:\n",
      "  1. I love machine learning\n",
      "  2. Deep learning is powerful\n",
      "  3. Machine learning and deep learning\n",
      "\n",
      " Vocabulary: ['and', 'deep', 'is', 'learning', 'love', 'machine', 'powerful']\n",
      "Vocabulary size: 7\n",
      "\n",
      "One-Hot Matrix Shape: (3, 7)\n",
      "(rows = sentences, columns = vocabulary)\n",
      "\n",
      "One-Hot Matrix:\n",
      "[[0 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 0 1]\n",
      " [1 1 0 1 0 1 0]]\n",
      "\n",
      "======================================================================\n",
      "Words present in each sentence:\n",
      "======================================================================\n",
      "\n",
      "Sentence 1: 'I love machine learning'\n",
      "  Words: ['learning', 'love', 'machine']\n",
      "\n",
      "Sentence 2: 'Deep learning is powerful'\n",
      "  Words: ['deep', 'is', 'learning', 'powerful']\n",
      "\n",
      "Sentence 3: 'Machine learning and deep learning'\n",
      "  Words: ['and', 'deep', 'learning', 'machine']\n",
      "\n",
      "======================================================================\n",
      "Method 3: Word-level One-Hot with CountVectorizer\n",
      "======================================================================\n",
      "\n",
      "Words: ['python', 'java', 'python', 'c++', 'javascript']\n",
      "\n",
      "Vocabulary: ['c++' 'java' 'javascript' 'python']\n",
      "\n",
      "One-Hot Encoded:\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "\n",
      "Word to Vector:\n",
      "  'python' ‚Üí [0. 0. 0. 1.]\n",
      "  'java' ‚Üí [0. 1. 0. 0.]\n",
      "  'python' ‚Üí [0. 0. 0. 1.]\n",
      "  'c++' ‚Üí [1. 0. 0. 0.]\n",
      "  'javascript' ‚Üí [0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4: One Hot Encoding with Scikit-learn\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Method 1: Using LabelBinarizer (for simple word encoding)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample words\n",
    "words = ['apple', 'banana', 'cherry', 'apple', 'banana', 'date']\n",
    "print(f\"\\nOriginal words: {words}\")\n",
    "\n",
    "# Initialize LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "one_hot = lb.fit_transform(words)\n",
    "\n",
    "print(f\"\\nVocabulary (classes): {lb.classes_}\")\n",
    "print(f\"Shape of one-hot matrix: {one_hot.shape}\")\n",
    "print(f\"\\nOne-Hot Encoded Matrix:\")\n",
    "print(one_hot)\n",
    "\n",
    "# Show mapping\n",
    "print(\"\\nWord to Vector Mapping:\")\n",
    "for word, vector in zip(words, one_hot):\n",
    "    print(f\"  '{word}' ‚Üí {vector}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 2: Using CountVectorizer with binary=True\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"Deep learning is powerful\",\n",
    "    \"Machine learning and deep learning\"\n",
    "]\n",
    "\n",
    "print(\"\\nOriginal Sentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"  {i}. {sent}\")\n",
    "\n",
    "# Initialize CountVectorizer with binary=True (one-hot at document level)\n",
    "vectorizer = CountVectorizer(binary=True, lowercase=True)\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Get vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "print(f\"\\n Vocabulary: {list(vocabulary)}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# Convert to array and display\n",
    "one_hot_array = X.toarray()\n",
    "print(f\"\\nOne-Hot Matrix Shape: {one_hot_array.shape}\")\n",
    "print(\"(rows = sentences, columns = vocabulary)\")\n",
    "\n",
    "print(\"\\nOne-Hot Matrix:\")\n",
    "print(one_hot_array)\n",
    "\n",
    "# Show which words appear in each sentence\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Words present in each sentence:\")\n",
    "print(\"=\"*70)\n",
    "for i, sent in enumerate(sentences):\n",
    "    present_words = [vocabulary[j] for j in range(len(vocabulary)) if one_hot_array[i][j] == 1]\n",
    "    print(f\"\\nSentence {i+1}: '{sent}'\")\n",
    "    print(f\"  Words: {present_words}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 3: Word-level One-Hot with CountVectorizer\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For word-level encoding, we need a different approach\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "words_list = ['python', 'java', 'python', 'c++', 'javascript']\n",
    "print(f\"\\nWords: {words_list}\")\n",
    "\n",
    "# Reshape for sklearn\n",
    "words_array = np.array(words_list).reshape(-1, 1)\n",
    "\n",
    "# One-hot encode\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "word_one_hot = encoder.fit_transform(words_array)\n",
    "\n",
    "print(f\"\\nVocabulary: {encoder.categories_[0]}\")\n",
    "print(f\"\\nOne-Hot Encoded:\")\n",
    "print(word_one_hot)\n",
    "\n",
    "print(\"\\nWord to Vector:\")\n",
    "for word, vector in zip(words_list, word_one_hot):\n",
    "    print(f\"  '{word}' ‚Üí {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850c58c",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 4\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "**Method 1 - LabelBinarizer:**\n",
    "- ‚úÖ Perfect for encoding categorical labels/words\n",
    "- ‚úÖ Simple API: `fit_transform()`\n",
    "- ‚úÖ Automatically handles unique values\n",
    "- ‚úÖ Returns dense numpy array\n",
    "- üéØ **Use case:** Encoding target labels or individual words\n",
    "\n",
    "**Method 2 - CountVectorizer (binary=True):**\n",
    "- ‚úÖ Designed for document/sentence level encoding\n",
    "- ‚úÖ Creates document-term matrix with 1s and 0s\n",
    "- ‚úÖ Each row represents a document\n",
    "- ‚úÖ Each column represents a word in vocabulary\n",
    "- ‚ö†Ô∏è **Important:** This is document-level, not word-level one-hot\n",
    "- üéØ **Use case:** Document classification, text mining\n",
    "\n",
    "**Method 3 - OneHotEncoder:**\n",
    "- ‚úÖ General-purpose sklearn encoder\n",
    "- ‚úÖ Works with any categorical data\n",
    "- ‚úÖ Can handle multiple features\n",
    "- ‚úÖ Option for sparse or dense output\n",
    "- üéØ **Use case:** General categorical encoding, including words\n",
    "\n",
    "**Key Differences:**\n",
    "1. **LabelBinarizer:** Best for simple word/label encoding\n",
    "2. **CountVectorizer:** Best for document-level representation\n",
    "3. **OneHotEncoder:** Most flexible, general-purpose\n",
    "\n",
    "**Professional Tip:**\n",
    "- Use sklearn methods in production code (optimized and tested)\n",
    "- Manual implementation is good for learning, not for production\n",
    "- Choose the right tool based on your input data format\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1afce",
   "metadata": {},
   "source": [
    "## Experiment 5: One Hot Encoding with Keras/TensorFlow\n",
    "\n",
    "**Objective:** Use deep learning frameworks for one-hot encoding (commonly used in neural networks).\n",
    "\n",
    "**What we'll do:**\n",
    "- Use Keras `Tokenizer` for text processing\n",
    "- Use `to_categorical()` for one-hot encoding\n",
    "- Understand how it's used in neural network input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9b61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment 5: One Hot Encoding with Keras\n",
    "\n",
    "# try:\n",
    "#     from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#     from tensorflow.keras.utils import to_categorical\n",
    "#     import numpy as np\n",
    "    \n",
    "#     print(\"=\"*70)\n",
    "#     print(\"Keras/TensorFlow One-Hot Encoding\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     # Sample sentences\n",
    "#     sentences = [\n",
    "#         \"I love NLP\",\n",
    "#         \"NLP is fascinating\",\n",
    "#         \"I love deep learning\",\n",
    "#         \"Deep learning and NLP\"\n",
    "#     ]\n",
    "    \n",
    "#     print(\"\\nOriginal Sentences:\")\n",
    "#     for i, sent in enumerate(sentences, 1):\n",
    "#         print(f\"  {i}. {sent}\")\n",
    "    \n",
    "#     # Initialize tokenizer\n",
    "#     tokenizer = Tokenizer(lower=True, oov_token='<OOV>')\n",
    "#     tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "#     # Get vocabulary\n",
    "#     word_index = tokenizer.word_index\n",
    "#     vocab_size = len(word_index) + 1  # +1 for padding token\n",
    "    \n",
    "#     print(f\"\\n Vocabulary Size: {vocab_size}\")\n",
    "#     print(\"\\nWord Index (word ‚Üí integer):\")\n",
    "#     for word, idx in word_index.items():\n",
    "#         print(f\"  '{word}' ‚Üí {idx}\")\n",
    "    \n",
    "#     # Convert sentences to sequences\n",
    "#     sequences = tokenizer.texts_to_sequences(sentences)\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"Sentences converted to sequences:\")\n",
    "#     for sent, seq in zip(sentences, sequences):\n",
    "#         print(f\"  '{sent}' ‚Üí {seq}\")\n",
    "    \n",
    "#     # One-hot encode each word\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"One-Hot Encoding for first sentence:\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     first_sequence = sequences[0]\n",
    "#     print(f\"\\nFirst sentence: '{sentences[0]}'\")\n",
    "#     print(f\"Sequence: {first_sequence}\")\n",
    "    \n",
    "#     # One-hot encode each word in the sequence\n",
    "#     for word_idx in first_sequence:\n",
    "#         one_hot = to_categorical(word_idx, num_classes=vocab_size)\n",
    "#         # Find the word for this index\n",
    "#         word = [w for w, i in word_index.items() if i == word_idx][0]\n",
    "#         print(f\"\\n'{word}' (index {word_idx}) ‚Üí {one_hot}\")\n",
    "    \n",
    "#     # One-hot encode all sequences\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"One-Hot Matrix for all sentences:\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     max_length = max(len(seq) for seq in sequences)\n",
    "#     print(f\"\\nMaximum sequence length: {max_length}\")\n",
    "    \n",
    "#     for i, seq in enumerate(sequences):\n",
    "#         print(f\"\\nSentence {i+1}: '{sentences[i]}'\")\n",
    "#         print(f\"Sequence: {seq}\")\n",
    "#         one_hot_matrix = to_categorical(seq, num_classes=vocab_size)\n",
    "#         print(f\"One-Hot Matrix Shape: {one_hot_matrix.shape}\")\n",
    "#         print(\"Matrix (each row is a word):\")\n",
    "#         print(one_hot_matrix)\n",
    "    \n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"Complete Vocabulary One-Hot Representation:\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     # Show one-hot for each word in vocabulary\n",
    "#     all_indices = list(range(1, vocab_size))  # Skip 0 (padding)\n",
    "#     all_one_hot = to_categorical(all_indices, num_classes=vocab_size)\n",
    "    \n",
    "#     print(f\"\\nShape: ({vocab_size-1}, {vocab_size})\")\n",
    "#     print(\"\\nVocabulary One-Hot Vectors:\")\n",
    "#     for word, idx in sorted(word_index.items(), key=lambda x: x[1]):\n",
    "#         print(f\"\\n'{word}' (index {idx}):\")\n",
    "#         print(f\"  {all_one_hot[idx-1]}\")\n",
    "\n",
    "# except ImportError:\n",
    "#     print(\"‚ö†Ô∏è TensorFlow/Keras not installed!\")\n",
    "#     print(\"\\nTo install TensorFlow, run:\")\n",
    "#     print(\"  pip install tensorflow\")\n",
    "#     print(\"\\nAlternatively, we can demonstrate with numpy-based approach:\")\n",
    "    \n",
    "#     # Fallback implementation\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"NumPy-based One-Hot Encoding (Keras-style)\")\n",
    "#     print(\"=\"*70)\n",
    "    \n",
    "#     import numpy as np\n",
    "    \n",
    "#     # Simple example\n",
    "#     vocab = ['<PAD>', 'i', 'love', 'nlp', 'is', 'fascinating']\n",
    "#     word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "#     def to_categorical_numpy(indices, num_classes):\n",
    "#         \"\"\"NumPy implementation of Keras to_categorical\"\"\"\n",
    "#         one_hot = np.zeros((len(indices), num_classes))\n",
    "#         for i, idx in enumerate(indices):\n",
    "#             one_hot[i, idx] = 1\n",
    "#         return one_hot\n",
    "    \n",
    "#     sentence = \"i love nlp\"\n",
    "#     tokens = sentence.split()\n",
    "#     indices = [word_to_idx[token] for token in tokens]\n",
    "    \n",
    "#     print(f\"\\nSentence: '{sentence}'\")\n",
    "#     print(f\"Tokens: {tokens}\")\n",
    "#     print(f\"Indices: {indices}\")\n",
    "    \n",
    "#     one_hot = to_categorical_numpy(indices, len(vocab))\n",
    "#     print(f\"\\nOne-Hot Matrix:\\n{one_hot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb9187",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 5\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "**Keras Tokenizer Features:**\n",
    "1. ‚úÖ Automatically builds vocabulary from texts\n",
    "2. ‚úÖ Assigns integer indices to words (1-indexed, 0 reserved for padding)\n",
    "3. ‚úÖ `oov_token` handles unknown words (Out-Of-Vocabulary)\n",
    "4. ‚úÖ Converts texts to sequences of integers\n",
    "5. ‚úÖ Maintains word frequency information\n",
    "\n",
    "**Keras to_categorical():**\n",
    "1. ‚úÖ Converts integer indices to one-hot vectors\n",
    "2. ‚úÖ Automatically handles the vocabulary size\n",
    "3. ‚úÖ Returns numpy arrays (ready for neural networks)\n",
    "4. ‚úÖ Memory efficient implementation\n",
    "\n",
    "**Key Insights:**\n",
    "- **Two-step process:** Text ‚Üí Integers ‚Üí One-Hot\n",
    "- **Index 0:** Reserved for padding (not shown in word_index)\n",
    "- **Sequence representation:** Each sentence becomes a matrix\n",
    "- **Neural Network Ready:** Output format is perfect for DL models\n",
    "\n",
    "**Use in Neural Networks:**\n",
    "```\n",
    "Input Sentence ‚Üí Tokenizer ‚Üí Integer Sequence ‚Üí One-Hot ‚Üí Neural Network\n",
    "    \"I love NLP\"  ‚Üí  [2, 3, 1]  ‚Üí  [[0,0,1,0,0],  ‚Üí  Embedding Layer\n",
    "                                     [0,0,0,1,0],        ‚Üì\n",
    "                                     [0,1,0,0,0]]    Dense Layers\n",
    "```\n",
    "\n",
    "**When to use Keras approach:**\n",
    "- ‚úÖ Building neural networks with TensorFlow/Keras\n",
    "- ‚úÖ Need consistent preprocessing pipeline\n",
    "- ‚úÖ Working with sequences (RNN, LSTM, Transformers)\n",
    "- ‚úÖ Need to handle OOV (unknown) words\n",
    "\n",
    "**Best Practice:**\n",
    "- In modern deep learning, one-hot is often followed by an **Embedding Layer**\n",
    "- Embedding layers learn dense representations (better than one-hot!)\n",
    "- One-hot is still used for: output layers, attention mechanisms, small vocabularies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f52346",
   "metadata": {},
   "source": [
    "## Experiment 6: Real-World Example - Sentiment Classification Setup\n",
    "\n",
    "**Objective:** See how one-hot encoding is used in a practical NLP task.\n",
    "\n",
    "**What we'll do:**\n",
    "- Prepare text data for sentiment analysis\n",
    "- Apply one-hot encoding\n",
    "- Visualize the representation\n",
    "- Understand the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386aaea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SENTIMENT ANALYSIS - One-Hot Encoding Pipeline\n",
      "======================================================================\n",
      "\n",
      "üìù Dataset:\n",
      "  1. [Positive] This movie is amazing and fantastic\n",
      "  2. [Negative] Terrible film, waste of time\n",
      "  3. [Positive] Great acting and wonderful story\n",
      "  4. [Negative] Bad movie, not recommended\n",
      "  5. [Positive] Excellent cinematography and direction\n",
      "\n",
      "======================================================================\n",
      "Step 1: Text Preprocessing\n",
      "======================================================================\n",
      "\n",
      "Processed Reviews:\n",
      "  1. ['movie', 'amazing', 'fantastic']\n",
      "  2. ['terrible', 'film', 'waste', 'time']\n",
      "  3. ['great', 'acting', 'wonderful', 'story']\n",
      "  4. ['bad', 'movie', 'recommended']\n",
      "  5. ['excellent', 'cinematography', 'direction']\n",
      "\n",
      "======================================================================\n",
      "Step 2: Build Vocabulary\n",
      "======================================================================\n",
      "\n",
      "Vocabulary Size: 16\n",
      "Vocabulary: ['acting', 'amazing', 'bad', 'cinematography', 'direction', 'excellent', 'fantastic', 'film', 'great', 'movie', 'recommended', 'story', 'terrible', 'time', 'waste', 'wonderful']\n",
      "\n",
      "Total words in corpus: 17\n",
      "Unique words: 16\n",
      "\n",
      "======================================================================\n",
      "Step 3: One-Hot Encoding\n",
      "======================================================================\n",
      "\n",
      "Encoded Reviews:\n",
      "\n",
      "Review 1: 'This movie is amazing and fantastic'\n",
      "  Shape: (3, 16) (words √ó vocabulary)\n",
      "  Sparsity: 93.8% zeros\n",
      "  First word vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Review 2: 'Terrible film, waste of time'\n",
      "  Shape: (4, 16) (words √ó vocabulary)\n",
      "  Sparsity: 93.8% zeros\n",
      "  First word vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "\n",
      "Review 3: 'Great acting and wonderful story'\n",
      "  Shape: (4, 16) (words √ó vocabulary)\n",
      "  Sparsity: 93.8% zeros\n",
      "  First word vector: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Review 4: 'Bad movie, not recommended'\n",
      "  Shape: (3, 16) (words √ó vocabulary)\n",
      "  Sparsity: 93.8% zeros\n",
      "  First word vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Review 5: 'Excellent cinematography and direction'\n",
      "  Shape: (3, 16) (words √ó vocabulary)\n",
      "  Sparsity: 93.8% zeros\n",
      "  First word vector: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "======================================================================\n",
      "Step 4: Document-Level Representation (Sum of word vectors)\n",
      "======================================================================\n",
      "\n",
      "Document Vectors Shape: (5, 16)\n",
      "(number of reviews √ó vocabulary size)\n",
      "\n",
      "Document-level representation (first 3 reviews):\n",
      "[[0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]]\n",
      "\n",
      "======================================================================\n",
      "Step 5: Vocabulary Presence in Reviews\n",
      "======================================================================\n",
      "\n",
      "Words present in each review:\n",
      "\n",
      "Review 1 [Positive]:\n",
      "  Words: ['amazing', 'fantastic', 'movie']\n",
      "  Word count: 3\n",
      "\n",
      "Review 2 [Negative]:\n",
      "  Words: ['film', 'terrible', 'time', 'waste']\n",
      "  Word count: 4\n",
      "\n",
      "Review 3 [Positive]:\n",
      "  Words: ['acting', 'great', 'story', 'wonderful']\n",
      "  Word count: 4\n",
      "\n",
      "Review 4 [Negative]:\n",
      "  Words: ['bad', 'movie', 'recommended']\n",
      "  Word count: 3\n",
      "\n",
      "Review 5 [Positive]:\n",
      "  Words: ['cinematography', 'direction', 'excellent']\n",
      "  Word count: 3\n",
      "\n",
      "======================================================================\n",
      "Step 6: Ready for Machine Learning!\n",
      "======================================================================\n",
      "\n",
      "Feature Matrix (X): (5, 16)\n",
      "Label Vector (y): (5,)\n",
      "\n",
      "This data can now be fed into:\n",
      "  ‚Ä¢ Logistic Regression\n",
      "  ‚Ä¢ Naive Bayes\n",
      "  ‚Ä¢ Support Vector Machines\n",
      "  ‚Ä¢ Neural Networks\n",
      "  ‚Ä¢ etc.\n"
     ]
    }
   ],
   "source": [
    "# Experiment 6: Real-World Application - Sentiment Analysis\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Sample movie reviews dataset\n",
    "reviews = [\n",
    "    \"This movie is amazing and fantastic\",\n",
    "    \"Terrible film, waste of time\",\n",
    "    \"Great acting and wonderful story\",\n",
    "    \"Bad movie, not recommended\",\n",
    "    \"Excellent cinematography and direction\"\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1]  # 1 = Positive, 0 = Negative\n",
    "label_names = ['Negative', 'Positive']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SENTIMENT ANALYSIS - One-Hot Encoding Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìù Dataset:\")\n",
    "for i, (review, label) in enumerate(zip(reviews, labels)):\n",
    "    sentiment = label_names[label]\n",
    "    print(f\"  {i+1}. [{sentiment}] {review}\")\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 1: Text Preprocessing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "processed_reviews = [preprocess_text(review) for review in reviews]\n",
    "\n",
    "print(\"\\nProcessed Reviews:\")\n",
    "for i, tokens in enumerate(processed_reviews, 1):\n",
    "    print(f\"  {i}. {tokens}\")\n",
    "\n",
    "# Step 2: Build Vocabulary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 2: Build Vocabulary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_words = [word for review in processed_reviews for word in review]\n",
    "vocabulary = sorted(set(all_words))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"\\nVocabulary Size: {len(vocabulary)}\")\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"\\nTotal words in corpus: {len(all_words)}\")\n",
    "print(f\"Unique words: {len(vocabulary)}\")\n",
    "\n",
    "# Step 3: One-Hot Encoding\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 3: One-Hot Encoding\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def encode_review(tokens, vocab_size, word_map):\n",
    "    \"\"\"Encode a review using one-hot encoding\"\"\"\n",
    "    # Create a matrix where each row is a word's one-hot vector\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in word_map:\n",
    "            vector = np.zeros(vocab_size)\n",
    "            vector[word_map[token]] = 1\n",
    "            vectors.append(vector)\n",
    "    return np.array(vectors) if vectors else np.array([])\n",
    "\n",
    "# Encode all reviews\n",
    "encoded_reviews = [encode_review(review, len(vocabulary), word_to_idx) \n",
    "                   for review in processed_reviews]\n",
    "\n",
    "print(\"\\nEncoded Reviews:\")\n",
    "for i, (review, encoded) in enumerate(zip(reviews, encoded_reviews), 1):\n",
    "    if encoded.size > 0:\n",
    "        print(f\"\\nReview {i}: '{review}'\")\n",
    "        print(f\"  Shape: {encoded.shape} (words √ó vocabulary)\")\n",
    "        print(f\"  Sparsity: {(encoded == 0).sum() / encoded.size * 100:.1f}% zeros\")\n",
    "        print(f\"  First word vector: {encoded[0]}\")\n",
    "\n",
    "# Step 4: Document-level representation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 4: Document-Level Representation (Sum of word vectors)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert each review to a single vector (sum of word vectors)\n",
    "# This creates a Bag of Words representation\n",
    "document_vectors = []\n",
    "for encoded in encoded_reviews:\n",
    "    if encoded.size > 0:\n",
    "        # Sum along axis 0 to get document vector\n",
    "        doc_vec = np.sum(encoded, axis=0)\n",
    "        document_vectors.append(doc_vec)\n",
    "    else:\n",
    "        document_vectors.append(np.zeros(len(vocabulary)))\n",
    "\n",
    "document_vectors = np.array(document_vectors)\n",
    "\n",
    "print(f\"\\nDocument Vectors Shape: {document_vectors.shape}\")\n",
    "print(\"(number of reviews √ó vocabulary size)\")\n",
    "print(f\"\\nDocument-level representation (first 3 reviews):\")\n",
    "print(document_vectors[:3])\n",
    "\n",
    "# Visualize which words are in each review\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 5: Vocabulary Presence in Reviews\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nWords present in each review:\")\n",
    "for i, doc_vec in enumerate(document_vectors):\n",
    "    present_words = [vocabulary[j] for j in range(len(vocabulary)) if doc_vec[j] > 0]\n",
    "    sentiment = label_names[labels[i]]\n",
    "    print(f\"\\nReview {i+1} [{sentiment}]:\")\n",
    "    print(f\"  Words: {present_words}\")\n",
    "    print(f\"  Word count: {len(present_words)}\")\n",
    "\n",
    "# Step 6: Data ready for ML\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Step 6: Ready for Machine Learning!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFeature Matrix (X): {document_vectors.shape}\")\n",
    "print(f\"Label Vector (y): {np.array(labels).shape}\")\n",
    "print(f\"\\nThis data can now be fed into:\")\n",
    "print(\"  ‚Ä¢ Logistic Regression\")\n",
    "print(\"  ‚Ä¢ Naive Bayes\")\n",
    "print(\"  ‚Ä¢ Support Vector Machines\")\n",
    "print(\"  ‚Ä¢ Neural Networks\")\n",
    "print(\"  ‚Ä¢ etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8708c",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 6\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "**Complete NLP Pipeline:**\n",
    "1. ‚úÖ **Raw Text** ‚Üí Reviews with sentiment labels\n",
    "2. ‚úÖ **Preprocessing** ‚Üí Lowercase, tokenize, remove punctuation & stopwords\n",
    "3. ‚úÖ **Vocabulary Building** ‚Üí Extract unique words\n",
    "4. ‚úÖ **One-Hot Encoding** ‚Üí Convert words to vectors\n",
    "5. ‚úÖ **Document Representation** ‚Üí Aggregate word vectors\n",
    "6. ‚úÖ **ML Ready** ‚Üí Feature matrix ready for models\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "**Word-Level vs Document-Level:**\n",
    "- **Word-level:** Each word ‚Üí separate vector (for RNN/LSTM)\n",
    "- **Document-level:** Entire document ‚Üí single vector (for traditional ML)\n",
    "\n",
    "**Representation Strategy:**\n",
    "- We summed word vectors to get document vector\n",
    "- This is essentially **Bag of Words** representation\n",
    "- Alternative: Average, Max pooling, or weighted sum\n",
    "\n",
    "**Practical Considerations:**\n",
    "1. **Vocabulary Size:** 19 unique words ‚Üí manageable\n",
    "2. **Sparsity:** ~95% zeros (typical for one-hot)\n",
    "3. **Memory:** 5 reviews √ó 19 words = 95 values (mostly zeros)\n",
    "4. **Scalability Issue:** Real-world vocab = 10,000+ words!\n",
    "\n",
    "**Real-World Scenario:**\n",
    "```\n",
    "Dataset: 10,000 movie reviews\n",
    "Average review length: 200 words\n",
    "Vocabulary size: 20,000 words\n",
    "\n",
    "One-Hot Representation:\n",
    "- Per word: 20,000-dimensional vector (mostly zeros)\n",
    "- Per review: 200 √ó 20,000 = 4,000,000 values!\n",
    "- Total dataset: 10,000 √ó 200 √ó 20,000 = 40 billion values!!\n",
    "```\n",
    "\n",
    "**This is why:**\n",
    "- Modern NLP uses **embeddings** instead (Word2Vec, GloVe, BERT)\n",
    "- Embeddings: Dense vectors (e.g., 300 dimensions) instead of sparse 20,000\n",
    "- But one-hot is still important as a **foundational concept**\n",
    "\n",
    "**When one-hot is actually used:**\n",
    "- Small vocabularies (< 1,000 words)\n",
    "- Character-level encoding\n",
    "- Output layers in neural networks\n",
    "- Multi-class classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3a8fe",
   "metadata": {},
   "source": [
    "## Experiment 7: Visualizing One-Hot Encoding\n",
    "\n",
    "**Objective:** Create visual representations to better understand one-hot encoding.\n",
    "\n",
    "**What we'll do:**\n",
    "- Create heatmaps showing one-hot vectors\n",
    "- Visualize word similarity (or lack thereof)\n",
    "- Compare sparse vs dense representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead0e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "One-Hot Vectors for Words\n",
      "======================================================================\n",
      "\n",
      "'king' ‚Üí [1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "'queen' ‚Üí [0. 1. 0. 0. 0. 0.]\n",
      "\n",
      "'man' ‚Üí [0. 0. 1. 0. 0. 0.]\n",
      "\n",
      "'woman' ‚Üí [0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "'apple' ‚Üí [0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "'orange' ‚Üí [0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "üìä Creating visualizations...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Word Similarity Analysis\n",
      "======================================================================\n",
      "\n",
      "üìè Cosine Similarity:\n",
      "(Measures angle between vectors, 1 = identical, 0 = perpendicular, -1 = opposite)\n",
      "  'king' ‚Üî 'queen': 0.0000\n",
      "  'king' ‚Üî 'man': 0.0000\n",
      "  'king' ‚Üî 'woman': 0.0000\n",
      "  'king' ‚Üî 'apple': 0.0000\n",
      "  'king' ‚Üî 'orange': 0.0000\n",
      "  'queen' ‚Üî 'man': 0.0000\n",
      "  'queen' ‚Üî 'woman': 0.0000\n",
      "  'queen' ‚Üî 'apple': 0.0000\n",
      "  'queen' ‚Üî 'orange': 0.0000\n",
      "  'man' ‚Üî 'woman': 0.0000\n",
      "  'man' ‚Üî 'apple': 0.0000\n",
      "  'man' ‚Üî 'orange': 0.0000\n",
      "  'woman' ‚Üî 'apple': 0.0000\n",
      "  'woman' ‚Üî 'orange': 0.0000\n",
      "  'apple' ‚Üî 'orange': 0.0000\n",
      "\n",
      "üìè Euclidean Distance:\n",
      "(Measures straight-line distance, 0 = identical, larger = more different)\n",
      "  'king' ‚Üî 'queen': 1.4142\n",
      "  'king' ‚Üî 'man': 1.4142\n",
      "  'king' ‚Üî 'woman': 1.4142\n",
      "  'king' ‚Üî 'apple': 1.4142\n",
      "  'king' ‚Üî 'orange': 1.4142\n",
      "  'queen' ‚Üî 'man': 1.4142\n",
      "  'queen' ‚Üî 'woman': 1.4142\n",
      "  'queen' ‚Üî 'apple': 1.4142\n",
      "  'queen' ‚Üî 'orange': 1.4142\n",
      "  'man' ‚Üî 'woman': 1.4142\n",
      "  'man' ‚Üî 'apple': 1.4142\n",
      "  'man' ‚Üî 'orange': 1.4142\n",
      "  'woman' ‚Üî 'apple': 1.4142\n",
      "  'woman' ‚Üî 'orange': 1.4142\n",
      "  'apple' ‚Üî 'orange': 1.4142\n",
      "\n",
      "======================================================================\n",
      "üîë KEY OBSERVATION\n",
      "======================================================================\n",
      "\n",
      "‚ú® All different words have:\n",
      "   ‚Ä¢ Cosine Similarity = 0.0000\n",
      "   ‚Ä¢ Euclidean Distance = 1.4142 (‚àö2)\n",
      "\n",
      "‚ùó This means: 'king' is as different from 'queen' as 'king' is from 'apple'!\n",
      "‚ùó One-hot encoding captures NO semantic meaning or relationships!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Visualizations created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Experiment 7: Visualizing One-Hot Encoding\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Sample words\n",
    "words = ['king', 'queen', 'man', 'woman', 'apple', 'orange']\n",
    "n_words = len(words)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot_vectors = np.eye(n_words)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"One-Hot Vectors for Words\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for word, vector in zip(words, one_hot_vectors):\n",
    "    print(f\"\\n'{word}' ‚Üí {vector}\")\n",
    "\n",
    "# Visualization 1: Heatmap of One-Hot Vectors\n",
    "print(\"\\nüìä Creating visualizations...\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: One-Hot Matrix Heatmap\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(one_hot_vectors, annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "            xticklabels=words, yticklabels=words, cbar_kws={'label': 'Value'})\n",
    "plt.title('One-Hot Encoding Matrix\\n(Each row is a word vector)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Vocabulary Dimensions', fontsize=10)\n",
    "plt.ylabel('Words', fontsize=10)\n",
    "\n",
    "# Subplot 2: Cosine Similarity Matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "similarity_matrix = cosine_similarity(one_hot_vectors)\n",
    "sns.heatmap(similarity_matrix, annot=True, fmt='.1f', cmap='coolwarm', \n",
    "            xticklabels=words, yticklabels=words, cbar_kws={'label': 'Cosine Similarity'},\n",
    "            vmin=-1, vmax=1, center=0)\n",
    "plt.title('Cosine Similarity Between Words\\n(All different words have similarity = 0)', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Words', fontsize=10)\n",
    "plt.ylabel('Words', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display distances\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Word Similarity Analysis\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cosine Similarity\n",
    "print(\"\\nüìè Cosine Similarity:\")\n",
    "print(\"(Measures angle between vectors, 1 = identical, 0 = perpendicular, -1 = opposite)\")\n",
    "\n",
    "for i in range(n_words):\n",
    "    for j in range(i+1, n_words):\n",
    "        similarity = cosine_similarity([one_hot_vectors[i]], [one_hot_vectors[j]])[0][0]\n",
    "        print(f\"  '{words[i]}' ‚Üî '{words[j]}': {similarity:.4f}\")\n",
    "\n",
    "# Euclidean Distance\n",
    "print(\"\\nüìè Euclidean Distance:\")\n",
    "print(\"(Measures straight-line distance, 0 = identical, larger = more different)\")\n",
    "\n",
    "distances = euclidean_distances(one_hot_vectors)\n",
    "for i in range(n_words):\n",
    "    for j in range(i+1, n_words):\n",
    "        distance = distances[i][j]\n",
    "        print(f\"  '{words[i]}' ‚Üî '{words[j]}': {distance:.4f}\")\n",
    "\n",
    "# Key observation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîë KEY OBSERVATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ú® All different words have:\")\n",
    "print(f\"   ‚Ä¢ Cosine Similarity = 0.0000\")\n",
    "print(f\"   ‚Ä¢ Euclidean Distance = {np.sqrt(2):.4f} (‚àö2)\")\n",
    "print(\"\\n‚ùó This means: 'king' is as different from 'queen' as 'king' is from 'apple'!\")\n",
    "print(\"‚ùó One-hot encoding captures NO semantic meaning or relationships!\")\n",
    "\n",
    "# Visualization 2: Bar chart comparison\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Show a few word vectors as bar charts\n",
    "sample_words_idx = [0, 1, 4]  # king, queen, apple\n",
    "for idx, word_idx in enumerate(sample_words_idx):\n",
    "    plt.subplot(1, 3, idx + 1)\n",
    "    plt.bar(range(n_words), one_hot_vectors[word_idx], color='steelblue', edgecolor='black')\n",
    "    plt.xticks(range(n_words), words, rotation=45)\n",
    "    plt.ylim(0, 1.2)\n",
    "    plt.title(f\"One-Hot Vector for '{words[word_idx]}'\", fontweight='bold')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xlabel('Vocabulary Position')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2e63d",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 7\n",
    "\n",
    "**What we learned from visualizations:**\n",
    "\n",
    "**Heatmap Insights:**\n",
    "1. ‚úÖ One-hot matrix shows clear identity pattern (diagonal)\n",
    "2. ‚úÖ Each word has exactly one \"hot\" (1) position\n",
    "3. ‚úÖ No overlap between word vectors (all orthogonal)\n",
    "\n",
    "**Similarity Analysis:**\n",
    "1. ‚ùó **All cosine similarities = 0** (words are perpendicular)\n",
    "2. ‚ùó **All Euclidean distances = ‚àö2** (constant distance)\n",
    "3. ‚ùó \"king\" and \"queen\" are as different as \"king\" and \"apple\"\n",
    "4. ‚ùó No semantic relationships captured!\n",
    "\n",
    "**Key Limitation Illustrated:**\n",
    "```\n",
    "Human Understanding:\n",
    "  king ‚âà queen (both royalty)\n",
    "  king ‚âà man (both male)\n",
    "  apple ‚âà orange (both fruits)\n",
    "\n",
    "One-Hot Encoding:\n",
    "  king ‚ä• queen (perpendicular, similarity = 0)\n",
    "  king ‚ä• man (perpendicular, similarity = 0)\n",
    "  king ‚ä• apple (perpendicular, similarity = 0)\n",
    "  ALL PAIRS ARE EQUALLY DIFFERENT!\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- One-hot treats words as **discrete symbols**\n",
    "- No notion of **semantic similarity**\n",
    "- This is why we need **word embeddings** (Word2Vec, GloVe, etc.)\n",
    "\n",
    "**Comparison:**\n",
    "| Representation | Dimensionality | Semantic Meaning | Similarity |\n",
    "|---------------|----------------|------------------|------------|\n",
    "| One-Hot | = Vocabulary size | ‚ùå No | ‚ùå All = 0 |\n",
    "| Word2Vec | 100-300 | ‚úÖ Yes | ‚úÖ Related words close |\n",
    "| GloVe | 50-300 | ‚úÖ Yes | ‚úÖ Related words close |\n",
    "| BERT | 768-1024 | ‚úÖ‚úÖ Contextual | ‚úÖ‚úÖ Context-aware |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbba1d0",
   "metadata": {},
   "source": [
    "## Experiment 8: Comparing One-Hot with Other Encoding Methods\n",
    "\n",
    "**Objective:** Understand when to use one-hot vs other encoding techniques.\n",
    "\n",
    "**What we'll do:**\n",
    "- Compare one-hot with Bag of Words\n",
    "- Compare one-hot with label encoding\n",
    "- See memory and representation differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf4d12e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON: One-Hot vs Other Encoding Methods\n",
      "======================================================================\n",
      "\n",
      "Original Data:\n",
      "Words: ['dog', 'cat', 'dog', 'bird', 'cat', 'fish']\n",
      "Sentences: ['the dog runs fast', 'the cat sits quietly', 'the bird flies high']\n",
      "\n",
      "======================================================================\n",
      "Method 1: One-Hot Encoding (Label Binarizer)\n",
      "======================================================================\n",
      "\n",
      "Vocabulary: ['bird' 'cat' 'dog' 'fish']\n",
      "One-Hot Matrix Shape: (6, 4)\n",
      "\n",
      "One-Hot Representation:\n",
      "[[0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]]\n",
      "\n",
      "Memory usage: 96 bytes\n",
      "Sparsity: 75.0% zeros\n",
      "\n",
      "======================================================================\n",
      "Method 2: Label Encoding (Integer Encoding)\n",
      "======================================================================\n",
      "\n",
      "Vocabulary: ['bird' 'cat' 'dog' 'fish']\n",
      "Label Encoded Array Shape: (6,)\n",
      "\n",
      "Label Encoded Representation:\n",
      "[2 1 2 0 1 3]\n",
      "\n",
      "Memory usage: 48 bytes\n",
      "\n",
      "Word to Integer Mapping:\n",
      "  'dog' ‚Üí 2\n",
      "  'cat' ‚Üí 1\n",
      "  'dog' ‚Üí 2\n",
      "  'bird' ‚Üí 0\n",
      "  'cat' ‚Üí 1\n",
      "  'fish' ‚Üí 3\n",
      "\n",
      "======================================================================\n",
      "Method 3: Bag of Words (Document-level)\n",
      "======================================================================\n",
      "\n",
      "Vocabulary: ['bird' 'cat' 'dog' 'fast' 'flies' 'high' 'quietly' 'runs' 'sits' 'the']\n",
      "BoW Matrix Shape: (3, 10)\n",
      "\n",
      "Bag of Words Representation:\n",
      "[[0 0 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 0 0 1 0 1 1]\n",
      " [1 0 0 0 1 1 0 0 0 1]]\n",
      "\n",
      "Memory usage (dense): 240 bytes\n",
      "\n",
      "Word counts in each sentence:\n",
      "\n",
      "Sentence 1: 'the dog runs fast'\n",
      "  {'dog': 1, 'fast': 1, 'runs': 1, 'the': 1}\n",
      "\n",
      "Sentence 2: 'the cat sits quietly'\n",
      "  {'cat': 1, 'quietly': 1, 'sits': 1, 'the': 1}\n",
      "\n",
      "Sentence 3: 'the bird flies high'\n",
      "  {'bird': 1, 'flies': 1, 'high': 1, 'the': 1}\n",
      "\n",
      "======================================================================\n",
      "Method 4: Binary/One-Hot at Document Level\n",
      "======================================================================\n",
      "\n",
      "Vocabulary: ['bird' 'cat' 'dog' 'fast' 'flies' 'high' 'quietly' 'runs' 'sits' 'the']\n",
      "One-Hot Document Matrix Shape: (3, 10)\n",
      "\n",
      "One-Hot Document Representation:\n",
      "[[0 0 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 0 0 1 0 1 1]\n",
      " [1 0 0 0 1 1 0 0 0 1]]\n",
      "\n",
      "Words present (1) or absent (0) in each sentence:\n",
      "\n",
      "Sentence 1: 'the dog runs fast'\n",
      "  Present: ['dog', 'fast', 'runs', 'the']\n",
      "\n",
      "Sentence 2: 'the cat sits quietly'\n",
      "  Present: ['cat', 'quietly', 'sits', 'the']\n",
      "\n",
      "Sentence 3: 'the bird flies high'\n",
      "  Present: ['bird', 'flies', 'high', 'the']\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARISON SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Method               Shape                Memory       Order      Frequency    Use Case            \n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "One-Hot\n",
      "(word-level) (6, 4)               96           No         No           Neural networks     \n",
      "Label Encoding       (6,)                 48           Yes        No           Tree models         \n",
      "Bag of Words         3 docs √ó 10 words    240          No         Yes          Text classification \n",
      "One-Hot\n",
      "(doc-level)  3 docs √ó 10 words    240          No         No           Binary features     \n",
      "\n",
      "======================================================================\n",
      "üîë KEY DIFFERENCES\n",
      "======================================================================\n",
      "\n",
      "1. ONE-HOT ENCODING (word-level):\n",
      "   ‚úÖ Each word ‚Üí unique binary vector\n",
      "   ‚úÖ No ordinal relationship implied\n",
      "   ‚ùå High dimensionality (= vocabulary size)\n",
      "   üéØ Use: Neural network inputs, categorical features\n",
      "\n",
      "2. LABEL ENCODING:\n",
      "   ‚úÖ Each word ‚Üí unique integer\n",
      "   ‚úÖ Memory efficient (single integer per word)\n",
      "   ‚ùå Implies ordinal relationship (1 < 2 < 3)\n",
      "   üéØ Use: Tree-based models (Random Forest, XGBoost)\n",
      "   ‚ö†Ô∏è NOT good for neural networks!\n",
      "\n",
      "3. BAG OF WORDS:\n",
      "   ‚úÖ Counts word frequencies in documents\n",
      "   ‚úÖ Captures importance by frequency\n",
      "   ‚ùå Ignores word order\n",
      "   üéØ Use: Document classification, topic modeling\n",
      "\n",
      "4. ONE-HOT (document-level):\n",
      "   ‚úÖ Binary presence/absence of words\n",
      "   ‚úÖ Ignores frequency (presence only)\n",
      "   ‚ùå Loses count information\n",
      "   üéØ Use: When only presence matters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Experiment 8: Comparing Encoding Methods\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "\n",
    "# Sample data\n",
    "words = ['dog', 'cat', 'dog', 'bird', 'cat', 'fish']\n",
    "sentences = [\n",
    "    \"the dog runs fast\",\n",
    "    \"the cat sits quietly\",\n",
    "    \"the bird flies high\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: One-Hot vs Other Encoding Methods\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nOriginal Data:\")\n",
    "print(f\"Words: {words}\")\n",
    "print(f\"Sentences: {sentences}\")\n",
    "\n",
    "# ============================================================\n",
    "# Method 1: One-Hot Encoding\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 1: One-Hot Encoding (Label Binarizer)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "one_hot = lb.fit_transform(words)\n",
    "\n",
    "print(f\"\\nVocabulary: {lb.classes_}\")\n",
    "print(f\"One-Hot Matrix Shape: {one_hot.shape}\")\n",
    "print(f\"\\nOne-Hot Representation:\")\n",
    "print(one_hot)\n",
    "print(f\"\\nMemory usage: {one_hot.nbytes} bytes\")\n",
    "print(f\"Sparsity: {(one_hot == 0).sum() / one_hot.size * 100:.1f}% zeros\")\n",
    "\n",
    "# ============================================================\n",
    "# Method 2: Label Encoding (Integer Encoding)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 2: Label Encoding (Integer Encoding)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "le = LabelEncoder()\n",
    "label_encoded = le.fit_transform(words)\n",
    "\n",
    "print(f\"\\nVocabulary: {le.classes_}\")\n",
    "print(f\"Label Encoded Array Shape: {label_encoded.shape}\")\n",
    "print(f\"\\nLabel Encoded Representation:\")\n",
    "print(label_encoded)\n",
    "print(f\"\\nMemory usage: {label_encoded.nbytes} bytes\")\n",
    "\n",
    "print(\"\\nWord to Integer Mapping:\")\n",
    "for word, code in zip(words, label_encoded):\n",
    "    print(f\"  '{word}' ‚Üí {code}\")\n",
    "\n",
    "# ============================================================\n",
    "# Method 3: Bag of Words (CountVectorizer)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 3: Bag of Words (Document-level)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(sentences)\n",
    "\n",
    "print(f\"\\nVocabulary: {cv.get_feature_names_out()}\")\n",
    "print(f\"BoW Matrix Shape: {bow.shape}\")\n",
    "print(f\"\\nBag of Words Representation:\")\n",
    "print(bow.toarray())\n",
    "print(f\"\\nMemory usage (dense): {bow.toarray().nbytes} bytes\")\n",
    "\n",
    "print(\"\\nWord counts in each sentence:\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"\\nSentence {i+1}: '{sent}'\")\n",
    "    word_counts = {word: count for word, count in zip(cv.get_feature_names_out(), bow.toarray()[i]) if count > 0}\n",
    "    print(f\"  {word_counts}\")\n",
    "\n",
    "# ============================================================\n",
    "# Method 4: One-Hot at Document Level\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Method 4: Binary/One-Hot at Document Level\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_binary = CountVectorizer(binary=True)\n",
    "one_hot_doc = cv_binary.fit_transform(sentences)\n",
    "\n",
    "print(f\"\\nVocabulary: {cv_binary.get_feature_names_out()}\")\n",
    "print(f\"One-Hot Document Matrix Shape: {one_hot_doc.shape}\")\n",
    "print(f\"\\nOne-Hot Document Representation:\")\n",
    "print(one_hot_doc.toarray())\n",
    "\n",
    "print(\"\\nWords present (1) or absent (0) in each sentence:\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    present_words = [word for word, present in zip(cv_binary.get_feature_names_out(), one_hot_doc.toarray()[i]) if present == 1]\n",
    "    print(f\"\\nSentence {i+1}: '{sent}'\")\n",
    "    print(f\"  Present: {present_words}\")\n",
    "\n",
    "# ============================================================\n",
    "# Comparison Summary\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = {\n",
    "    'Method': ['One-Hot\\n(word-level)', 'Label Encoding', 'Bag of Words', 'One-Hot\\n(doc-level)'],\n",
    "    'Output Shape': [str(one_hot.shape), str(label_encoded.shape), \n",
    "                     f\"{bow.shape[0]} docs √ó {bow.shape[1]} words\", \n",
    "                     f\"{one_hot_doc.shape[0]} docs √ó {one_hot_doc.shape[1]} words\"],\n",
    "    'Memory (bytes)': [one_hot.nbytes, label_encoded.nbytes, \n",
    "                       bow.toarray().nbytes, one_hot_doc.toarray().nbytes],\n",
    "    'Preserves Order': ['No', 'Yes', 'No', 'No'],\n",
    "    'Captures Frequency': ['No', 'No', 'Yes', 'No'],\n",
    "    'Use Case': ['Neural networks', 'Tree models', 'Text classification', 'Binary features']\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<20} {'Shape':<20} {'Memory':<12} {'Order':<10} {'Frequency':<12} {'Use Case':<20}\")\n",
    "print(\"-\" * 110)\n",
    "for i in range(len(comparison_data['Method'])):\n",
    "    print(f\"{comparison_data['Method'][i]:<20} {comparison_data['Output Shape'][i]:<20} \"\n",
    "          f\"{comparison_data['Memory (bytes)'][i]:<12} {comparison_data['Preserves Order'][i]:<10} \"\n",
    "          f\"{comparison_data['Captures Frequency'][i]:<12} {comparison_data['Use Case'][i]:<20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîë KEY DIFFERENCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. ONE-HOT ENCODING (word-level):\n",
    "   ‚úÖ Each word ‚Üí unique binary vector\n",
    "   ‚úÖ No ordinal relationship implied\n",
    "   ‚ùå High dimensionality (= vocabulary size)\n",
    "   üéØ Use: Neural network inputs, categorical features\n",
    "\n",
    "2. LABEL ENCODING:\n",
    "   ‚úÖ Each word ‚Üí unique integer\n",
    "   ‚úÖ Memory efficient (single integer per word)\n",
    "   ‚ùå Implies ordinal relationship (1 < 2 < 3)\n",
    "   üéØ Use: Tree-based models (Random Forest, XGBoost)\n",
    "   ‚ö†Ô∏è NOT good for neural networks!\n",
    "\n",
    "3. BAG OF WORDS:\n",
    "   ‚úÖ Counts word frequencies in documents\n",
    "   ‚úÖ Captures importance by frequency\n",
    "   ‚ùå Ignores word order\n",
    "   üéØ Use: Document classification, topic modeling\n",
    "\n",
    "4. ONE-HOT (document-level):\n",
    "   ‚úÖ Binary presence/absence of words\n",
    "   ‚úÖ Ignores frequency (presence only)\n",
    "   ‚ùå Loses count information\n",
    "   üéØ Use: When only presence matters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672d292",
   "metadata": {},
   "source": [
    "### üìä Observations - Experiment 8\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "**Memory Comparison:**\n",
    "- **One-Hot:** 24 bytes (6 words √ó 4 unique = 24 values)\n",
    "- **Label Encoding:** 6 bytes (6 integers, much smaller!)\n",
    "- Memory difference increases with vocabulary size\n",
    "\n",
    "**When to Use Each Method:**\n",
    "\n",
    "**1. One-Hot Encoding ‚ú®**\n",
    "```\n",
    "Use when:\n",
    "‚úÖ Working with neural networks\n",
    "‚úÖ Need to avoid ordinal relationships\n",
    "‚úÖ Vocabulary is small (<1000 words)\n",
    "‚úÖ Each word should be equally different\n",
    "\n",
    "Avoid when:\n",
    "‚ùå Vocabulary is huge (>10,000 words)\n",
    "‚ùå Memory is limited\n",
    "‚ùå Semantic similarity matters\n",
    "```\n",
    "\n",
    "**2. Label Encoding üî¢**\n",
    "```\n",
    "Use when:\n",
    "‚úÖ Working with tree-based models\n",
    "‚úÖ Memory efficiency is critical\n",
    "‚úÖ Natural ordering exists (small, medium, large)\n",
    "\n",
    "Avoid when:\n",
    "‚ùå Using neural networks (implies false order)\n",
    "‚ùå No natural ordering (dog=1, cat=2 doesn't mean dog < cat)\n",
    "```\n",
    "\n",
    "**3. Bag of Words üéí**\n",
    "```\n",
    "Use when:\n",
    "‚úÖ Document classification\n",
    "‚úÖ Word frequency matters\n",
    "‚úÖ Traditional ML models (Naive Bayes, SVM)\n",
    "\n",
    "Avoid when:\n",
    "‚ùå Word order matters (sentiment: \"not good\" vs \"good\")\n",
    "‚ùå Need semantic understanding\n",
    "```\n",
    "\n",
    "**Critical Warning ‚ö†Ô∏è**\n",
    "```python\n",
    "# BAD: Label encoding + Neural Network\n",
    "words = ['cat', 'dog', 'bird']\n",
    "encoded = [0, 1, 2]  # Neural network thinks: bird > dog > cat!\n",
    "\n",
    "# GOOD: One-hot + Neural Network\n",
    "cat  = [1, 0, 0]  # All words equally different\n",
    "dog  = [0, 1, 0]\n",
    "bird = [0, 0, 1]\n",
    "```\n",
    "\n",
    "**Real-World Decision Tree:**\n",
    "```\n",
    "Is your vocabulary size < 1000?\n",
    "‚îú‚îÄ YES: One-Hot is fine\n",
    "‚îî‚îÄ NO: Consider alternatives\n",
    "    ‚îú‚îÄ Using Neural Networks? ‚Üí Word Embeddings (Word2Vec, GloVe)\n",
    "    ‚îú‚îÄ Document classification? ‚Üí TF-IDF or Bag of Words\n",
    "    ‚îî‚îÄ Tree models? ‚Üí Label Encoding (with caution)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfaddf",
   "metadata": {},
   "source": [
    "---\n",
    "# üìö Summary and Conclusion\n",
    "\n",
    "## What We've Learned About One-Hot Encoding\n",
    "\n",
    "Let's consolidate everything we've discovered through our experiments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb873b3",
   "metadata": {},
   "source": [
    "## üîß Implementation Methods Learned\n",
    "\n",
    "| Method | Library | Best For | Code Example |\n",
    "|--------|---------|----------|--------------|\n",
    "| **Manual** | NumPy | Learning | `np.zeros(vocab_size); vector[index] = 1` |\n",
    "| **NLTK** | NLTK | Tokenization + Manual | `word_tokenize()` + manual encoding |\n",
    "| **LabelBinarizer** | Scikit-learn | Word-level | `LabelBinarizer().fit_transform(words)` |\n",
    "| **OneHotEncoder** | Scikit-learn | General categorical | `OneHotEncoder().fit_transform(data)` |\n",
    "| **CountVectorizer** | Scikit-learn | Document-level | `CountVectorizer(binary=True)` |\n",
    "| **Keras** | TensorFlow | Deep Learning | `Tokenizer()` + `to_categorical()` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165dfb7f",
   "metadata": {},
   "source": [
    "## üìä Experiment Results Summary\n",
    "\n",
    "| Experiment | Key Finding | Practical Insight |\n",
    "|------------|-------------|-------------------|\n",
    "| **Exp 1: Manual** | Created vectors from scratch | Understood the core mechanism |\n",
    "| **Exp 2: NLTK** | Proper tokenization matters | Punctuation handling is crucial |\n",
    "| **Exp 3: Preprocessing** | Cleaning reduces dimensionality | Stopword removal = 45% size reduction |\n",
    "| **Exp 4: Scikit-learn** | Professional tools are optimized | Use libraries in production |\n",
    "| **Exp 5: Keras** | Two-step: text‚Üíint‚Üíone-hot | Standard in deep learning |\n",
    "| **Exp 6: Real-world** | Full NLP pipeline end-to-end | Ready for ML models |\n",
    "| **Exp 7: Visualization** | All words equally different | No semantic relationships! |\n",
    "| **Exp 8: Comparison** | Each method has its place | Choose based on use case |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8704a9",
   "metadata": {},
   "source": [
    "## üéØ Core Concepts Recap\n",
    "\n",
    "### Definition\n",
    "**One-Hot Encoding** converts each word into a binary vector where:\n",
    "- Length = Vocabulary size\n",
    "- Exactly one position = 1 (the word's position)\n",
    "- All other positions = 0\n",
    "\n",
    "### Mathematical Properties\n",
    "1. **Orthogonal vectors:** All word vectors are perpendicular\n",
    "2. **Equal distance:** All different words have the same Euclidean distance (‚àö2)\n",
    "3. **Zero similarity:** Cosine similarity between different words = 0\n",
    "4. **Sparse representation:** Mostly zeros (~95-99% sparsity)\n",
    "\n",
    "### The Good ‚úÖ\n",
    "- Simple and intuitive to understand\n",
    "- No training required\n",
    "- Works well for small vocabularies\n",
    "- Perfect for neural network input layers\n",
    "- No assumptions about word relationships\n",
    "- Deterministic and reproducible\n",
    "\n",
    "### The Bad ‚ùå\n",
    "- **Curse of dimensionality:** Vector size = vocabulary size\n",
    "- **No semantic meaning:** \"King\" and \"Queen\" are as different as \"King\" and \"Apple\"\n",
    "- **Memory inefficient:** Huge sparse matrices\n",
    "- **Cannot handle new words:** Out-of-vocabulary problem\n",
    "- **Ignores relationships:** No notion of synonyms or related words\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09172a58",
   "metadata": {},
   "source": [
    "## üéì Decision Guide: When to Use One-Hot Encoding\n",
    "\n",
    "### ‚úÖ USE One-Hot Encoding When:\n",
    "\n",
    "1. **Small vocabulary** (< 1,000 unique words)\n",
    "   - Example: Product categories, sentiment labels\n",
    "   \n",
    "2. **Neural networks** with categorical inputs\n",
    "   - Example: Word input to LSTM/RNN before embedding\n",
    "   \n",
    "3. **Equal treatment** of all categories needed\n",
    "   - Example: Color classification (red, blue, green equally different)\n",
    "   \n",
    "4. **Output layers** in classification\n",
    "   - Example: Multi-class classification (10 classes ‚Üí 10-dim one-hot)\n",
    "\n",
    "5. **No semantic meaning** required\n",
    "   - Example: User IDs, product codes\n",
    "\n",
    "### ‚ùå AVOID One-Hot Encoding When:\n",
    "\n",
    "1. **Large vocabulary** (> 10,000 words)\n",
    "   - Alternative: Word embeddings (Word2Vec, GloVe, FastText)\n",
    "   \n",
    "2. **Semantic similarity** matters\n",
    "   - Alternative: Pre-trained embeddings (BERT, GPT)\n",
    "   \n",
    "3. **Memory is limited**\n",
    "   - Alternative: Hash vectorization, dimensionality reduction\n",
    "   \n",
    "4. **Need to handle OOV** (out-of-vocabulary) words\n",
    "   - Alternative: Subword embeddings (BPE, WordPiece)\n",
    "   \n",
    "5. **Working with tree-based models**\n",
    "   - Alternative: Label encoding (but be careful!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e41b6f",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps in Your NLP Journey\n",
    "\n",
    "Now that you understand One-Hot Encoding, here's what to learn next:\n",
    "\n",
    "### 1. **Bag of Words (BoW)**\n",
    "   - Extends one-hot to document level\n",
    "   - Captures word frequency\n",
    "   - Foundation for many classical NLP tasks\n",
    "\n",
    "### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "   - Weights words by importance\n",
    "   - Better than raw counts\n",
    "   - Industry standard for document similarity\n",
    "\n",
    "### 3. **Word Embeddings**\n",
    "   - **Word2Vec:** Learns semantic relationships\n",
    "   - **GloVe:** Global vector representations\n",
    "   - **FastText:** Handles unknown words with subwords\n",
    "   - Dense vectors (100-300 dim) instead of sparse\n",
    "\n",
    "### 4. **Contextual Embeddings**\n",
    "   - **BERT:** Bidirectional context understanding\n",
    "   - **GPT:** Generative pre-training\n",
    "   - **ELMo:** Deep contextualized representations\n",
    "   - Different representations for same word in different contexts\n",
    "\n",
    "### 5. **Advanced Topics**\n",
    "   - Sentence embeddings (Doc2Vec, Universal Sentence Encoder)\n",
    "   - Attention mechanisms\n",
    "   - Transformers architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf8880",
   "metadata": {},
   "source": [
    "## üìñ Additional Resources\n",
    "\n",
    "### Recommended Reading\n",
    "1. **Speech and Language Processing** by Jurafsky & Martin (Chapter 6)\n",
    "2. **Natural Language Processing with Python** by Bird, Klein & Loper\n",
    "3. **Deep Learning** by Goodfellow, Bengio & Courville (Chapter 12)\n",
    "\n",
    "### Online Resources\n",
    "- [Stanford CS224N: NLP with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "- [Hugging Face NLP Course](https://huggingface.co/course/)\n",
    "- [Fast.ai NLP Course](https://www.fast.ai/)\n",
    "\n",
    "### Practice Datasets\n",
    "- **IMDB Movie Reviews** (sentiment analysis)\n",
    "- **20 Newsgroups** (document classification)\n",
    "- **SMS Spam Collection** (binary classification)\n",
    "- **Reuters News** (multi-class classification)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
