{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0589241",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) - Text to Vector Representation\n",
    "\n",
    "## üìö Table of Contents\n",
    "1. Introduction to Bag of Words\n",
    "2. How Bag of Words Works\n",
    "3. Prerequisites and Setup\n",
    "4. Basic Implementation\n",
    "5. Advanced Implementations\n",
    "6. Applications and Use Cases\n",
    "7. Advantages and Limitations\n",
    "8. Comparison with Other Techniques\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Bag of Words\n",
    "\n",
    "**Bag of Words (BoW)** is one of the simplest and most widely used techniques for converting text into numerical vectors that machine learning algorithms can understand.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Text Vectorization**: Converting text data into numerical format\n",
    "- **Feature Extraction**: Extracting meaningful features from text\n",
    "- **Vocabulary**: Collection of unique words in the corpus\n",
    "- **Document-Term Matrix**: Matrix representation of documents and word frequencies\n",
    "\n",
    "### Why Bag of Words?\n",
    "- Simple to understand and implement\n",
    "- Computationally efficient\n",
    "- Works well for many NLP tasks\n",
    "- Foundation for understanding advanced techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4ef0b",
   "metadata": {},
   "source": [
    "## 2. How Bag of Words Works\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "#### Step 1: **Tokenization**\n",
    "Break down text into individual words (tokens)\n",
    "```\n",
    "Text: \"The cat sat on the mat\"\n",
    "Tokens: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "```\n",
    "\n",
    "#### Step 2: **Build Vocabulary**\n",
    "Create a list of unique words from all documents\n",
    "```\n",
    "Vocabulary: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"dog\", \"ran\"]\n",
    "```\n",
    "\n",
    "#### Step 3: **Create Vector Representation**\n",
    "Count the frequency of each word in each document\n",
    "```\n",
    "Document: \"The cat sat on the mat\"\n",
    "Vector: [2, 1, 1, 1, 1, 0, 0]  # [the, cat, sat, on, mat, dog, ran]\n",
    "```\n",
    "\n",
    "### Important Note:\n",
    "- **\"Bag\"** means we **ignore the order** of words\n",
    "- We only care about **word frequency**\n",
    "- Different documents can be compared using these vectors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08227d",
   "metadata": {},
   "source": [
    "## 3. Prerequisites and Setup\n",
    "\n",
    "Let's import all necessary libraries for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74dbc0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "‚úÖ NLTK data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"‚úÖ NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3858bdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Basic Implementation - Experiment 1\n",
    "\n",
    "### üî¨ Experiment 1: Simple Bag of Words (Manual Implementation)\n",
    "\n",
    "**Objective**: Understand the basic concept by manually creating a Bag of Words representation.\n",
    "\n",
    "**Dataset**: Simple sentences about animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c60ff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original Documents:\n",
      "--------------------------------------------------\n",
      "Document 1: The cat sat on the mat\n",
      "Document 2: The dog sat on the log\n",
      "Document 3: Cats and dogs are great pets\n",
      "Document 4: I love my cat and dog\n"
     ]
    }
   ],
   "source": [
    "# Sample documents (corpus)\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"Cats and dogs are great pets\",\n",
    "    \"I love my cat and dog\"\n",
    "]\n",
    "\n",
    "print(\"üìÑ Original Documents:\")\n",
    "print(\"-\" * 50)\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"Document {i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70198ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ Tokenized Documents:\n",
      "--------------------------------------------------\n",
      "Document 1: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Document 2: ['the', 'dog', 'sat', 'on', 'the', 'log']\n",
      "Document 3: ['cats', 'and', 'dogs', 'are', 'great', 'pets']\n",
      "Document 4: ['i', 'love', 'my', 'cat', 'and', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization and Preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase and tokenize\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize using NLTK\n",
    "    return tokens\n",
    "\n",
    "# Tokenize all documents\n",
    "tokenized_docs = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "print(\"\\nüî§ Tokenized Documents:\")\n",
    "print(\"-\" * 50)\n",
    "for i, tokens in enumerate(tokenized_docs, 1):\n",
    "    print(f\"Document {i}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3673e90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Vocabulary (Unique Words):\n",
      "--------------------------------------------------\n",
      "Total unique words: 16\n",
      "Vocabulary: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'great', 'i', 'log', 'love', 'mat', 'my', 'on', 'pets', 'sat', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build Vocabulary\n",
    "def build_vocabulary(tokenized_documents):\n",
    "    \"\"\"Create a vocabulary of unique words from all documents\"\"\"\n",
    "    vocabulary = set()\n",
    "    for tokens in tokenized_documents:\n",
    "        vocabulary.update(tokens)\n",
    "    return sorted(list(vocabulary))  # Sort for consistency\n",
    "\n",
    "vocabulary = build_vocabulary(tokenized_docs)\n",
    "\n",
    "print(\"\\nüìñ Vocabulary (Unique Words):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total unique words: {len(vocabulary)}\")\n",
    "print(f\"Vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d27e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Bag of Words Vectors:\n",
      "--------------------------------------------------\n",
      "Document 1: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 2]\n",
      "Document 2: [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2]\n",
      "Document 3: [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "Document 4: [1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create Bag of Words Vectors\n",
    "def create_bow_vector(tokens, vocabulary):\n",
    "    \"\"\"Create a BoW vector for a document\"\"\"\n",
    "    vector = []\n",
    "    for word in vocabulary:\n",
    "        vector.append(tokens.count(word))  # Count frequency of each word\n",
    "    return vector\n",
    "\n",
    "# Create BoW vectors for all documents\n",
    "bow_vectors = [create_bow_vector(tokens, vocabulary) for tokens in tokenized_docs]\n",
    "\n",
    "print(\"\\nüéØ Bag of Words Vectors:\")\n",
    "print(\"-\" * 50)\n",
    "for i, vector in enumerate(bow_vectors, 1):\n",
    "    print(f\"Document {i}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eef4176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Document-Term Matrix (Bag of Words Representation):\n",
      "--------------------------------------------------------------------------------\n",
      "       and  are  cat  cats  dog  dogs  great  i  log  love  mat  my  on  pets  \\\n",
      "Doc 1    0    0    1     0    0     0      0  0    0     0    1   0   1     0   \n",
      "Doc 2    0    0    0     0    1     0      0  0    1     0    0   0   1     0   \n",
      "Doc 3    1    1    0     1    0     1      1  0    0     0    0   0   0     1   \n",
      "Doc 4    1    0    1     0    1     0      0  1    0     1    0   1   0     0   \n",
      "\n",
      "       sat  the  \n",
      "Doc 1    1    2  \n",
      "Doc 2    1    2  \n",
      "Doc 3    0    0  \n",
      "Doc 4    0    0  \n",
      "\n",
      "üìè Matrix Shape: (4, 16)\n",
      "   - Rows (Documents): 4\n",
      "   - Columns (Vocabulary): 16\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create a Document-Term Matrix (DataFrame for better visualization)\n",
    "bow_df = pd.DataFrame(bow_vectors, columns=vocabulary)\n",
    "bow_df.index = [f\"Doc {i}\" for i in range(1, len(documents) + 1)]\n",
    "\n",
    "print(\"\\nüìä Document-Term Matrix (Bag of Words Representation):\")\n",
    "print(\"-\" * 80)\n",
    "print(bow_df)\n",
    "\n",
    "# Show dimensions\n",
    "print(f\"\\nüìè Matrix Shape: {bow_df.shape}\")\n",
    "print(f\"   - Rows (Documents): {bow_df.shape[0]}\")\n",
    "print(f\"   - Columns (Vocabulary): {bow_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175a4f5",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 1:\n",
    "\n",
    "1. **Tokenization**: Text was broken down into individual words (tokens)\n",
    "2. **Vocabulary Size**: We got unique words from all documents\n",
    "3. **Vector Representation**: Each document is now represented as a numerical vector\n",
    "4. **Word Order Lost**: The order of words is not preserved (that's why it's called \"Bag\" of words)\n",
    "5. **Sparse Matrix**: Many zeros in the matrix (words that don't appear in certain documents)\n",
    "6. **Word Frequency**: The numbers represent how many times each word appears in each document\n",
    "\n",
    "**Key Insight**: Each document is now a vector of numbers, making it suitable for machine learning algorithms!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978447c",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 2: Bag of Words with Stop Words Removal\n",
    "\n",
    "**Objective**: Improve the BoW representation by removing common words (stop words) that don't add much meaning.\n",
    "\n",
    "**Why Remove Stop Words?**\n",
    "- Words like \"the\", \"is\", \"on\", \"and\" appear frequently but carry little meaning\n",
    "- Removing them reduces vocabulary size and focuses on meaningful words\n",
    "- Improves model efficiency and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df150eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Stop Words Sample:\n",
      "--------------------------------------------------\n",
      "Total stop words: 198\n",
      "Sample stop words: ['didn', 'it', 'once', \"they're\", 'am', 'as', 'i', 'shouldn', 'm', 'don', \"she'd\", 'theirs', 'not', 'yours', \"you've\", \"it's\", 'why', \"she'll\", \"doesn't\", 'having']\n"
     ]
    }
   ],
   "source": [
    "# Get English stop words from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"üõë Stop Words Sample:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total stop words: {len(stop_words)}\")\n",
    "print(f\"Sample stop words: {list(stop_words)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf62bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî§ Tokenized Documents (After Stop Words Removal):\n",
      "--------------------------------------------------\n",
      "Document 1: ['cat', 'sat', 'mat']\n",
      "Document 2: ['dog', 'sat', 'log']\n",
      "Document 3: ['cats', 'dogs', 'great', 'pets']\n",
      "Document 4: ['love', 'cat', 'dog']\n",
      "\n",
      "üìä Comparison:\n",
      "--------------------------------------------------\n",
      "Original tokens in Doc 1: 6\n",
      "Filtered tokens in Doc 1: 3\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess with stop words removal\n",
    "def preprocess_with_stopwords_removal(text):\n",
    "    \"\"\"Tokenize, lowercase, and remove stop words\"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and punctuation\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply preprocessing with stop words removal\n",
    "tokenized_docs_filtered = [preprocess_with_stopwords_removal(doc) for doc in documents]\n",
    "\n",
    "print(\"\\nüî§ Tokenized Documents (After Stop Words Removal):\")\n",
    "print(\"-\" * 50)\n",
    "for i, tokens in enumerate(tokenized_docs_filtered, 1):\n",
    "    print(f\"Document {i}: {tokens}\")\n",
    "    \n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original tokens in Doc 1: {len(tokenized_docs[0])}\")\n",
    "print(f\"Filtered tokens in Doc 1: {len(tokenized_docs_filtered[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b20618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Document-Term Matrix (After Stop Words Removal):\n",
      "--------------------------------------------------------------------------------\n",
      "       cat  cats  dog  dogs  great  log  love  mat  pets  sat\n",
      "Doc 1    1     0    0     0      0    0     0    1     0    1\n",
      "Doc 2    0     0    1     0      0    1     0    0     0    1\n",
      "Doc 3    0     1    0     1      1    0     0    0     1    0\n",
      "Doc 4    1     0    1     0      0    0     1    0     0    0\n",
      "\n",
      "üìè Matrix Shape Comparison:\n",
      "   - Original: (4, 16) (Docs √ó Vocabulary)\n",
      "   - Filtered: (4, 10) (Docs √ó Vocabulary)\n",
      "   - Vocabulary reduced by: 6 words\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary and create BoW vectors\n",
    "vocabulary_filtered = build_vocabulary(tokenized_docs_filtered)\n",
    "bow_vectors_filtered = [create_bow_vector(tokens, vocabulary_filtered) for tokens in tokenized_docs_filtered]\n",
    "\n",
    "# Create DataFrame\n",
    "bow_df_filtered = pd.DataFrame(bow_vectors_filtered, columns=vocabulary_filtered)\n",
    "bow_df_filtered.index = [f\"Doc {i}\" for i in range(1, len(documents) + 1)]\n",
    "\n",
    "print(\"\\nüìä Document-Term Matrix (After Stop Words Removal):\")\n",
    "print(\"-\" * 80)\n",
    "print(bow_df_filtered)\n",
    "\n",
    "print(f\"\\nüìè Matrix Shape Comparison:\")\n",
    "print(f\"   - Original: {bow_df.shape} (Docs √ó Vocabulary)\")\n",
    "print(f\"   - Filtered: {bow_df_filtered.shape} (Docs √ó Vocabulary)\")\n",
    "print(f\"   - Vocabulary reduced by: {bow_df.shape[1] - bow_df_filtered.shape[1]} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebf6d1",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 2:\n",
    "\n",
    "1. **Vocabulary Reduction**: Vocabulary size decreased significantly after removing stop words\n",
    "2. **More Meaningful Features**: Remaining words carry more semantic meaning (cat, dog, sat, log, pets, love)\n",
    "3. **Cleaner Representation**: Less noise from common words\n",
    "4. **Focused Analysis**: Model can focus on words that actually differentiate documents\n",
    "5. **Computational Efficiency**: Smaller vocabulary means faster processing and less memory\n",
    "\n",
    "**Key Insight**: Stop words removal is a crucial preprocessing step in text analysis!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b580cca",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 3: Bag of Words with Stemming\n",
    "\n",
    "**Objective**: Apply stemming to reduce words to their root form and further reduce vocabulary.\n",
    "\n",
    "**What is Stemming?**\n",
    "- Reduces words to their base/root form (e.g., \"running\" ‚Üí \"run\", \"cats\" ‚Üí \"cat\")\n",
    "- Helps group related words together\n",
    "- Further reduces vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2284669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenized Documents (After Stemming):\n",
      "--------------------------------------------------\n",
      "Document 1: ['cat', 'sat', 'mat']\n",
      "Document 2: ['dog', 'sat', 'log']\n",
      "Document 3: ['cat', 'dog', 'great', 'pet']\n",
      "Document 4: ['love', 'cat', 'dog']\n",
      "\n",
      "üå± Stemming Examples:\n",
      "--------------------------------------------------\n",
      "cats ‚Üí cat\n",
      "dogs ‚Üí dog\n",
      "sitting ‚Üí sit\n",
      "loved ‚Üí love\n",
      "running ‚Üí run\n",
      "playing ‚Üí play\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess with stemming\n",
    "def preprocess_with_stemming(text):\n",
    "    \"\"\"Tokenize, lowercase, remove stop words, and apply stemming\"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and punctuation\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Apply preprocessing with stemming\n",
    "tokenized_docs_stemmed = [preprocess_with_stemming(doc) for doc in documents]\n",
    "\n",
    "print(\"üî§ Tokenized Documents (After Stemming):\")\n",
    "print(\"-\" * 50)\n",
    "for i, tokens in enumerate(tokenized_docs_stemmed, 1):\n",
    "    print(f\"Document {i}: {tokens}\")\n",
    "\n",
    "# Show stemming examples\n",
    "print(\"\\nüå± Stemming Examples:\")\n",
    "print(\"-\" * 50)\n",
    "example_words = [\"cats\", \"dogs\", \"sitting\", \"loved\", \"running\", \"playing\"]\n",
    "for word in example_words:\n",
    "    print(f\"{word} ‚Üí {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677c522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Document-Term Matrix (After Stemming):\n",
      "--------------------------------------------------------------------------------\n",
      "       cat  dog  great  log  love  mat  pet  sat\n",
      "Doc 1    1    0      0    0     0    1    0    1\n",
      "Doc 2    0    1      0    1     0    0    0    1\n",
      "Doc 3    1    1      1    0     0    0    1    0\n",
      "Doc 4    1    1      0    0     1    0    0    0\n",
      "\n",
      "üìè Matrix Shape Evolution:\n",
      "   - Original:            (4, 16) (Docs √ó Vocabulary)\n",
      "   - Stop Words Removed:  (4, 10) (Docs √ó Vocabulary)\n",
      "   - With Stemming:       (4, 8) (Docs √ó Vocabulary)\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary and create BoW vectors\n",
    "vocabulary_stemmed = build_vocabulary(tokenized_docs_stemmed)\n",
    "bow_vectors_stemmed = [create_bow_vector(tokens, vocabulary_stemmed) for tokens in tokenized_docs_stemmed]\n",
    "\n",
    "# Create DataFrame\n",
    "bow_df_stemmed = pd.DataFrame(bow_vectors_stemmed, columns=vocabulary_stemmed)\n",
    "bow_df_stemmed.index = [f\"Doc {i}\" for i in range(1, len(documents) + 1)]\n",
    "\n",
    "print(\"\\nüìä Document-Term Matrix (After Stemming):\")\n",
    "print(\"-\" * 80)\n",
    "print(bow_df_stemmed)\n",
    "\n",
    "print(f\"\\nüìè Matrix Shape Evolution:\")\n",
    "print(f\"   - Original:            {bow_df.shape} (Docs √ó Vocabulary)\")\n",
    "print(f\"   - Stop Words Removed:  {bow_df_filtered.shape} (Docs √ó Vocabulary)\")\n",
    "print(f\"   - With Stemming:       {bow_df_stemmed.shape} (Docs √ó Vocabulary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88040ccd",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 3:\n",
    "\n",
    "1. **Word Normalization**: \"cats\" and \"cat\" are now treated as the same word (\"cat\")\n",
    "2. **Vocabulary Reduction**: Similar words are grouped under their root form\n",
    "3. **Loss of Meaning**: Stemming can sometimes produce non-words (e.g., \"running\" ‚Üí \"run\")\n",
    "4. **Better Generalization**: Model treats related words similarly\n",
    "5. **Trade-off**: Gain efficiency but may lose some semantic information\n",
    "\n",
    "**Key Insight**: Stemming is useful for grouping related words, but it's a crude approach that may lose some meaning!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab6515",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 4: Bag of Words with Lemmatization\n",
    "\n",
    "**Objective**: Use lemmatization as a better alternative to stemming.\n",
    "\n",
    "**What is Lemmatization?**\n",
    "- Reduces words to their dictionary form (lemma)\n",
    "- More intelligent than stemming - considers context and part of speech\n",
    "- Produces actual words (not stems)\n",
    "- Example: \"running\" ‚Üí \"run\", \"better\" ‚Üí \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb44850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenized Documents (After Lemmatization):\n",
      "--------------------------------------------------\n",
      "Document 1: ['cat', 'sat', 'mat']\n",
      "Document 2: ['dog', 'sat', 'log']\n",
      "Document 3: ['cat', 'dog', 'great', 'pet']\n",
      "Document 4: ['love', 'cat', 'dog']\n",
      "\n",
      "üå± Stemming vs Lemmatization Comparison:\n",
      "--------------------------------------------------\n",
      "cats         ‚Üí Stem: cat          | Lemma: cat\n",
      "dogs         ‚Üí Stem: dog          | Lemma: dog\n",
      "running      ‚Üí Stem: run          | Lemma: running\n",
      "better       ‚Üí Stem: better       | Lemma: better\n",
      "caring       ‚Üí Stem: care         | Lemma: caring\n",
      "leaves       ‚Üí Stem: leav         | Lemma: leaf\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess with lemmatization\n",
    "def preprocess_with_lemmatization(text):\n",
    "    \"\"\"Tokenize, lowercase, remove stop words, and apply lemmatization\"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and punctuation\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Apply preprocessing with lemmatization\n",
    "tokenized_docs_lemmatized = [preprocess_with_lemmatization(doc) for doc in documents]\n",
    "\n",
    "print(\"üî§ Tokenized Documents (After Lemmatization):\")\n",
    "print(\"-\" * 50)\n",
    "for i, tokens in enumerate(tokenized_docs_lemmatized, 1):\n",
    "    print(f\"Document {i}: {tokens}\")\n",
    "\n",
    "# Compare stemming vs lemmatization\n",
    "print(\"\\nüå± Stemming vs Lemmatization Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "comparison_words = [\"cats\", \"dogs\", \"running\", \"better\", \"caring\", \"leaves\"]\n",
    "for word in comparison_words:\n",
    "    print(f\"{word:12} ‚Üí Stem: {stemmer.stem(word):12} | Lemma: {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7846f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Document-Term Matrix (After Lemmatization):\n",
      "--------------------------------------------------------------------------------\n",
      "       cat  dog  great  log  love  mat  pet  sat\n",
      "Doc 1    1    0      0    0     0    1    0    1\n",
      "Doc 2    0    1      0    1     0    0    0    1\n",
      "Doc 3    1    1      1    0     0    0    1    0\n",
      "Doc 4    1    1      0    0     1    0    0    0\n",
      "\n",
      "üìè Complete Matrix Shape Evolution:\n",
      "   - Original:            (4, 16) (Docs √ó Vocabulary)\n",
      "   - Stop Words Removed:  (4, 10) (Docs √ó Vocabulary)\n",
      "   - With Stemming:       (4, 8) (Docs √ó Vocabulary)\n",
      "   - With Lemmatization:  (4, 8) (Docs √ó Vocabulary)\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary and create BoW vectors\n",
    "vocabulary_lemmatized = build_vocabulary(tokenized_docs_lemmatized)\n",
    "bow_vectors_lemmatized = [create_bow_vector(tokens, vocabulary_lemmatized) for tokens in tokenized_docs_lemmatized]\n",
    "\n",
    "# Create DataFrame\n",
    "bow_df_lemmatized = pd.DataFrame(bow_vectors_lemmatized, columns=vocabulary_lemmatized)\n",
    "bow_df_lemmatized.index = [f\"Doc {i}\" for i in range(1, len(documents) + 1)]\n",
    "\n",
    "print(\"\\nüìä Document-Term Matrix (After Lemmatization):\")\n",
    "print(\"-\" * 80)\n",
    "print(bow_df_lemmatized)\n",
    "\n",
    "print(f\"\\nüìè Complete Matrix Shape Evolution:\")\n",
    "print(f\"   - Original:            {bow_df.shape} (Docs √ó Vocabulary)\")\n",
    "print(f\"   - Stop Words Removed:  {bow_df_filtered.shape} (Docs √ó Vocabulary)\")\n",
    "print(f\"   - With Stemming:       {bow_df_stemmed.shape} (Docs √ó Vocabulary)\")\n",
    "print(f\"   - With Lemmatization:  {bow_df_lemmatized.shape} (Docs √ó Vocabulary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e09db0",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 4:\n",
    "\n",
    "1. **Better Quality**: Lemmatization produces actual dictionary words\n",
    "2. **Context Aware**: Considers grammatical context (though basic lemmatization doesn't use POS tags)\n",
    "3. **More Accurate**: Preserves more semantic meaning compared to stemming\n",
    "4. **Computational Cost**: Slower than stemming but produces better results\n",
    "5. **Preferred Method**: Generally preferred over stemming in modern NLP applications\n",
    "\n",
    "**Key Insight**: Lemmatization is the preferred normalization technique when quality matters more than speed!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577538ca",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 5: Using Sklearn's CountVectorizer\n",
    "\n",
    "**Objective**: Use scikit-learn's CountVectorizer for more efficient and feature-rich BoW implementation.\n",
    "\n",
    "**Why CountVectorizer?**\n",
    "- Industry-standard implementation\n",
    "- Built-in preprocessing options\n",
    "- Efficient and optimized\n",
    "- Many configurable parameters\n",
    "- Easy to use and integrate with ML pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eaa7b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "METHOD 1: Basic CountVectorizer\n",
      "================================================================================\n",
      "\n",
      "üìä Document-Term Matrix (Basic CountVectorizer):\n",
      "--------------------------------------------------------------------------------\n",
      "       and  are  cat  cats  dog  dogs  great  log  love  mat  my  on  pets  \\\n",
      "Doc 1    0    0    1     0    0     0      0    0     0    1   0   1     0   \n",
      "Doc 2    0    0    0     0    1     0      0    1     0    0   0   1     0   \n",
      "Doc 3    1    1    0     1    0     1      1    0     0    0   0   0     1   \n",
      "Doc 4    1    0    1     0    1     0      0    0     1    0   1   0     0   \n",
      "\n",
      "       sat  the  \n",
      "Doc 1    1    2  \n",
      "Doc 2    1    2  \n",
      "Doc 3    0    0  \n",
      "Doc 4    0    0  \n",
      "\n",
      "üìè Vocabulary size: 15\n",
      "üìñ Vocabulary: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'great', 'log', 'love', 'mat', 'my', 'on', 'pets', 'sat', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Method 1: Basic CountVectorizer (no preprocessing)\n",
    "print(\"=\" * 80)\n",
    "print(\"METHOD 1: Basic CountVectorizer\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_basic = CountVectorizer()\n",
    "bow_sklearn_basic = cv_basic.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "bow_sklearn_basic_df = pd.DataFrame(\n",
    "    bow_sklearn_basic.toarray(),\n",
    "    columns=cv_basic.get_feature_names_out(),\n",
    "    index=[f\"Doc {i}\" for i in range(1, len(documents) + 1)]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Document-Term Matrix (Basic CountVectorizer):\")\n",
    "print(\"-\" * 80)\n",
    "print(bow_sklearn_basic_df)\n",
    "\n",
    "print(f\"\\nüìè Vocabulary size: {len(cv_basic.get_feature_names_out())}\")\n",
    "print(f\"üìñ Vocabulary: {list(cv_basic.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "326e70a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METHOD 2: CountVectorizer with Stop Words Removal\n",
      "================================================================================\n",
      "\n",
      "üìä Document-Term Matrix (With Stop Words Removal):\n",
      "--------------------------------------------------------------------------------\n",
      "       cat  cats  dog  dogs  great  log  love  mat  pets  sat\n",
      "Doc 1    1     0    0     0      0    0     0    1     0    1\n",
      "Doc 2    0     0    1     0      0    1     0    0     0    1\n",
      "Doc 3    0     1    0     1      1    0     0    0     1    0\n",
      "Doc 4    1     0    1     0      0    0     1    0     0    0\n",
      "\n",
      "üìè Vocabulary size: 10\n",
      "üìñ Vocabulary: ['cat', 'cats', 'dog', 'dogs', 'great', 'log', 'love', 'mat', 'pets', 'sat']\n"
     ]
    }
   ],
   "source": [
    "# Method 2: CountVectorizer with Stop Words Removal\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD 2: CountVectorizer with Stop Words Removal\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_stopwords = CountVectorizer(stop_words='english')\n",
    "bow_sklearn_stopwords = cv_stopwords.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "bow_sklearn_stopwords_df = pd.DataFrame(\n",
    "    bow_sklearn_stopwords.toarray(),\n",
    "    columns=cv_stopwords.get_feature_names_out(),\n",
    "    index=[f\"Doc {i}\" for i in range(1, len(documents) + 1)]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Document-Term Matrix (With Stop Words Removal):\")\n",
    "print(\"-\" * 80)\n",
    "print(bow_sklearn_stopwords_df)\n",
    "\n",
    "print(f\"\\nüìè Vocabulary size: {len(cv_stopwords.get_feature_names_out())}\")\n",
    "print(f\"üìñ Vocabulary: {list(cv_stopwords.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "416a6aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METHOD 3: CountVectorizer with Advanced Parameters\n",
      "================================================================================\n",
      "\n",
      "üìä Document-Term Matrix (Advanced Parameters with Bigrams):\n",
      "--------------------------------------------------------------------------------\n",
      "       cat  dog  great  great pets  log  love  love cat  mat  pets  sat\n",
      "Doc 1    1    0      0           0    0     0         0    1     0    1\n",
      "Doc 2    0    1      0           0    1     0         0    0     0    1\n",
      "Doc 3    0    0      1           1    0     0         0    0     1    0\n",
      "Doc 4    1    1      0           0    0     1         1    0     0    0\n",
      "\n",
      "üìè Vocabulary size: 10\n",
      "üìñ Vocabulary: ['cat', 'dog', 'great', 'great pets', 'log', 'love', 'love cat', 'mat', 'pets', 'sat']\n",
      "\n",
      "üí° Note: Includes bigrams (2-word combinations) like 'cat dog', 'great pets', etc.\n"
     ]
    }
   ],
   "source": [
    "# Method 3: CountVectorizer with Advanced Parameters\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD 3: CountVectorizer with Advanced Parameters\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_advanced = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    max_features=10,  # Keep only top 10 most frequent words\n",
    "    ngram_range=(1, 2),  # Include both unigrams and bigrams\n",
    "    min_df=1,  # Minimum document frequency\n",
    "    max_df=1.0  # Maximum document frequency\n",
    ")\n",
    "\n",
    "bow_sklearn_advanced = cv_advanced.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "bow_sklearn_advanced_df = pd.DataFrame(\n",
    "    bow_sklearn_advanced.toarray(),\n",
    "    columns=cv_advanced.get_feature_names_out(),\n",
    "    index=[f\"Doc {i}\" for i in range(1, len(documents) + 1)]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Document-Term Matrix (Advanced Parameters with Bigrams):\")\n",
    "print(\"-\" * 80)\n",
    "print(bow_sklearn_advanced_df)\n",
    "\n",
    "print(f\"\\nüìè Vocabulary size: {len(cv_advanced.get_feature_names_out())}\")\n",
    "print(f\"üìñ Vocabulary: {list(cv_advanced.get_feature_names_out())}\")\n",
    "print(\"\\nüí° Note: Includes bigrams (2-word combinations) like 'cat dog', 'great pets', etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358c633",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 5:\n",
    "\n",
    "1. **Industry Standard**: CountVectorizer is the professional way to create BoW representations\n",
    "2. **Automatic Preprocessing**: Built-in lowercase conversion, tokenization, and stop words removal\n",
    "3. **N-grams Support**: Can capture multi-word phrases (bigrams, trigrams, etc.)\n",
    "4. **Sparse Matrix**: Returns efficient sparse matrices for large datasets\n",
    "5. **Vocabulary Control**: Parameters like `max_features`, `min_df`, `max_df` help control vocabulary\n",
    "6. **Easy Integration**: Works seamlessly with scikit-learn ML pipelines\n",
    "\n",
    "**Key Parameters Explained**:\n",
    "- `stop_words='english'`: Remove English stop words\n",
    "- `lowercase=True`: Convert all text to lowercase\n",
    "- `max_features=10`: Keep only 10 most frequent words\n",
    "- `ngram_range=(1,2)`: Include single words and 2-word combinations\n",
    "- `min_df=1`: Word must appear in at least 1 document\n",
    "- `max_df=1.0`: Word can appear in up to 100% of documents\n",
    "\n",
    "**Key Insight**: Always use CountVectorizer for production code - it's optimized, feature-rich, and industry-standard!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0c961",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 6: Real-World Application - Sentiment Analysis Dataset\n",
    "\n",
    "**Objective**: Apply BoW on a realistic text classification scenario.\n",
    "\n",
    "**Dataset**: Movie reviews (positive and negative sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf20753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Movie Reviews Dataset:\n",
      "================================================================================\n",
      "1. [POSITIVE ‚úÖ] This movie was absolutely amazing! Great acting and wonderful story.\n",
      "2. [NEGATIVE ‚ùå] Terrible film. Waste of time and money. Very disappointing.\n",
      "3. [POSITIVE ‚úÖ] Excellent cinematography and brilliant performances by the cast.\n",
      "4. [NEGATIVE ‚ùå] Boring plot with poor character development. Not recommended.\n",
      "5. [POSITIVE ‚úÖ] A masterpiece! One of the best movies I have ever seen.\n",
      "6. [NEGATIVE ‚ùå] Awful experience. The worst movie of the year.\n",
      "7. [POSITIVE ‚úÖ] Fantastic storyline with unexpected twists. Highly entertaining!\n",
      "8. [NEGATIVE ‚ùå] Dull and uninteresting. Could not finish watching it.\n"
     ]
    }
   ],
   "source": [
    "# Create a realistic movie review dataset\n",
    "movie_reviews = [\n",
    "    \"This movie was absolutely amazing! Great acting and wonderful story.\",\n",
    "    \"Terrible film. Waste of time and money. Very disappointing.\",\n",
    "    \"Excellent cinematography and brilliant performances by the cast.\",\n",
    "    \"Boring plot with poor character development. Not recommended.\",\n",
    "    \"A masterpiece! One of the best movies I have ever seen.\",\n",
    "    \"Awful experience. The worst movie of the year.\",\n",
    "    \"Fantastic storyline with unexpected twists. Highly entertaining!\",\n",
    "    \"Dull and uninteresting. Could not finish watching it.\"\n",
    "]\n",
    "\n",
    "# Labels: 1 = Positive, 0 = Negative\n",
    "sentiments = [1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(\"üé¨ Movie Reviews Dataset:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (review, sentiment) in enumerate(zip(movie_reviews, sentiments), 1):\n",
    "    label = \"POSITIVE ‚úÖ\" if sentiment == 1 else \"NEGATIVE ‚ùå\"\n",
    "    print(f\"{i}. [{label}] {review}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1916feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Bag of Words Representation of Movie Reviews:\n",
      "====================================================================================================\n",
      "          absolutely  masterpiece  money  movie  movies  performances  plot  \\\n",
      "Review 1           1            0      0      1       0             0     0   \n",
      "Review 2           0            0      1      0       0             0     0   \n",
      "Review 3           0            0      0      0       0             1     0   \n",
      "Review 4           0            0      0      0       0             0     1   \n",
      "Review 5           0            1      0      0       1             0     0   \n",
      "Review 6           0            0      0      1       0             0     0   \n",
      "Review 7           0            0      0      0       0             0     0   \n",
      "Review 8           0            0      0      0       0             0     0   \n",
      "\n",
      "          poor  recommended  seen  ...  storyline  terrible  time  twists  \\\n",
      "Review 1     0            0     0  ...          0         0     0       0   \n",
      "Review 2     0            0     0  ...          0         1     1       0   \n",
      "Review 3     0            0     0  ...          0         0     0       0   \n",
      "Review 4     1            1     0  ...          0         0     0       0   \n",
      "Review 5     0            0     1  ...          0         0     0       0   \n",
      "Review 6     0            0     0  ...          0         0     0       0   \n",
      "Review 7     0            0     0  ...          1         0     0       1   \n",
      "Review 8     0            0     0  ...          0         0     0       0   \n",
      "\n",
      "          unexpected  uninteresting  waste  watching  wonderful  Sentiment  \n",
      "Review 1           0              0      0         0          1   Positive  \n",
      "Review 2           0              0      1         0          0   Negative  \n",
      "Review 3           0              0      0         0          0   Positive  \n",
      "Review 4           0              0      0         0          0   Negative  \n",
      "Review 5           0              0      0         0          0   Positive  \n",
      "Review 6           0              0      0         0          0   Negative  \n",
      "Review 7           1              0      0         0          0   Positive  \n",
      "Review 8           0              1      0         1          0   Negative  \n",
      "\n",
      "[8 rows x 21 columns]\n",
      "\n",
      "üìè Matrix Shape: (8, 20)\n",
      "üìñ Vocabulary Size: 20\n",
      "üìñ Features: ['absolutely', 'masterpiece', 'money', 'movie', 'movies', 'performances', 'plot', 'poor', 'recommended', 'seen', 'story', 'storyline', 'terrible', 'time', 'twists', 'unexpected', 'uninteresting', 'waste', 'watching', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "# Create BoW representation\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    max_features=20  # Keep top 20 features\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(movie_reviews)\n",
    "y = np.array(sentiments)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "bow_reviews_df = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Review {i}\" for i in range(1, len(movie_reviews) + 1)]\n",
    ")\n",
    "\n",
    "# Add sentiment column\n",
    "bow_reviews_df['Sentiment'] = ['Positive' if s == 1 else 'Negative' for s in sentiments]\n",
    "\n",
    "print(\"\\nüìä Bag of Words Representation of Movie Reviews:\")\n",
    "print(\"=\" * 100)\n",
    "print(bow_reviews_df)\n",
    "\n",
    "print(f\"\\nüìè Matrix Shape: {X.shape}\")\n",
    "print(f\"üìñ Vocabulary Size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"üìñ Features: {list(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ebf7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Word Frequency Analysis by Sentiment:\n",
      "================================================================================\n",
      "         Word  Positive_Count  Negative_Count  Total_Count Sentiment_Indicator\n",
      "        movie               1               1            2             NEUTRAL\n",
      "   absolutely               1               0            1            POSITIVE\n",
      "    storyline               1               0            1            POSITIVE\n",
      "     watching               0               1            1            NEGATIVE\n",
      "        waste               0               1            1            NEGATIVE\n",
      "uninteresting               0               1            1            NEGATIVE\n",
      "   unexpected               1               0            1            POSITIVE\n",
      "       twists               1               0            1            POSITIVE\n",
      "         time               0               1            1            NEGATIVE\n",
      "     terrible               0               1            1            NEGATIVE\n",
      "        story               1               0            1            POSITIVE\n",
      "  masterpiece               1               0            1            POSITIVE\n",
      "         seen               1               0            1            POSITIVE\n",
      "  recommended               0               1            1            NEGATIVE\n",
      "         poor               0               1            1            NEGATIVE\n",
      "         plot               0               1            1            NEGATIVE\n",
      " performances               1               0            1            POSITIVE\n",
      "       movies               1               0            1            POSITIVE\n",
      "        money               0               1            1            NEGATIVE\n",
      "    wonderful               1               0            1            POSITIVE\n",
      "\n",
      "üéØ Key Sentiment Indicators:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Positive Words: ['absolutely', 'storyline', 'unexpected', 'twists', 'story', 'masterpiece', 'seen', 'performances', 'movies', 'wonderful']\n",
      "‚ùå Negative Words: ['watching', 'waste', 'uninteresting', 'time', 'terrible', 'recommended', 'poor', 'plot', 'money']\n"
     ]
    }
   ],
   "source": [
    "# Analyze which words are most indicative of positive vs negative sentiment\n",
    "positive_reviews = X[np.array(sentiments) == 1].toarray().sum(axis=0)\n",
    "negative_reviews = X[np.array(sentiments) == 0].toarray().sum(axis=0)\n",
    "\n",
    "word_sentiment_df = pd.DataFrame({\n",
    "    'Word': vectorizer.get_feature_names_out(),\n",
    "    'Positive_Count': positive_reviews,\n",
    "    'Negative_Count': negative_reviews,\n",
    "    'Total_Count': positive_reviews + negative_reviews\n",
    "})\n",
    "\n",
    "word_sentiment_df['Sentiment_Indicator'] = word_sentiment_df.apply(\n",
    "    lambda row: 'POSITIVE' if row['Positive_Count'] > row['Negative_Count'] \n",
    "    else ('NEGATIVE' if row['Negative_Count'] > row['Positive_Count'] else 'NEUTRAL'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Sort by total count\n",
    "word_sentiment_df = word_sentiment_df.sort_values('Total_Count', ascending=False)\n",
    "\n",
    "print(\"\\nüìà Word Frequency Analysis by Sentiment:\")\n",
    "print(\"=\" * 80)\n",
    "print(word_sentiment_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüéØ Key Sentiment Indicators:\")\n",
    "print(\"-\" * 80)\n",
    "positive_words = word_sentiment_df[word_sentiment_df['Sentiment_Indicator'] == 'POSITIVE']['Word'].tolist()\n",
    "negative_words = word_sentiment_df[word_sentiment_df['Sentiment_Indicator'] == 'NEGATIVE']['Word'].tolist()\n",
    "\n",
    "print(f\"‚úÖ Positive Words: {positive_words}\")\n",
    "print(f\"‚ùå Negative Words: {negative_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f31afc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Machine Learning Classification Results:\n",
      "================================================================================\n",
      "Model: Multinomial Naive Bayes\n",
      "Accuracy: 100.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         4\n",
      "    Positive       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00         8\n",
      "   macro avg       1.00      1.00      1.00         8\n",
      "weighted avg       1.00      1.00      1.00         8\n",
      "\n",
      "\n",
      "üîÆ Predictions on New Reviews:\n",
      "================================================================================\n",
      "Review: \"Amazing movie with great performances!\"\n",
      "Predicted Sentiment: POSITIVE ‚úÖ\n",
      "\n",
      "Review: \"Terrible waste of my time.\"\n",
      "Predicted Sentiment: NEGATIVE ‚ùå\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Machine Learning Application\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# For this small dataset, we'll use the same data for training and testing (just for demonstration)\n",
    "# In real scenarios, you'd have separate train/test sets\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "print(\"\\nü§ñ Machine Learning Classification Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model: Multinomial Naive Bayes\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y, predictions, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Test on new reviews\n",
    "new_reviews = [\n",
    "    \"Amazing movie with great performances!\",\n",
    "    \"Terrible waste of my time.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîÆ Predictions on New Reviews:\")\n",
    "print(\"=\" * 80)\n",
    "new_reviews_bow = vectorizer.transform(new_reviews)\n",
    "new_predictions = classifier.predict(new_reviews_bow)\n",
    "\n",
    "for review, pred in zip(new_reviews, new_predictions):\n",
    "    sentiment = \"POSITIVE ‚úÖ\" if pred == 1 else \"NEGATIVE ‚ùå\"\n",
    "    print(f\"Review: \\\"{review}\\\"\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081e4a2",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 6:\n",
    "\n",
    "1. **Real-World Application**: Demonstrated how BoW is used in sentiment analysis\n",
    "2. **Feature Analysis**: Identified which words indicate positive vs negative sentiment\n",
    "3. **ML Integration**: Successfully trained a classifier using BoW features\n",
    "4. **Prediction Capability**: Model can classify new, unseen reviews\n",
    "5. **Interpretability**: Easy to understand which words contribute to predictions\n",
    "6. **Scalability**: This approach scales to thousands or millions of documents\n",
    "\n",
    "**Key Insights**:\n",
    "- Words like \"amazing\", \"excellent\", \"great\" ‚Üí Positive sentiment\n",
    "- Words like \"terrible\", \"worst\", \"poor\" ‚Üí Negative sentiment\n",
    "- BoW + Naive Bayes is a simple yet effective baseline for text classification\n",
    "- This is the foundation for more advanced NLP applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f95fb5",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 7: Binary vs Frequency Representation\n",
    "\n",
    "**Objective**: Compare binary (presence/absence) vs frequency-based BoW representations.\n",
    "\n",
    "**Difference**:\n",
    "- **Frequency**: Count how many times each word appears (1, 2, 3, ...)\n",
    "- **Binary**: Only record if word is present (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91709a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample Documents with Repeated Words:\n",
      "================================================================================\n",
      "Doc 1: good good good excellent\n",
      "Doc 2: bad bad bad terrible\n",
      "Doc 3: good excellent amazing\n",
      "Doc 4: bad terrible awful\n",
      "\n",
      "üìä Frequency-Based Representation:\n",
      "================================================================================\n",
      "       amazing  awful  bad  excellent  good  terrible\n",
      "Doc 1        0      0    0          1     3         0\n",
      "Doc 2        0      0    3          0     0         1\n",
      "Doc 3        1      0    0          1     1         0\n",
      "Doc 4        0      1    1          0     0         1\n",
      "\n",
      "üí° Values represent how many times each word appears\n"
     ]
    }
   ],
   "source": [
    "# Sample documents with repeated words\n",
    "repeated_docs = [\n",
    "    \"good good good excellent\",\n",
    "    \"bad bad bad terrible\",\n",
    "    \"good excellent amazing\",\n",
    "    \"bad terrible awful\"\n",
    "]\n",
    "\n",
    "print(\"üìÑ Sample Documents with Repeated Words:\")\n",
    "print(\"=\" * 80)\n",
    "for i, doc in enumerate(repeated_docs, 1):\n",
    "    print(f\"Doc {i}: {doc}\")\n",
    "\n",
    "# Frequency-based BoW\n",
    "cv_frequency = CountVectorizer()\n",
    "bow_frequency = cv_frequency.fit_transform(repeated_docs)\n",
    "\n",
    "bow_frequency_df = pd.DataFrame(\n",
    "    bow_frequency.toarray(),\n",
    "    columns=cv_frequency.get_feature_names_out(),\n",
    "    index=[f\"Doc {i}\" for i in range(1, len(repeated_docs) + 1)]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Frequency-Based Representation:\")\n",
    "print(\"=\" * 80)\n",
    "print(bow_frequency_df)\n",
    "print(\"\\nüí° Values represent how many times each word appears\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e81c97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Binary Representation:\n",
      "================================================================================\n",
      "       amazing  awful  bad  excellent  good  terrible\n",
      "Doc 1        0      0    0          1     1         0\n",
      "Doc 2        0      0    1          0     0         1\n",
      "Doc 3        1      0    0          1     1         0\n",
      "Doc 4        0      1    1          0     0         1\n",
      "\n",
      "üí° Values are 0 (absent) or 1 (present), regardless of frequency\n",
      "\n",
      "‚öñÔ∏è Comparison:\n",
      "================================================================================\n",
      "Document 1: 'good good good excellent'\n",
      "Frequency representation of 'good': 3\n",
      "Binary representation of 'good': 1\n",
      "\n",
      "Document 2: 'bad bad bad terrible'\n",
      "Frequency representation of 'bad': 3\n",
      "Binary representation of 'bad': 1\n"
     ]
    }
   ],
   "source": [
    "# Binary BoW (presence/absence only)\n",
    "cv_binary = CountVectorizer(binary=True)\n",
    "bow_binary = cv_binary.fit_transform(repeated_docs)\n",
    "\n",
    "bow_binary_df = pd.DataFrame(\n",
    "    bow_binary.toarray(),\n",
    "    columns=cv_binary.get_feature_names_out(),\n",
    "    index=[f\"Doc {i}\" for i in range(1, len(repeated_docs) + 1)]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Binary Representation:\")\n",
    "print(\"=\" * 80)\n",
    "print(bow_binary_df)\n",
    "print(\"\\nüí° Values are 0 (absent) or 1 (present), regardless of frequency\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n‚öñÔ∏è Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Document 1: 'good good good excellent'\")\n",
    "print(f\"Frequency representation of 'good': {bow_frequency_df.iloc[0]['good']}\")\n",
    "print(f\"Binary representation of 'good': {bow_binary_df.iloc[0]['good']}\")\n",
    "print(\"\\nDocument 2: 'bad bad bad terrible'\")\n",
    "print(f\"Frequency representation of 'bad': {bow_frequency_df.iloc[1]['bad']}\")\n",
    "print(f\"Binary representation of 'bad': {bow_binary_df.iloc[1]['bad']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c6258",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 7:\n",
    "\n",
    "1. **Frequency Representation**: \n",
    "   - Captures how many times a word appears\n",
    "   - Useful when word frequency matters (e.g., emphasis)\n",
    "   - More informative but sensitive to word repetition\n",
    "\n",
    "2. **Binary Representation**: \n",
    "   - Only indicates presence or absence\n",
    "   - Treats \"good\" and \"good good good\" the same way\n",
    "   - Less sensitive to word repetition\n",
    "   - Better for some classification tasks where presence matters more than frequency\n",
    "\n",
    "3. **When to Use Each**:\n",
    "   - **Frequency**: Topic modeling, information retrieval, document similarity\n",
    "   - **Binary**: Text classification, spam detection (word presence often enough)\n",
    "\n",
    "**Key Insight**: Choose binary vs frequency based on whether word repetition carries meaningful information!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67015dd0",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 8: Handling Large Vocabulary with Parameters\n",
    "\n",
    "**Objective**: Learn to control vocabulary size in real-world scenarios.\n",
    "\n",
    "**Problem**: Real documents can have thousands of unique words, creating huge, sparse matrices.\n",
    "\n",
    "**Solution**: Use `min_df`, `max_df`, and `max_features` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd81ef7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Large Corpus Sample:\n",
      "================================================================================\n",
      "1. Python is a great programming language for data science\n",
      "2. Machine learning requires understanding of algorithms and mathematics\n",
      "3. Deep learning is a subset of machine learning\n",
      "4. Natural language processing helps computers understand human language\n",
      "5. Data science involves statistics, programming, and domain knowledge\n",
      "6. Python libraries like NumPy and Pandas are essential for data analysis\n",
      "7. Neural networks are inspired by biological neurons\n",
      "8. Text mining extracts insights from unstructured text data\n",
      "\n",
      "================================================================================\n",
      "METHOD 1: No Constraints (All Words)\n",
      "================================================================================\n",
      "Vocabulary Size: 40\n",
      "Matrix Shape: (8, 40)\n",
      "Vocabulary: ['algorithms', 'analysis', 'biological', 'computers', 'data', 'deep', 'domain', 'essential', 'extracts', 'great', 'helps', 'human', 'insights', 'inspired', 'involves', 'knowledge', 'language', 'learning', 'libraries', 'like', 'machine', 'mathematics', 'mining', 'natural', 'networks', 'neural', 'neurons', 'numpy', 'pandas', 'processing', 'programming', 'python', 'requires', 'science', 'statistics', 'subset', 'text', 'understand', 'understanding', 'unstructured']\n"
     ]
    }
   ],
   "source": [
    "# Larger corpus with diverse vocabulary\n",
    "large_corpus = [\n",
    "    \"Python is a great programming language for data science\",\n",
    "    \"Machine learning requires understanding of algorithms and mathematics\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Natural language processing helps computers understand human language\",\n",
    "    \"Data science involves statistics, programming, and domain knowledge\",\n",
    "    \"Python libraries like NumPy and Pandas are essential for data analysis\",\n",
    "    \"Neural networks are inspired by biological neurons\",\n",
    "    \"Text mining extracts insights from unstructured text data\"\n",
    "]\n",
    "\n",
    "print(\"üìÑ Large Corpus Sample:\")\n",
    "print(\"=\" * 80)\n",
    "for i, doc in enumerate(large_corpus, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "# Method 1: No constraints (all words)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD 1: No Constraints (All Words)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_all = CountVectorizer(stop_words='english')\n",
    "bow_all = cv_all.fit_transform(large_corpus)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(cv_all.get_feature_names_out())}\")\n",
    "print(f\"Matrix Shape: {bow_all.shape}\")\n",
    "print(f\"Vocabulary: {list(cv_all.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METHOD 2: min_df=2 (word must appear in at least 2 documents)\n",
      "================================================================================\n",
      "Vocabulary Size: 7\n",
      "Matrix Shape: (8, 7)\n",
      "Vocabulary: ['data', 'language', 'learning', 'machine', 'programming', 'python', 'science']\n",
      "üí° Removed rare words that appear in only 1 document\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using min_df (minimum document frequency)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD 2: min_df=2 (word must appear in at least 2 documents)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_min_df = CountVectorizer(stop_words='english', min_df=2)\n",
    "bow_min_df = cv_min_df.fit_transform(large_corpus)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(cv_min_df.get_feature_names_out())}\")\n",
    "print(f\"Matrix Shape: {bow_min_df.shape}\")\n",
    "print(f\"Vocabulary: {list(cv_min_df.get_feature_names_out())}\")\n",
    "print(\"üí° Removed rare words that appear in only 1 document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cce1057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METHOD 3: max_df=0.5 (word can appear in max 50% of documents)\n",
      "================================================================================\n",
      "Vocabulary Size: 40\n",
      "Matrix Shape: (8, 40)\n",
      "Vocabulary: ['algorithms', 'analysis', 'biological', 'computers', 'data', 'deep', 'domain', 'essential', 'extracts', 'great', 'helps', 'human', 'insights', 'inspired', 'involves', 'knowledge', 'language', 'learning', 'libraries', 'like', 'machine', 'mathematics', 'mining', 'natural', 'networks', 'neural', 'neurons', 'numpy', 'pandas', 'processing', 'programming', 'python', 'requires', 'science', 'statistics', 'subset', 'text', 'understand', 'understanding', 'unstructured']\n",
      "üí° Removed very common words that appear in >50% of documents\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Using max_df (maximum document frequency)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD 3: max_df=0.5 (word can appear in max 50% of documents)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_max_df = CountVectorizer(stop_words='english', max_df=0.5)\n",
    "bow_max_df = cv_max_df.fit_transform(large_corpus)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(cv_max_df.get_feature_names_out())}\")\n",
    "print(f\"Matrix Shape: {bow_max_df.shape}\")\n",
    "print(f\"Vocabulary: {list(cv_max_df.get_feature_names_out())}\")\n",
    "print(\"üí° Removed very common words that appear in >50% of documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d826dbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "METHOD 4: max_features=10 (keep only top 10 most frequent words)\n",
      "================================================================================\n",
      "Vocabulary Size: 10\n",
      "Matrix Shape: (8, 10)\n",
      "Vocabulary: ['data', 'language', 'learning', 'machine', 'natural', 'networks', 'programming', 'python', 'science', 'text']\n",
      "üí° Kept only the 10 most frequent words\n",
      "\n",
      "================================================================================\n",
      "METHOD 5: Combined (min_df=2, max_df=0.7, max_features=15)\n",
      "================================================================================\n",
      "Vocabulary Size: 7\n",
      "Matrix Shape: (8, 7)\n",
      "Vocabulary: ['data', 'language', 'learning', 'machine', 'programming', 'python', 'science']\n",
      "üí° Applied multiple filters for optimal vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Method 4: Using max_features (keep only top N features)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD 4: max_features=10 (keep only top 10 most frequent words)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_max_features = CountVectorizer(stop_words='english', max_features=10)\n",
    "bow_max_features = cv_max_features.fit_transform(large_corpus)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(cv_max_features.get_feature_names_out())}\")\n",
    "print(f\"Matrix Shape: {bow_max_features.shape}\")\n",
    "print(f\"Vocabulary: {list(cv_max_features.get_feature_names_out())}\")\n",
    "print(\"üí° Kept only the 10 most frequent words\")\n",
    "\n",
    "# Method 5: Combining parameters\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METHOD 5: Combined (min_df=2, max_df=0.7, max_features=15)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cv_combined = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    min_df=2,\n",
    "    max_df=0.7,\n",
    "    max_features=15\n",
    ")\n",
    "bow_combined = cv_combined.fit_transform(large_corpus)\n",
    "\n",
    "print(f\"Vocabulary Size: {len(cv_combined.get_feature_names_out())}\")\n",
    "print(f\"Matrix Shape: {bow_combined.shape}\")\n",
    "print(f\"Vocabulary: {list(cv_combined.get_feature_names_out())}\")\n",
    "print(\"üí° Applied multiple filters for optimal vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9a012",
   "metadata": {},
   "source": [
    "### üìù Observations - Experiment 8:\n",
    "\n",
    "1. **min_df (Minimum Document Frequency)**:\n",
    "   - Removes rare words that appear in very few documents\n",
    "   - Helps eliminate typos and uncommon words\n",
    "   - Reduces noise and vocabulary size\n",
    "\n",
    "2. **max_df (Maximum Document Frequency)**:\n",
    "   - Removes very common words (like domain-specific stop words)\n",
    "   - Can be a number (count) or fraction (0.0 to 1.0)\n",
    "   - Helps focus on distinctive words\n",
    "\n",
    "3. **max_features**:\n",
    "   - Keeps only top N most frequent words\n",
    "   - Simple way to control vocabulary size\n",
    "   - Good for quick prototyping\n",
    "\n",
    "4. **Combined Approach**:\n",
    "   - Best practice: combine multiple parameters\n",
    "   - Balance between vocabulary size and information retention\n",
    "   - Improves model performance and efficiency\n",
    "\n",
    "**Key Insight**: Proper vocabulary filtering is crucial for real-world NLP applications!\n",
    "\n",
    "**Recommended Settings**:\n",
    "- Small dataset (< 1000 docs): `min_df=1`, `max_df=0.8`\n",
    "- Medium dataset (1000-10000 docs): `min_df=5`, `max_df=0.7`, `max_features=5000`\n",
    "- Large dataset (> 10000 docs): `min_df=10`, `max_df=0.5`, `max_features=10000`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5d5d4",
   "metadata": {},
   "source": [
    "## üìä Summary and Comparison\n",
    "\n",
    "### Complete Overview of All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0721449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "COMPREHENSIVE EXPERIMENTS SUMMARY\n",
      "========================================================================================================================\n",
      "                Experiment                    Technique                     Key Learning Vocabulary_Reduction                       Use_Case\n",
      "          Exp 1: Basic BoW        Manual implementation   Basic concept & implementation             Baseline         Learning/Understanding\n",
      " Exp 2: Stop Words Removal               NLTK stopwords Reducing noise from common words             ‚Üì Medium                  Preprocessing\n",
      "           Exp 3: Stemming                PorterStemmer   Normalizing words to root form             ‚Üì Medium             Text Normalization\n",
      "      Exp 4: Lemmatization            WordNetLemmatizer   Intelligent word normalization             ‚Üì Medium    Text Normalization (Better)\n",
      "    Exp 5: CountVectorizer      Sklearn CountVectorizer      Professional implementation       ‚Üì Configurable             Production Systems\n",
      "     Exp 6: Real-World App           Sentiment Analysis          ML application with BoW                  N/A           Classification Tasks\n",
      "Exp 7: Binary vs Frequency             Binary parameter    Presence vs frequency matters            Same size Spam Detection, Classification\n",
      " Exp 8: Vocabulary Control min_df, max_df, max_features      Controlling vocabulary size               ‚Üì High       Large-scale Applications\n",
      "\n",
      "========================================================================================================================\n",
      "VOCABULARY SIZE EVOLUTION (on original 4 documents)\n",
      "========================================================================================================================\n",
      "              Method  Vocabulary_Size Reduction\n",
      "Original (All words)               16        0%\n",
      "  Stop Words Removed               10     37.5%\n",
      "       With Stemming                8     50.0%\n",
      "  With Lemmatization                8     50.0%\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive comparison summary\n",
    "experiments_summary = pd.DataFrame({\n",
    "    'Experiment': [\n",
    "        'Exp 1: Basic BoW',\n",
    "        'Exp 2: Stop Words Removal',\n",
    "        'Exp 3: Stemming',\n",
    "        'Exp 4: Lemmatization',\n",
    "        'Exp 5: CountVectorizer',\n",
    "        'Exp 6: Real-World App',\n",
    "        'Exp 7: Binary vs Frequency',\n",
    "        'Exp 8: Vocabulary Control'\n",
    "    ],\n",
    "    'Technique': [\n",
    "        'Manual implementation',\n",
    "        'NLTK stopwords',\n",
    "        'PorterStemmer',\n",
    "        'WordNetLemmatizer',\n",
    "        'Sklearn CountVectorizer',\n",
    "        'Sentiment Analysis',\n",
    "        'Binary parameter',\n",
    "        'min_df, max_df, max_features'\n",
    "    ],\n",
    "    'Key Learning': [\n",
    "        'Basic concept & implementation',\n",
    "        'Reducing noise from common words',\n",
    "        'Normalizing words to root form',\n",
    "        'Intelligent word normalization',\n",
    "        'Professional implementation',\n",
    "        'ML application with BoW',\n",
    "        'Presence vs frequency matters',\n",
    "        'Controlling vocabulary size'\n",
    "    ],\n",
    "    'Vocabulary_Reduction': [\n",
    "        'Baseline',\n",
    "        '‚Üì Medium',\n",
    "        '‚Üì Medium',\n",
    "        '‚Üì Medium',\n",
    "        '‚Üì Configurable',\n",
    "        'N/A',\n",
    "        'Same size',\n",
    "        '‚Üì High'\n",
    "    ],\n",
    "    'Use_Case': [\n",
    "        'Learning/Understanding',\n",
    "        'Preprocessing',\n",
    "        'Text Normalization',\n",
    "        'Text Normalization (Better)',\n",
    "        'Production Systems',\n",
    "        'Classification Tasks',\n",
    "        'Spam Detection, Classification',\n",
    "        'Large-scale Applications'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"COMPREHENSIVE EXPERIMENTS SUMMARY\")\n",
    "print(\"=\" * 120)\n",
    "print(experiments_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"VOCABULARY SIZE EVOLUTION (on original 4 documents)\")\n",
    "print(\"=\" * 120)\n",
    "vocab_comparison = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Original (All words)',\n",
    "        'Stop Words Removed',\n",
    "        'With Stemming',\n",
    "        'With Lemmatization'\n",
    "    ],\n",
    "    'Vocabulary_Size': [\n",
    "        bow_df.shape[1],\n",
    "        bow_df_filtered.shape[1],\n",
    "        bow_df_stemmed.shape[1],\n",
    "        bow_df_lemmatized.shape[1]\n",
    "    ],\n",
    "    'Reduction': [\n",
    "        '0%',\n",
    "        f'{((bow_df.shape[1] - bow_df_filtered.shape[1]) / bow_df.shape[1] * 100):.1f}%',\n",
    "        f'{((bow_df.shape[1] - bow_df_stemmed.shape[1]) / bow_df.shape[1] * 100):.1f}%',\n",
    "        f'{((bow_df.shape[1] - bow_df_lemmatized.shape[1]) / bow_df.shape[1] * 100):.1f}%'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(vocab_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1590a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Advantages of Bag of Words\n",
    "\n",
    "### ‚úÖ Strengths:\n",
    "\n",
    "1. **Simplicity**: Easy to understand and implement\n",
    "2. **Interpretability**: Clear relationship between words and features\n",
    "3. **Efficiency**: Fast computation, especially with sparse matrices\n",
    "4. **Effectiveness**: Works well for many NLP tasks\n",
    "5. **Foundation**: Base for understanding advanced techniques\n",
    "6. **Scalability**: Can handle large documents with proper parameters\n",
    "7. **Flexibility**: Many parameters to tune (stop words, n-grams, etc.)\n",
    "8. **Tool Support**: Excellent library support (sklearn, nltk)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96816ffc",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Limitations of Bag of Words\n",
    "\n",
    "### ‚ùå Weaknesses:\n",
    "\n",
    "1. **Loss of Word Order**: \"Dog bites man\" vs \"Man bites dog\" are treated identically\n",
    "2. **No Semantic Understanding**: Doesn't understand word meanings or context\n",
    "3. **Sparse Matrices**: Large vocabularies create mostly zero values\n",
    "4. **Ignores Word Relationships**: Can't capture synonyms or related words\n",
    "5. **Fixed Vocabulary**: Can't handle new words not seen during training (OOV problem)\n",
    "6. **No Context**: Same word in different contexts treated identically\n",
    "7. **Size Issues**: Vocabulary can become very large with real datasets\n",
    "8. **Frequency Bias**: Very frequent words can dominate less frequent but important words\n",
    "\n",
    "### üí° Solutions to Limitations:\n",
    "\n",
    "| Limitation | Solution |\n",
    "|-----------|----------|\n",
    "| Loss of order | Use n-grams (bigrams, trigrams) |\n",
    "| Sparse matrices | Use TF-IDF, dimension reduction |\n",
    "| No semantics | Use Word2Vec, GloVe, BERT embeddings |\n",
    "| Fixed vocabulary | Use subword tokenization, character n-grams |\n",
    "| Frequency bias | Use TF-IDF weighting |\n",
    "| Large vocabulary | Use vocabulary filtering (min_df, max_df) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29581949",
   "metadata": {},
   "source": [
    "## üéì Applications and Use Cases\n",
    "\n",
    "### Where Bag of Words Works Well:\n",
    "\n",
    "1. **Text Classification**\n",
    "   - Spam detection\n",
    "   - Sentiment analysis\n",
    "   - Topic categorization\n",
    "   - Document categorization\n",
    "\n",
    "2. **Information Retrieval**\n",
    "   - Search engines (basic)\n",
    "   - Document similarity\n",
    "   - Content recommendation\n",
    "\n",
    "3. **Baseline Models**\n",
    "   - Quick prototyping\n",
    "   - Benchmarking\n",
    "   - Feature engineering baseline\n",
    "\n",
    "4. **Small to Medium Datasets**\n",
    "   - When you don't have millions of documents\n",
    "   - When training time matters\n",
    "   - When interpretability is important\n",
    "\n",
    "### When to Use Alternatives:\n",
    "\n",
    "- **TF-IDF**: When word importance matters more than raw frequency\n",
    "- **Word Embeddings (Word2Vec, GloVe)**: When semantic similarity is crucial\n",
    "- **Transformers (BERT, GPT)**: For state-of-the-art performance on complex tasks\n",
    "- **Character/Subword models**: For handling OOV words, multilingual text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e1d98",
   "metadata": {},
   "source": [
    "## üöÄ Best Practices and Recommendations\n",
    "\n",
    "### ‚ú® Production-Ready BoW Pipeline:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Recommended configuration\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,           # Normalize case\n",
    "    stop_words='english',     # Remove common words\n",
    "    min_df=5,                 # Word must appear in 5+ documents\n",
    "    max_df=0.7,               # Word can't appear in >70% of documents\n",
    "    max_features=5000,        # Keep top 5000 features\n",
    "    ngram_range=(1, 2),       # Include unigrams and bigrams\n",
    "    token_pattern=r'\\b\\w+\\b'  # Word boundary tokenization\n",
    ")\n",
    "\n",
    "# Create ML pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "```\n",
    "\n",
    "### üìã Preprocessing Checklist:\n",
    "\n",
    "1. ‚òëÔ∏è **Lowercase conversion**: Normalize case\n",
    "2. ‚òëÔ∏è **Remove stop words**: Eliminate common words\n",
    "3. ‚òëÔ∏è **Handle punctuation**: Remove or tokenize properly\n",
    "4. ‚òëÔ∏è **Lemmatization/Stemming**: Normalize word forms\n",
    "5. ‚òëÔ∏è **Remove special characters**: Clean text\n",
    "6. ‚òëÔ∏è **Handle numbers**: Remove or keep based on use case\n",
    "7. ‚òëÔ∏è **Set vocabulary limits**: Use min_df, max_df, max_features\n",
    "8. ‚òëÔ∏è **Consider n-grams**: Include bigrams/trigrams if needed\n",
    "\n",
    "### üéØ Parameter Selection Guide:\n",
    "\n",
    "| Dataset Size | min_df | max_df | max_features | ngram_range |\n",
    "|-------------|--------|--------|--------------|-------------|\n",
    "| Small (<1K docs) | 1 | 0.8 | 1000 | (1, 1) |\n",
    "| Medium (1K-10K) | 5 | 0.7 | 5000 | (1, 2) |\n",
    "| Large (>10K) | 10 | 0.5 | 10000 | (1, 2) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af82d81",
   "metadata": {},
   "source": [
    "## üìö Comparison with Other Techniques\n",
    "\n",
    "### BoW vs Other Text Vectorization Methods:\n",
    "\n",
    "| Technique | Word Order | Semantics | Size | Complexity | Use Case |\n",
    "|-----------|-----------|-----------|------|------------|----------|\n",
    "| **Bag of Words** | ‚ùå No | ‚ùå No | Large | Low | Baseline, simple classification |\n",
    "| **TF-IDF** | ‚ùå No | ‚ùå No | Large | Low | Document ranking, search |\n",
    "| **Word2Vec** | ‚ö†Ô∏è Partial | ‚úÖ Yes | Medium | Medium | Semantic similarity, analogies |\n",
    "| **GloVe** | ‚ö†Ô∏è Partial | ‚úÖ Yes | Medium | Medium | Similar to Word2Vec |\n",
    "| **FastText** | ‚ö†Ô∏è Partial | ‚úÖ Yes | Medium | Medium | Handles OOV, subwords |\n",
    "| **BERT/Transformers** | ‚úÖ Yes | ‚úÖ Yes | Small | High | State-of-art, context-aware |\n",
    "\n",
    "### Quick Comparison:\n",
    "\n",
    "**Bag of Words**:\n",
    "- ‚úÖ Simple, fast, interpretable\n",
    "- ‚ùå No semantics, no word order\n",
    "- üéØ Best for: Simple classification, baseline models\n",
    "\n",
    "**TF-IDF** (Next step from BoW):\n",
    "- ‚úÖ Weights words by importance\n",
    "- ‚ùå Still no semantics or order\n",
    "- üéØ Best for: Search, document ranking\n",
    "\n",
    "**Word Embeddings**:\n",
    "- ‚úÖ Captures semantics and relationships\n",
    "- ‚ùå Loses word order, fixed representation\n",
    "- üéØ Best for: Semantic tasks, document similarity\n",
    "\n",
    "**Transformers**:\n",
    "- ‚úÖ Context-aware, understands semantics and order\n",
    "- ‚ùå Computationally expensive\n",
    "- üéØ Best for: State-of-art performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae4cdc",
   "metadata": {},
   "source": [
    "## üé¨ Final Thoughts and Key Takeaways\n",
    "\n",
    "### üåü What You Learned:\n",
    "\n",
    "1. ‚úÖ **Core Concept**: Converting text to numerical vectors using word frequency\n",
    "2. ‚úÖ **Manual Implementation**: Built BoW from scratch to understand the fundamentals\n",
    "3. ‚úÖ **Preprocessing Techniques**: Stop words removal, stemming, lemmatization\n",
    "4. ‚úÖ **Professional Tools**: Used sklearn's CountVectorizer for production-ready code\n",
    "5. ‚úÖ **Real Applications**: Applied BoW to sentiment analysis\n",
    "6. ‚úÖ **Optimization**: Learned to control vocabulary with various parameters\n",
    "7. ‚úÖ **Trade-offs**: Understood advantages and limitations\n",
    "8. ‚úÖ **Best Practices**: Learned industry-standard approaches\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **TF-IDF**: Learn about term frequency-inverse document frequency\n",
    "2. **Word Embeddings**: Explore Word2Vec, GloVe, FastText\n",
    "3. **Advanced Techniques**: Study BERT, GPT, and transformer models\n",
    "4. **Practice Projects**: Build a spam classifier, sentiment analyzer, or document classifier\n",
    "5. **Large Datasets**: Work with real-world datasets (IMDB reviews, news articles)\n",
    "\n",
    "### üí° Remember:\n",
    "\n",
    "> \"Bag of Words is simple but powerful. It's the foundation of NLP and still widely used in production. Master the basics before moving to advanced techniques!\"\n",
    "\n",
    "### üéØ When to Use BoW:\n",
    "\n",
    "- ‚úÖ Quick prototyping\n",
    "- ‚úÖ Baseline models\n",
    "- ‚úÖ When interpretability matters\n",
    "- ‚úÖ Small to medium datasets\n",
    "- ‚úÖ Simple classification tasks\n",
    "- ‚úÖ When computational resources are limited\n",
    "\n",
    "### üìñ Key Formula:\n",
    "\n",
    "For a document $d$ and vocabulary $V$:\n",
    "\n",
    "$$\\\\text{BoW}(d) = [count(w_1), count(w_2), ..., count(w_n)]$$\n",
    "\n",
    "Where $n = |V|$ (vocabulary size) and $count(w_i)$ is the frequency of word $w_i$ in document $d$.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
