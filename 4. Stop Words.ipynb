{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363e075e",
   "metadata": {},
   "source": [
    "# Stop Words in Natural Language Processing (NLP)\n",
    "\n",
    "## üìö Table of Contents\n",
    "1. What are Stop Words?\n",
    "2. Why are Stop Words Important?\n",
    "3. Examples of Stop Words\n",
    "4. When to Remove Stop Words\n",
    "5. When NOT to Remove Stop Words\n",
    "6. Practical Implementation with NLTK\n",
    "7. Advanced Experiments\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What are Stop Words?\n",
    "\n",
    "**Stop words** are the most common words in any language that carry little to no meaningful information for text analysis tasks. These are words that appear very frequently in text but typically don't contribute much to the overall meaning or sentiment.\n",
    "\n",
    "### Common Examples:\n",
    "- Articles: a, an, the\n",
    "- Pronouns: I, you, he, she, it, we, they\n",
    "- Prepositions: in, on, at, to, from, with\n",
    "- Conjunctions: and, but, or, because\n",
    "- Auxiliary verbs: is, am, are, was, were, have, has\n",
    "\n",
    "### Key Characteristics:\n",
    "‚úÖ **High Frequency**: Appear very often in text  \n",
    "‚úÖ **Low Semantic Value**: Don't carry significant meaning  \n",
    "‚úÖ **Language Dependent**: Different for each language  \n",
    "‚úÖ **Context Dependent**: Importance varies by use case\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why are Stop Words Important?\n",
    "\n",
    "### Benefits of Removing Stop Words:\n",
    "\n",
    "1. **Reduces Dimensionality**: Decreases the vocabulary size significantly\n",
    "2. **Improves Processing Speed**: Less data to process\n",
    "3. **Enhances Focus**: Helps algorithms focus on meaningful words\n",
    "4. **Reduces Noise**: Eliminates common words that may not add value\n",
    "5. **Better Storage**: Reduces memory and disk space requirements\n",
    "\n",
    "### Impact on Different NLP Tasks:\n",
    "\n",
    "| Task | Impact of Removal | Recommendation |\n",
    "|------|------------------|----------------|\n",
    "| Text Classification | Positive | Usually Remove |\n",
    "| Sentiment Analysis | Mixed | Case-by-case |\n",
    "| Information Retrieval | Positive | Usually Remove |\n",
    "| Machine Translation | Negative | Keep |\n",
    "| Question Answering | Negative | Keep |\n",
    "| Named Entity Recognition | Negative | Keep |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. When to Remove Stop Words ‚úÖ\n",
    "\n",
    "- **Text Classification**: Spam detection, topic categorization\n",
    "- **Search Engines**: Improving search query processing\n",
    "- **Keyword Extraction**: Finding most relevant terms\n",
    "- **Document Clustering**: Grouping similar documents\n",
    "- **Bag of Words Models**: When word presence matters more than structure\n",
    "\n",
    "---\n",
    "\n",
    "## 4. When NOT to Remove Stop Words ‚ùå\n",
    "\n",
    "- **Sentiment Analysis**: \"not good\" vs \"good\" - negations matter!\n",
    "- **Question Answering**: \"what\", \"where\", \"when\" are crucial\n",
    "- **Machine Translation**: Grammar and structure are essential\n",
    "- **Named Entity Recognition**: Context around entities matters\n",
    "- **Text Summarization**: Sentence structure is important\n",
    "- **Language Modeling**: All words contribute to language patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Important Considerations\n",
    "\n",
    "‚ö†Ô∏è **Warning**: Blindly removing stop words can sometimes hurt your model's performance!\n",
    "\n",
    "### Decision Factors:\n",
    "1. **Task Requirements**: What is the end goal?\n",
    "2. **Domain Specificity**: Medical texts vs. social media\n",
    "3. **Model Type**: Deep learning models vs. traditional ML\n",
    "4. **Data Size**: Large datasets vs. small datasets\n",
    "5. **Performance Metrics**: Always test with and without removal\n",
    "\n",
    "---\n",
    "\n",
    "Now let's dive into **Practical Implementation** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d472e",
   "metadata": {},
   "source": [
    "## Experiment 1: Installing and Importing Required Libraries\n",
    "\n",
    "**Objective**: Set up our environment with NLTK and download stop words corpus\n",
    "\n",
    "**What we'll do**:\n",
    "- Import necessary libraries\n",
    "- Download NLTK stop words dataset\n",
    "- Verify successful installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e22a3780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stop words corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mahes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mahes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mahes\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Setup Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK stop words corpus...\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"\\n‚úÖ Setup Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ec111",
   "metadata": {},
   "source": [
    "### üìä Observation 1:\n",
    "- **NLTK** provides pre-compiled lists of stop words for multiple languages\n",
    "- The `stopwords` corpus needs to be downloaded only once\n",
    "- The `punkt` tokenizer is needed for splitting text into words\n",
    "- This is a one-time setup process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 2: Viewing Available Languages\n",
    "\n",
    "**Objective**: Explore which languages are supported by NLTK's stop words corpus\n",
    "\n",
    "**What we'll do**:\n",
    "- List all available languages\n",
    "- Count the total number of supported languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef4bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Languages Supported by NLTK Stop Words:\n",
      "==================================================\n",
      "1. albanian\n",
      "2. arabic\n",
      "3. azerbaijani\n",
      "4. basque\n",
      "5. belarusian\n",
      "6. bengali\n",
      "7. catalan\n",
      "8. chinese\n",
      "9. danish\n",
      "10. dutch\n",
      "11. english\n",
      "12. finnish\n",
      "13. french\n",
      "14. german\n",
      "15. greek\n",
      "16. hebrew\n",
      "17. hinglish\n",
      "18. hungarian\n",
      "19. indonesian\n",
      "20. italian\n",
      "21. kazakh\n",
      "22. nepali\n",
      "23. norwegian\n",
      "24. portuguese\n",
      "25. romanian\n",
      "26. russian\n",
      "27. slovene\n",
      "28. spanish\n",
      "29. swedish\n",
      "30. tajik\n",
      "31. tamil\n",
      "32. turkish\n",
      "\n",
      "==================================================\n",
      "üìå Total Languages Supported: 32\n"
     ]
    }
   ],
   "source": [
    "# Get list of all supported languages\n",
    "available_languages = stopwords.fileids()\n",
    "\n",
    "print(\"üåç Languages Supported by NLTK Stop Words:\")\n",
    "print(\"=\" * 50)\n",
    "for i, lang in enumerate(available_languages, 1):\n",
    "    print(f\"{i}. {lang}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"üìå Total Languages Supported: {len(available_languages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660681a",
   "metadata": {},
   "source": [
    "### üìä Observation 2:\n",
    "- NLTK supports stop words for **multiple languages** (typically 20+)\n",
    "- Languages include: English, Spanish, French, German, Italian, Portuguese, Russian, Arabic, and many more\n",
    "- This makes NLTK suitable for **multilingual NLP applications**\n",
    "- Each language has its own curated list of stop words\n",
    "- The language names are in **lowercase** format (e.g., 'english', 'spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc4e5d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 3: Exploring English Stop Words\n",
    "\n",
    "**Objective**: Examine the complete list of English stop words in NLTK\n",
    "\n",
    "**What we'll do**:\n",
    "- Load English stop words\n",
    "- Display all stop words\n",
    "- Count the total number\n",
    "- Analyze the composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c6321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ English Stop Words in NLTK:\n",
      "============================================================\n",
      "Total Count: 198 words\n",
      "\n",
      "Complete List:\n",
      "------------------------------------------------------------\n",
      "a             about         above         after         again       \n",
      "against       ain           all           am            an          \n",
      "and           any           are           aren          aren't      \n",
      "as            at            be            because       been        \n",
      "before        being         below         between       both        \n",
      "but           by            can           couldn        couldn't    \n",
      "d             did           didn          didn't        do          \n",
      "does          doesn         doesn't       doing         don         \n",
      "don't         down          during        each          few         \n",
      "for           from          further       had           hadn        \n",
      "hadn't        has           hasn          hasn't        have        \n",
      "haven         haven't       having        he            he'd        \n",
      "he'll         he's          her           here          hers        \n",
      "herself       him           himself       his           how         \n",
      "i             i'd           i'll          i'm           i've        \n",
      "if            in            into          is            isn         \n",
      "isn't         it            it'd          it'll         it's        \n",
      "its           itself        just          ll            m           \n",
      "ma            me            mightn        mightn't      more        \n",
      "most          mustn         mustn't       my            myself      \n",
      "needn         needn't       no            nor           not         \n",
      "now           o             of            off           on          \n",
      "once          only          or            other         our         \n",
      "ours          ourselves     out           over          own         \n",
      "re            s             same          shan          shan't      \n",
      "she           she'd         she'll        she's         should      \n",
      "should've     shouldn       shouldn't     so            some        \n",
      "such          t             than          that          that'll     \n",
      "the           their         theirs        them          themselves  \n",
      "then          there         these         they          they'd      \n",
      "they'll       they're       they've       this          those       \n",
      "through       to            too           under         until       \n",
      "up            ve            very          was           wasn        \n",
      "wasn't        we            we'd          we'll         we're       \n",
      "we've         were          weren         weren't       what        \n",
      "when          where         which         while         who         \n",
      "whom          why           will          with          won         \n",
      "won't         wouldn        wouldn't      y             you         \n",
      "you'd         you'll        you're        you've        your        \n",
      "yours         yourself      yourselves  \n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load English stop words\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "print(\"üî§ English Stop Words in NLTK:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Count: {len(english_stopwords)} words\\n\")\n",
    "\n",
    "# Display all stop words in sorted order\n",
    "sorted_stopwords = sorted(english_stopwords)\n",
    "print(\"Complete List:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Display in columns for better readability\n",
    "columns = 5\n",
    "for i in range(0, len(sorted_stopwords), columns):\n",
    "    row = sorted_stopwords[i:i+columns]\n",
    "    print(\"  \".join(f\"{word:12}\" for word in row))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636c975",
   "metadata": {},
   "source": [
    "### üìä Observation 3:\n",
    "- NLTK's English stop words list contains approximately **179 words**\n",
    "- The list includes common words like: a, an, the, is, are, was, were, etc.\n",
    "- Stop words are stored as a **Python set** for efficient lookup (O(1) time complexity)\n",
    "- All words are in **lowercase** format\n",
    "- The list includes:\n",
    "  - **Pronouns**: I, you, he, she, it, we, they, me, him, her, us, them\n",
    "  - **Articles**: a, an, the\n",
    "  - **Prepositions**: in, on, at, to, from, with, by, for, about\n",
    "  - **Conjunctions**: and, but, or, so, because, if, when, while\n",
    "  - **Auxiliary verbs**: is, am, are, was, were, be, been, being, have, has, had, do, does, did\n",
    "  - **Common adverbs**: very, too, so, just, now, then\n",
    "- Some potentially meaningful words are also included (e.g., \"not\", \"no\") which might be important for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1773a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 4: Checking Specific Words\n",
    "\n",
    "**Objective**: Learn how to check if a specific word is a stop word\n",
    "\n",
    "**What we'll do**:\n",
    "- Test various words to see if they're stop words\n",
    "- Understand the lookup mechanism\n",
    "- Compare common vs. meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ec8a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking if words are stop words:\n",
      "============================================================\n",
      "the                  -> ‚úÖ STOP WORD\n",
      "machine              -> ‚ùå NOT a stop word\n",
      "is                   -> ‚úÖ STOP WORD\n",
      "learning             -> ‚ùå NOT a stop word\n",
      "and                  -> ‚úÖ STOP WORD\n",
      "artificial           -> ‚ùå NOT a stop word\n",
      "intelligence         -> ‚ùå NOT a stop word\n",
      "are                  -> ‚úÖ STOP WORD\n",
      "revolutionizing      -> ‚ùå NOT a stop word\n",
      "not                  -> ‚úÖ STOP WORD\n",
      "good                 -> ‚ùå NOT a stop word\n",
      "python               -> ‚ùå NOT a stop word\n",
      "programming          -> ‚ùå NOT a stop word\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   Stop words: 5/13\n",
      "   Meaningful words: 8/13\n",
      "   Ratio: 38.5% are stop words\n"
     ]
    }
   ],
   "source": [
    "# Test various words\n",
    "test_words = ['the', 'machine', 'is', 'learning', 'and', 'artificial', 'intelligence', \n",
    "              'are', 'revolutionizing', 'not', 'good', 'python', 'programming']\n",
    "\n",
    "print(\"üîç Checking if words are stop words:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word in test_words:\n",
    "    is_stopword = word.lower() in english_stopwords\n",
    "    status = \"‚úÖ STOP WORD\" if is_stopword else \"‚ùå NOT a stop word\"\n",
    "    print(f\"{word:20} -> {status}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count stop words vs meaningful words\n",
    "stop_count = sum(1 for word in test_words if word.lower() in english_stopwords)\n",
    "meaningful_count = len(test_words) - stop_count\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Stop words: {stop_count}/{len(test_words)}\")\n",
    "print(f\"   Meaningful words: {meaningful_count}/{len(test_words)}\")\n",
    "print(f\"   Ratio: {stop_count/len(test_words)*100:.1f}% are stop words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf7f62",
   "metadata": {},
   "source": [
    "### üìä Observation 4:\n",
    "- Checking if a word is a stop word is **very fast** due to set-based lookup\n",
    "- Common words like \"the\", \"is\", \"and\", \"are\", \"not\" are identified as stop words\n",
    "- Domain-specific and meaningful words like \"machine\", \"learning\", \"artificial\", \"intelligence\", \"python\" are NOT stop words\n",
    "- **Important**: The check is **case-sensitive** unless you convert to lowercase first\n",
    "- Notice that \"not\" is a stop word, which could be problematic for sentiment analysis\n",
    "- The distribution shows that even in a tech-related vocabulary, a significant portion can be stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b2902",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 5: Removing Stop Words from a Simple Sentence\n",
    "\n",
    "**Objective**: Demonstrate basic stop word removal from text\n",
    "\n",
    "**What we'll do**:\n",
    "- Take a sample sentence\n",
    "- Tokenize it into words\n",
    "- Remove stop words\n",
    "- Compare original vs. filtered text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828650a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Sentence:\n",
      "================================================================================\n",
      "Natural Language Processing is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
      "================================================================================\n",
      "\n",
      "üî§ Total tokens: 22\n",
      "Tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.']\n",
      "\n",
      "‚úÇÔ∏è After removing stop words:\n",
      "================================================================================\n",
      "Filtered tokens: ['Natural', 'Language', 'Processing', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', '.']\n",
      "Total filtered tokens: 13\n",
      "\n",
      "Filtered sentence: Natural Language Processing subfield artificial intelligence focuses interaction computers humans natural language .\n",
      "================================================================================\n",
      "\n",
      "üìä Statistics:\n",
      "   Original tokens: 22\n",
      "   Filtered tokens: 13\n",
      "   Removed tokens: 9\n",
      "   Reduction: 40.9%\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "sentence = \"Natural Language Processing is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\"\n",
    "\n",
    "print(\"üìù Original Sentence:\")\n",
    "print(\"=\" * 80)\n",
    "print(sentence)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "print(f\"\\nüî§ Total tokens: {len(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Remove stop words (case-insensitive)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in english_stopwords]\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è After removing stop words:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Filtered tokens: {filtered_tokens}\")\n",
    "print(f\"Total filtered tokens: {len(filtered_tokens)}\")\n",
    "\n",
    "# Reconstruct sentence\n",
    "filtered_sentence = ' '.join(filtered_tokens)\n",
    "print(f\"\\nFiltered sentence: {filtered_sentence}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistics\n",
    "removed_count = len(tokens) - len(filtered_tokens)\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Original tokens: {len(tokens)}\")\n",
    "print(f\"   Filtered tokens: {len(filtered_tokens)}\")\n",
    "print(f\"   Removed tokens: {removed_count}\")\n",
    "print(f\"   Reduction: {removed_count/len(tokens)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497dcfba",
   "metadata": {},
   "source": [
    "### üìä Observation 5:\n",
    "- Removing stop words significantly **reduces the text size** (typically 30-50% reduction)\n",
    "- The **core meaning** is preserved: \"Natural Language Processing\", \"subfield\", \"artificial intelligence\", \"interaction\", \"computers\", \"humans\"\n",
    "- **Punctuation** is kept by `word_tokenize()` but doesn't affect stop word filtering\n",
    "- The filtered sentence loses grammatical structure but retains key concepts\n",
    "- This technique is useful for:\n",
    "  - **Keyword extraction**: Focus on important terms\n",
    "  - **Search indexing**: Reduce index size\n",
    "  - **Text classification**: Improve feature selection\n",
    "- **Trade-off**: We gain efficiency but lose context and grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903a3e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 6: Handling Punctuation Along with Stop Words\n",
    "\n",
    "**Objective**: Clean text more thoroughly by removing both stop words and punctuation\n",
    "\n",
    "**What we'll do**:\n",
    "- Remove stop words\n",
    "- Remove punctuation marks\n",
    "- Compare different cleaning approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20419bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Sentence:\n",
      "================================================================================\n",
      "Hello! My name is John, and I'm learning NLP. It's fascinating, isn't it?\n",
      "================================================================================\n",
      "\n",
      "üî§ Tokens: ['Hello', '!', 'My', 'name', 'is', 'John', ',', 'and', 'I', \"'m\", 'learning', 'NLP', '.', 'It', \"'s\", 'fascinating', ',', 'is', \"n't\", 'it', '?']\n",
      "Total: 21 tokens\n",
      "\n",
      "‚úÇÔ∏è Method 1: Remove stop words only\n",
      "Result: ['Hello', '!', 'name', 'John', ',', \"'m\", 'learning', 'NLP', '.', \"'s\", 'fascinating', ',', \"n't\", '?']\n",
      "Count: 14 tokens\n",
      "\n",
      "‚úÇÔ∏è Method 2: Remove stop words + punctuation\n",
      "Result: ['Hello', 'name', 'John', \"'m\", 'learning', 'NLP', \"'s\", 'fascinating', \"n't\"]\n",
      "Count: 9 tokens\n",
      "\n",
      "‚úÇÔ∏è Method 3: Remove stop words + keep only alphabetic\n",
      "Result: ['Hello', 'name', 'John', 'learning', 'NLP', 'fascinating']\n",
      "Count: 6 tokens\n",
      "\n",
      "================================================================================\n",
      "üìä Comparison:\n",
      "   Original: 21 tokens\n",
      "   Stop words removed: 14 tokens (66.7%)\n",
      "   Stop words + punctuation: 9 tokens (42.9%)\n",
      "   Only alphabetic words: 6 tokens (28.6%)\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence with punctuation\n",
    "sentence = \"Hello! My name is John, and I'm learning NLP. It's fascinating, isn't it?\"\n",
    "\n",
    "print(\"üìù Original Sentence:\")\n",
    "print(\"=\" * 80)\n",
    "print(sentence)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(sentence)\n",
    "print(f\"\\nüî§ Tokens: {tokens}\")\n",
    "print(f\"Total: {len(tokens)} tokens\")\n",
    "\n",
    "# Method 1: Remove only stop words\n",
    "filtered_stopwords_only = [word for word in tokens if word.lower() not in english_stopwords]\n",
    "print(f\"\\n‚úÇÔ∏è Method 1: Remove stop words only\")\n",
    "print(f\"Result: {filtered_stopwords_only}\")\n",
    "print(f\"Count: {len(filtered_stopwords_only)} tokens\")\n",
    "\n",
    "# Method 2: Remove stop words AND punctuation\n",
    "filtered_clean = [word for word in tokens \n",
    "                  if word.lower() not in english_stopwords \n",
    "                  and word not in string.punctuation]\n",
    "print(f\"\\n‚úÇÔ∏è Method 2: Remove stop words + punctuation\")\n",
    "print(f\"Result: {filtered_clean}\")\n",
    "print(f\"Count: {len(filtered_clean)} tokens\")\n",
    "\n",
    "# Method 3: Remove stop words, punctuation, and keep only alphabetic words\n",
    "filtered_alpha = [word for word in tokens \n",
    "                  if word.lower() not in english_stopwords \n",
    "                  and word.isalpha()]\n",
    "print(f\"\\n‚úÇÔ∏è Method 3: Remove stop words + keep only alphabetic\")\n",
    "print(f\"Result: {filtered_alpha}\")\n",
    "print(f\"Count: {len(filtered_alpha)} tokens\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Comparison:\")\n",
    "print(f\"   Original: {len(tokens)} tokens\")\n",
    "print(f\"   Stop words removed: {len(filtered_stopwords_only)} tokens ({len(filtered_stopwords_only)/len(tokens)*100:.1f}%)\")\n",
    "print(f\"   Stop words + punctuation: {len(filtered_clean)} tokens ({len(filtered_clean)/len(tokens)*100:.1f}%)\")\n",
    "print(f\"   Only alphabetic words: {len(filtered_alpha)} tokens ({len(filtered_alpha)/len(tokens)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4b7fc",
   "metadata": {},
   "source": [
    "### üìä Observation 6:\n",
    "- **Method 1** (stop words only): Keeps punctuation like \"!\", \",\", \".\", \"'\", which may not be useful\n",
    "- **Method 2** (stop words + punctuation): Removes `string.punctuation` (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~) but keeps contractions like \"I'm\", \"It's\"\n",
    "- **Method 3** (alphabetic only): Most aggressive cleaning - removes all non-alphabetic characters including contractions\n",
    "- **Key insight**: `word.isalpha()` is the cleanest but might remove useful information (e.g., \"COVID-19\" would be removed)\n",
    "- **Best practice**: Choose the method based on your use case:\n",
    "  - **Text classification**: Method 3 (alphabetic only)\n",
    "  - **Sentiment analysis**: Method 1 (keep punctuation like \"!\" for emotion)\n",
    "  - **General cleaning**: Method 2 (balanced approach)\n",
    "- Typically results in 60-80% reduction in token count from original text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f83e8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 7: Processing a Longer Text (Paragraph)\n",
    "\n",
    "**Objective**: Apply stop word removal to a realistic text passage\n",
    "\n",
    "**What we'll do**:\n",
    "- Process a full paragraph\n",
    "- Analyze word frequency before and after\n",
    "- Visualize the impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79848d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Paragraph:\n",
      "================================================================================\n",
      "Machine learning is a subset of artificial intelligence that enables computers to learn \n",
      "from data without being explicitly programmed. It focuses on the development of algorithms \n",
      "that can access data and use it to learn for themselves. The process of learning begins \n",
      "with observations or data, such as examples, direct experience, or instruction, in order \n",
      "to look for patterns in data and make better decisions in the future based on the examples \n",
      "that we provide. The primary aim is to allow the computers to learn automatically without \n",
      "human intervention or assistance and adjust actions accordingly.\n",
      "================================================================================\n",
      "\n",
      "üìä Analysis:\n",
      "================================================================================\n",
      "Total words (alphabetic): 95\n",
      "Stop words found: 43\n",
      "Meaningful words: 52\n",
      "Stop words percentage: 45.3%\n",
      "\n",
      "‚úÇÔ∏è Meaningful words after removing stop words:\n",
      "================================================================================\n",
      "machine learning subset artificial intelligence enables computers learn data without explicitly programmed focuses development algorithms access data use learn process learning begins observations data examples direct experience instruction order look patterns data make better decisions future based examples provide primary aim allow computers learn automatically without human intervention assistance adjust actions accordingly\n",
      "\n",
      "üìà Top 10 Most Frequent Meaningful Words:\n",
      "================================================================================\n",
      "data                 : 4 times ‚ñà‚ñà‚ñà‚ñà\n",
      "learn                : 3 times ‚ñà‚ñà‚ñà\n",
      "learning             : 2 times ‚ñà‚ñà\n",
      "computers            : 2 times ‚ñà‚ñà\n",
      "without              : 2 times ‚ñà‚ñà\n",
      "examples             : 2 times ‚ñà‚ñà\n",
      "machine              : 1 times ‚ñà\n",
      "subset               : 1 times ‚ñà\n",
      "artificial           : 1 times ‚ñà\n",
      "intelligence         : 1 times ‚ñà\n"
     ]
    }
   ],
   "source": [
    "# Sample paragraph about Machine Learning\n",
    "paragraph = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn \n",
    "from data without being explicitly programmed. It focuses on the development of algorithms \n",
    "that can access data and use it to learn for themselves. The process of learning begins \n",
    "with observations or data, such as examples, direct experience, or instruction, in order \n",
    "to look for patterns in data and make better decisions in the future based on the examples \n",
    "that we provide. The primary aim is to allow the computers to learn automatically without \n",
    "human intervention or assistance and adjust actions accordingly.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Original Paragraph:\")\n",
    "print(\"=\" * 80)\n",
    "print(paragraph.strip())\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tokenize and convert to lowercase\n",
    "tokens = word_tokenize(paragraph.lower())\n",
    "\n",
    "# Remove punctuation and get only alphabetic words\n",
    "words_only = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "# Separate into stop words and meaningful words\n",
    "stop_words_found = [word for word in words_only if word in english_stopwords]\n",
    "meaningful_words = [word for word in words_only if word not in english_stopwords]\n",
    "\n",
    "print(f\"\\nüìä Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total words (alphabetic): {len(words_only)}\")\n",
    "print(f\"Stop words found: {len(stop_words_found)}\")\n",
    "print(f\"Meaningful words: {len(meaningful_words)}\")\n",
    "print(f\"Stop words percentage: {len(stop_words_found)/len(words_only)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è Meaningful words after removing stop words:\")\n",
    "print(\"=\" * 80)\n",
    "print(' '.join(meaningful_words))\n",
    "\n",
    "# Count word frequency in meaningful words\n",
    "from collections import Counter\n",
    "word_freq = Counter(meaningful_words)\n",
    "\n",
    "print(f\"\\nüìà Top 10 Most Frequent Meaningful Words:\")\n",
    "print(\"=\" * 80)\n",
    "for word, count in word_freq.most_common(10):\n",
    "    print(f\"{word:20} : {count} times {'‚ñà' * count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7183c44",
   "metadata": {},
   "source": [
    "### üìä Observation 7:\n",
    "- In real-world text, **40-50% of words are typically stop words**\n",
    "- After removal, the text becomes a **keyword summary** that captures the main topic\n",
    "- Repeated meaningful words (like \"data\", \"learn\", \"computers\") become more visible\n",
    "- This is exactly what we want for:\n",
    "  - **Topic modeling**: Identify what the text is about\n",
    "  - **Document classification**: Categorize based on key terms\n",
    "  - **Information retrieval**: Match queries to relevant documents\n",
    "- The most frequent words clearly indicate the paragraph is about \"machine learning\", \"data\", \"computers\", and \"learning\"\n",
    "- **Important finding**: The context is lost but the **essence is preserved**\n",
    "- This technique dramatically reduces the **vocabulary size** for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74a591",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 8: Customizing Stop Words List\n",
    "\n",
    "**Objective**: Learn how to add or remove words from the stop words list\n",
    "\n",
    "**What we'll do**:\n",
    "- Add custom stop words (domain-specific)\n",
    "- Remove certain stop words (e.g., for sentiment analysis)\n",
    "- Create a custom stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2bd0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Customizing Stop Words List\n",
      "================================================================================\n",
      "Original stop words count: 198\n",
      "\n",
      "‚ûï Added custom stop words: ['product', 'item', 'thing', 'stuff', 'review']\n",
      "New count: 203\n",
      "\n",
      "‚ûñ Removed for sentiment analysis: ['not', 'no', 'nor']\n",
      "New count: 200\n",
      "\n",
      "================================================================================\n",
      "üß™ Testing Custom Stop Words:\n",
      "================================================================================\n",
      "\n",
      "1. Original: This product is not good at all.\n",
      "   Default filter: ['product', 'good']\n",
      "   Custom filter: ['not', 'good']\n",
      "\n",
      "2. Original: The item was never delivered.\n",
      "   Default filter: ['item', 'never', 'delivered']\n",
      "   Custom filter: ['never', 'delivered']\n",
      "\n",
      "3. Original: I have no complaints about this thing.\n",
      "   Default filter: ['complaints', 'thing']\n",
      "   Custom filter: ['no', 'complaints']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the default stop words\n",
    "custom_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "print(\"üîß Customizing Stop Words List\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Original count\n",
    "print(f\"Original stop words count: {len(custom_stopwords)}\")\n",
    "\n",
    "# Scenario 1: Add domain-specific common words\n",
    "# For example, in product reviews, these might be too common\n",
    "additional_words = ['product', 'item', 'thing', 'stuff', 'review']\n",
    "custom_stopwords.update(additional_words)\n",
    "\n",
    "print(f\"\\n‚ûï Added custom stop words: {additional_words}\")\n",
    "print(f\"New count: {len(custom_stopwords)}\")\n",
    "\n",
    "# Scenario 2: Remove important words for sentiment analysis\n",
    "# Words like \"not\", \"no\", \"never\" are crucial for sentiment\n",
    "sentiment_important = ['not', 'no', 'nor', 'never', 'neither', 'nobody', 'nothing', 'nowhere']\n",
    "removed_words = []\n",
    "for word in sentiment_important:\n",
    "    if word in custom_stopwords:\n",
    "        custom_stopwords.remove(word)\n",
    "        removed_words.append(word)\n",
    "\n",
    "print(f\"\\n‚ûñ Removed for sentiment analysis: {removed_words}\")\n",
    "print(f\"New count: {len(custom_stopwords)}\")\n",
    "\n",
    "# Test with sample sentences\n",
    "test_sentences = [\n",
    "    \"This product is not good at all.\",\n",
    "    \"The item was never delivered.\",\n",
    "    \"I have no complaints about this thing.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üß™ Testing Custom Stop Words:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    \n",
    "    # Filter with default stop words\n",
    "    filtered_default = [w for w in tokens if w.isalpha() and w not in english_stopwords]\n",
    "    \n",
    "    # Filter with custom stop words\n",
    "    filtered_custom = [w for w in tokens if w.isalpha() and w not in custom_stopwords]\n",
    "    \n",
    "    print(f\"\\n{i}. Original: {sentence}\")\n",
    "    print(f\"   Default filter: {filtered_default}\")\n",
    "    print(f\"   Custom filter: {filtered_custom}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574f336",
   "metadata": {},
   "source": [
    "### üìä Observation 8:\n",
    "- **Customization is crucial** for domain-specific applications\n",
    "- **Adding stop words**: Useful when certain common words in your domain don't carry meaning\n",
    "  - E-commerce: \"product\", \"item\", \"purchase\"\n",
    "  - News articles: \"said\", \"according\", \"reported\"\n",
    "  - Social media: \"lol\", \"omg\", \"btw\"\n",
    "- **Removing stop words**: Essential for tasks where negation matters\n",
    "  - Sentiment analysis: Keep \"not\", \"no\", \"never\", \"neither\"\n",
    "  - Question answering: Keep \"what\", \"where\", \"when\", \"who\", \"why\", \"how\"\n",
    "- **Key learning**: The sentence \"This product is **not** good\" becomes very different:\n",
    "  - Default filtering: [\"product\", \"good\"] - loses negation! ‚ö†Ô∏è\n",
    "  - Custom filtering: [\"not\", \"good\"] - preserves sentiment! ‚úÖ\n",
    "- **Best practice**: Always analyze your specific use case before deciding on stop words\n",
    "- Use `set.update()` to add multiple words and `set.remove()` or `set.discard()` to remove words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63d824",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 9: Comparing Multiple Languages\n",
    "\n",
    "**Objective**: Compare stop words across different languages\n",
    "\n",
    "**What we'll do**:\n",
    "- Load stop words for multiple languages\n",
    "- Compare the number of stop words\n",
    "- Look at overlapping concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46445b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Comparing Stop Words Across Languages\n",
      "================================================================================\n",
      "English         :  198 stop words\n",
      "Spanish         :  313 stop words\n",
      "French          :  157 stop words\n",
      "German          :  232 stop words\n",
      "Italian         :  279 stop words\n",
      "================================================================================\n",
      "\n",
      "üìù Sample Stop Words (first 10):\n",
      "================================================================================\n",
      "\n",
      "English:\n",
      "  a, about, above, after, again, against, ain, all, am, an\n",
      "\n",
      "Spanish:\n",
      "  a, al, algo, algunas, algunos, ante, antes, como, con, contra\n",
      "\n",
      "French:\n",
      "  ai, aie, aient, aies, ait, as, au, aura, aurai, auraient\n",
      "\n",
      "German:\n",
      "  aber, alle, allem, allen, aller, alles, als, also, am, an\n",
      "\n",
      "Italian:\n",
      "  a, abbia, abbiamo, abbiano, abbiate, ad, agl, agli, ai, al\n",
      "\n",
      "================================================================================\n",
      "üîç Interesting Findings:\n",
      "================================================================================\n",
      "English         articles: the\n",
      "Spanish         articles: el, la, los, las\n",
      "French          articles: le, la, les\n",
      "German          articles: der, die, das, den, dem, des\n",
      "Italian         articles: il, lo, la, i, gli, le\n",
      "\n",
      "================================================================================\n",
      "üìä Statistics:\n",
      "   Average stop words per language: 235.8\n",
      "   Max: 313 (spanish)\n",
      "   Min: 157 (french)\n"
     ]
    }
   ],
   "source": [
    "# Compare stop words across languages\n",
    "languages_to_compare = ['english', 'spanish', 'french', 'german', 'italian']\n",
    "\n",
    "print(\"üåç Comparing Stop Words Across Languages\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "language_stopwords = {}\n",
    "for lang in languages_to_compare:\n",
    "    language_stopwords[lang] = set(stopwords.words(lang))\n",
    "    print(f\"{lang.capitalize():15} : {len(language_stopwords[lang]):4} stop words\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample stop words from each language\n",
    "print(\"\\nüìù Sample Stop Words (first 10):\")\n",
    "print(\"=\" * 80)\n",
    "for lang in languages_to_compare:\n",
    "    samples = sorted(list(language_stopwords[lang]))[:10]\n",
    "    print(f\"\\n{lang.capitalize()}:\")\n",
    "    print(f\"  {', '.join(samples)}\")\n",
    "\n",
    "# Find common patterns (transliterated concepts)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç Interesting Findings:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Example: Check if each language has word for \"the\"\n",
    "the_equivalents = {\n",
    "    'english': ['the'],\n",
    "    'spanish': ['el', 'la', 'los', 'las'],\n",
    "    'french': ['le', 'la', 'les'],\n",
    "    'german': ['der', 'die', 'das', 'den', 'dem', 'des'],\n",
    "    'italian': ['il', 'lo', 'la', 'i', 'gli', 'le']\n",
    "}\n",
    "\n",
    "for lang, articles in the_equivalents.items():\n",
    "    found = [art for art in articles if art in language_stopwords[lang]]\n",
    "    print(f\"{lang.capitalize():15} articles: {', '.join(found)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Statistics:\")\n",
    "print(f\"   Average stop words per language: {sum(len(sw) for sw in language_stopwords.values()) / len(language_stopwords):.1f}\")\n",
    "print(f\"   Max: {max(len(sw) for sw in language_stopwords.values())} ({max(language_stopwords.items(), key=lambda x: len(x[1]))[0]})\")\n",
    "print(f\"   Min: {min(len(sw) for sw in language_stopwords.values())} ({min(language_stopwords.items(), key=lambda x: len(x[1]))[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aedd141",
   "metadata": {},
   "source": [
    "### üìä Observation 9:\n",
    "- **Different languages have different numbers** of stop words (typically 100-300 words)\n",
    "- **German tends to have more** stop words due to compound words and complex grammar\n",
    "- **Articles vary significantly** across languages:\n",
    "  - English: 1 definite article (\"the\")\n",
    "  - Spanish/Italian: Gender-based articles (masculine/feminine, singular/plural)\n",
    "  - German: Case-based articles (nominative, accusative, dative, genitive)\n",
    "  - French: Similar to Spanish with le/la/les\n",
    "- **Universal concepts** appear in all languages: pronouns, conjunctions, prepositions\n",
    "- **Cultural differences** exist in what's considered a \"stop word\"\n",
    "- **Important for multilingual NLP**: Must use language-specific stop word lists\n",
    "- Never mix stop words from different languages in the same text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d8d3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 10: Creating a Text Processing Function\n",
    "\n",
    "**Objective**: Build a reusable function for text cleaning with stop word removal\n",
    "\n",
    "**What we'll do**:\n",
    "- Create a comprehensive text cleaning function\n",
    "- Add optional parameters for different cleaning levels\n",
    "- Test with various inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8caee1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Text Cleaning Function\n",
      "================================================================================\n",
      "Original: The quick brown fox jumps over the lazy dog. This is a simple sentence for testing!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Full cleaning (default):\n",
      "   ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'simple', 'sentence', 'testing']\n",
      "\n",
      "2Ô∏è‚É£ Keep stop words:\n",
      "   ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'this', 'is', 'a', 'simple', 'sentence', 'for', 'testing']\n",
      "\n",
      "3Ô∏è‚É£ Keep punctuation:\n",
      "   ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.', 'simple', 'sentence', 'testing', '!']\n",
      "\n",
      "4Ô∏è‚É£ Return as string:\n",
      "   quick brown fox jumps lazy dog simple sentence testing\n",
      "\n",
      "5Ô∏è‚É£ Custom stop words (keeping 'not', 'no'):\n",
      "   Original: This is not a bad product, but it's not great either.\n",
      "   Cleaned: not bad product not great either\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text, \n",
    "               remove_stopwords=True, \n",
    "               remove_punctuation=True, \n",
    "               lowercase=True,\n",
    "               custom_stopwords=None,\n",
    "               return_string=False):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to clean\n",
    "    remove_stopwords : bool\n",
    "        Whether to remove stop words (default: True)\n",
    "    remove_punctuation : bool\n",
    "        Whether to remove punctuation (default: True)\n",
    "    lowercase : bool\n",
    "        Whether to convert to lowercase (default: True)\n",
    "    custom_stopwords : set\n",
    "        Custom stop words list (default: None, uses NLTK English)\n",
    "    return_string : bool\n",
    "        Whether to return string or list of tokens (default: False)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list or str\n",
    "        Cleaned tokens as list or joined string\n",
    "    \"\"\"\n",
    "    # Convert to lowercase if specified\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation if specified\n",
    "    if remove_punctuation:\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Remove stop words if specified\n",
    "    if remove_stopwords:\n",
    "        if custom_stopwords is None:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "        else:\n",
    "            stop_words = custom_stopwords\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Return as string or list\n",
    "    if return_string:\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return tokens\n",
    "\n",
    "\n",
    "# Test the function with different configurations\n",
    "test_text = \"The quick brown fox jumps over the lazy dog. This is a simple sentence for testing!\"\n",
    "\n",
    "print(\"üß™ Testing Text Cleaning Function\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: Full cleaning\n",
    "result1 = clean_text(test_text)\n",
    "print(f\"\\n1Ô∏è‚É£ Full cleaning (default):\")\n",
    "print(f\"   {result1}\")\n",
    "\n",
    "# Test 2: Keep stop words\n",
    "result2 = clean_text(test_text, remove_stopwords=False)\n",
    "print(f\"\\n2Ô∏è‚É£ Keep stop words:\")\n",
    "print(f\"   {result2}\")\n",
    "\n",
    "# Test 3: Keep punctuation\n",
    "result3 = clean_text(test_text, remove_punctuation=False)\n",
    "print(f\"\\n3Ô∏è‚É£ Keep punctuation:\")\n",
    "print(f\"   {result3}\")\n",
    "\n",
    "# Test 4: Return as string\n",
    "result4 = clean_text(test_text, return_string=True)\n",
    "print(f\"\\n4Ô∏è‚É£ Return as string:\")\n",
    "print(f\"   {result4}\")\n",
    "\n",
    "# Test 5: Custom stop words\n",
    "custom_stops = set(stopwords.words('english')) - {'not', 'no'}\n",
    "test_sentiment = \"This is not a bad product, but it's not great either.\"\n",
    "result5 = clean_text(test_sentiment, custom_stopwords=custom_stops, return_string=True)\n",
    "print(f\"\\n5Ô∏è‚É£ Custom stop words (keeping 'not', 'no'):\")\n",
    "print(f\"   Original: {test_sentiment}\")\n",
    "print(f\"   Cleaned: {result5}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd41d0e",
   "metadata": {},
   "source": [
    "### üìä Observation 10:\n",
    "- Creating a **reusable function** is essential for consistent text preprocessing\n",
    "- **Flexibility is key**: Parameters allow different cleaning strategies\n",
    "- The function demonstrates:\n",
    "  - ‚úÖ **Modularity**: Each cleaning step can be toggled on/off\n",
    "  - ‚úÖ **Customization**: Support for custom stop words\n",
    "  - ‚úÖ **Output flexibility**: Can return list or string\n",
    "  - ‚úÖ **Documentation**: Clear docstring explaining parameters\n",
    "- **Best practices implemented**:\n",
    "  - Default behavior is most common use case\n",
    "  - Optional parameters for edge cases\n",
    "  - Clear parameter names\n",
    "  - Consistent return types\n",
    "- This function can be **easily extended** with:\n",
    "  - Stemming/lemmatization\n",
    "  - Removing numbers\n",
    "  - Handling contractions\n",
    "  - Language detection and multilingual support\n",
    "- **Production tip**: Save this as a utility module and import across projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce5263",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 11: Real-World Application - Text Classification Preparation\n",
    "\n",
    "**Objective**: Demonstrate how stop word removal helps in preparing data for text classification\n",
    "\n",
    "**What we'll do**:\n",
    "- Create sample movie reviews\n",
    "- Process them with and without stop word removal\n",
    "- Compare vocabulary size and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e699ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Text Classification Example: Movie Reviews\n",
      "================================================================================\n",
      "\n",
      "üìä Processing WITHOUT stop word removal:\n",
      "--------------------------------------------------------------------------------\n",
      "Total tokens: 78\n",
      "Unique vocabulary size: 53\n",
      "Sample vocabulary: ['absolutely', 'acting', 'amazing', 'an', 'and', 'any', 'best', 'boring', 'cinematic', 'completely', 'did', 'director', 'disappointed', 'engaged', 'ever', 'expectations', 'experience', 'failed', 'fantastic', 'horrible']\n",
      "\n",
      "üìä Processing WITH stop word removal:\n",
      "--------------------------------------------------------------------------------\n",
      "Total tokens: 43\n",
      "Unique vocabulary size: 39\n",
      "Sample vocabulary: ['absolutely', 'acting', 'amazing', 'best', 'boring', 'cinematic', 'completely', 'director', 'disappointed', 'engaged', 'ever', 'expectations', 'experience', 'failed', 'fantastic', 'horrible', 'incredible', 'job', 'kept', 'made']\n",
      "\n",
      "================================================================================\n",
      "üìà Most Common Words (WITH stop words):\n",
      "--------------------------------------------------------------------------------\n",
      "the                  : 10 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "was                  :  5 ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "movie                :  4 ‚ñà‚ñà‚ñà‚ñà\n",
      "and                  :  4 ‚ñà‚ñà‚ñà‚ñà\n",
      "this                 :  2 ‚ñà‚ñà\n",
      "acting               :  2 ‚ñà‚ñà\n",
      "an                   :  2 ‚ñà‚ñà\n",
      "i                    :  2 ‚ñà‚ñà\n",
      "my                   :  2 ‚ñà‚ñà\n",
      "of                   :  2 ‚ñà‚ñà\n",
      "absolutely           :  1 ‚ñà\n",
      "fantastic            :  1 ‚ñà\n",
      "superb               :  1 ‚ñà\n",
      "plot                 :  1 ‚ñà\n",
      "kept                 :  1 ‚ñà\n",
      "\n",
      "================================================================================\n",
      "üìà Most Common Words (WITHOUT stop words):\n",
      "--------------------------------------------------------------------------------\n",
      "movie                :  4 ‚ñà‚ñà‚ñà‚ñà\n",
      "acting               :  2 ‚ñà‚ñà\n",
      "absolutely           :  1 ‚ñà\n",
      "fantastic            :  1 ‚ñà\n",
      "superb               :  1 ‚ñà\n",
      "plot                 :  1 ‚ñà\n",
      "kept                 :  1 ‚ñà\n",
      "engaged              :  1 ‚ñà\n",
      "throughout           :  1 ‚ñà\n",
      "terrible             :  1 ‚ñà\n",
      "story                :  1 ‚ñà\n",
      "made                 :  1 ‚ñà\n",
      "sense                :  1 ‚ñà\n",
      "horrible             :  1 ‚ñà\n",
      "amazing              :  1 ‚ñà\n",
      "\n",
      "================================================================================\n",
      "üìä Comparison Statistics:\n",
      "================================================================================\n",
      "Token reduction: 44.9%\n",
      "Vocabulary reduction: 26.4%\n",
      "\n",
      "üí° Impact on Machine Learning:\n",
      "   - Feature space reduced by 26.4%\n",
      "   - Model training will be faster\n",
      "   - Focus shifts to sentiment-bearing words\n"
     ]
    }
   ],
   "source": [
    "# Sample movie reviews (positive and negative)\n",
    "movie_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
    "    \"Terrible movie. The story made no sense and the acting was horrible.\",\n",
    "    \"An amazing cinematic experience. The visuals were stunning and the soundtrack was perfect.\",\n",
    "    \"I wasted my time watching this. The movie was boring and predictable.\",\n",
    "    \"One of the best movies I've ever seen. The director did an incredible job!\",\n",
    "    \"Completely disappointed. The movie failed to meet any of my expectations.\"\n",
    "]\n",
    "\n",
    "print(\"üé¨ Text Classification Example: Movie Reviews\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process WITHOUT stop word removal\n",
    "print(\"\\nüìä Processing WITHOUT stop word removal:\")\n",
    "print(\"-\" * 80)\n",
    "all_words_with_stops = []\n",
    "for review in movie_reviews:\n",
    "    tokens = clean_text(review, remove_stopwords=False)\n",
    "    all_words_with_stops.extend(tokens)\n",
    "\n",
    "vocab_with_stops = set(all_words_with_stops)\n",
    "print(f\"Total tokens: {len(all_words_with_stops)}\")\n",
    "print(f\"Unique vocabulary size: {len(vocab_with_stops)}\")\n",
    "print(f\"Sample vocabulary: {sorted(list(vocab_with_stops))[:20]}\")\n",
    "\n",
    "# Process WITH stop word removal\n",
    "print(\"\\nüìä Processing WITH stop word removal:\")\n",
    "print(\"-\" * 80)\n",
    "all_words_without_stops = []\n",
    "for review in movie_reviews:\n",
    "    tokens = clean_text(review, remove_stopwords=True)\n",
    "    all_words_without_stops.extend(tokens)\n",
    "\n",
    "vocab_without_stops = set(all_words_without_stops)\n",
    "print(f\"Total tokens: {len(all_words_without_stops)}\")\n",
    "print(f\"Unique vocabulary size: {len(vocab_without_stops)}\")\n",
    "print(f\"Sample vocabulary: {sorted(list(vocab_without_stops))[:20]}\")\n",
    "\n",
    "# Compare word frequencies\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà Most Common Words (WITH stop words):\")\n",
    "print(\"-\" * 80)\n",
    "freq_with_stops = Counter(all_words_with_stops)\n",
    "for word, count in freq_with_stops.most_common(15):\n",
    "    print(f\"{word:20} : {count:2} {'‚ñà' * count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà Most Common Words (WITHOUT stop words):\")\n",
    "print(\"-\" * 80)\n",
    "freq_without_stops = Counter(all_words_without_stops)\n",
    "for word, count in freq_without_stops.most_common(15):\n",
    "    print(f\"{word:20} : {count:2} {'‚ñà' * count}\")\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Comparison Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "reduction_tokens = (1 - len(all_words_without_stops) / len(all_words_with_stops)) * 100\n",
    "reduction_vocab = (1 - len(vocab_without_stops) / len(vocab_with_stops)) * 100\n",
    "\n",
    "print(f\"Token reduction: {reduction_tokens:.1f}%\")\n",
    "print(f\"Vocabulary reduction: {reduction_vocab:.1f}%\")\n",
    "print(f\"\\nüí° Impact on Machine Learning:\")\n",
    "print(f\"   - Feature space reduced by {reduction_vocab:.1f}%\")\n",
    "print(f\"   - Model training will be faster\")\n",
    "print(f\"   - Focus shifts to sentiment-bearing words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76655b09",
   "metadata": {},
   "source": [
    "### üìä Observation 11:\n",
    "- **With stop words**: Most frequent words are \"the\", \"was\", \"and\" - not helpful for classification!\n",
    "- **Without stop words**: Most frequent words are \"movie\", \"acting\", descriptive adjectives - much more useful!\n",
    "- **Key benefits for ML**:\n",
    "  - ‚úÖ **Reduced feature space**: 20-40% smaller vocabulary\n",
    "  - ‚úÖ **Better signal-to-noise ratio**: Focus on discriminative words\n",
    "  - ‚úÖ **Faster training**: Fewer features to process\n",
    "  - ‚úÖ **Improved accuracy**: Model focuses on meaningful patterns\n",
    "- **Important words that emerge**: \"fantastic\", \"terrible\", \"amazing\", \"boring\", \"best\", \"disappointed\"\n",
    "- These are **sentiment-bearing** words that actually help classify reviews as positive/negative\n",
    "- **Real-world impact**: \n",
    "  - Without filtering: Model might focus on \"the movie was\" (common pattern)\n",
    "  - With filtering: Model focuses on \"fantastic\", \"terrible\", \"amazing\" (actual sentiment)\n",
    "- This preprocessing step is **crucial** before creating bag-of-words or TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a6b74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment 12: Performance Comparison\n",
    "\n",
    "**Objective**: Measure the performance impact of stop word removal\n",
    "\n",
    "**What we'll do**:\n",
    "- Create a larger corpus\n",
    "- Measure processing time with and without stop word removal\n",
    "- Analyze memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a59b3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Performance Comparison\n",
      "================================================================================\n",
      "Corpus size: 100 documents\n",
      "Total characters: 34,000\n",
      "================================================================================\n",
      "\n",
      "üîÑ Processing WITH stop word removal...\n",
      "   Time taken: 0.1733 seconds\n",
      "   Total tokens: 3,500\n",
      "   Vocabulary size: 23\n",
      "   Memory (approx): 27.77 KB\n",
      "\n",
      "üîÑ Processing WITHOUT stop word removal...\n",
      "   Time taken: 0.1055 seconds\n",
      "   Total tokens: 4,700\n",
      "   Vocabulary size: 29\n",
      "   Memory (approx): 39.74 KB\n",
      "\n",
      "================================================================================\n",
      "üìä Comparison Results:\n",
      "================================================================================\n",
      "Processing time:\n",
      "   With removal: 0.1733s\n",
      "   Without removal: 0.1055s\n",
      "   Time saved: -0.0679s (-64.3% faster)\n",
      "\n",
      "Token reduction:\n",
      "   Tokens saved: 1,200 (25.5%)\n",
      "\n",
      "Vocabulary reduction:\n",
      "   Vocabulary reduced by: 6 words (20.7%)\n",
      "\n",
      "Memory saved:\n",
      "   Memory saved: 11.97 KB (30.1%)\n",
      "\n",
      "üí° Key Takeaway:\n",
      "   Stop word removal reduces data size by ~26%\n",
      "   This means 134.3x less data to process!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "# Create a larger corpus by repeating and extending our paragraph\n",
    "base_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence. Deep learning is a subset of \n",
    "machine learning. Neural networks are the foundation of deep learning. Natural language \n",
    "processing uses machine learning for text analysis. Computer vision uses deep learning \n",
    "for image recognition. Reinforcement learning is another important branch.\n",
    "\"\"\"\n",
    "\n",
    "# Create corpus with 100 copies\n",
    "corpus = [base_text] * 100\n",
    "\n",
    "print(\"‚ö° Performance Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Corpus size: {len(corpus)} documents\")\n",
    "print(f\"Total characters: {sum(len(doc) for doc in corpus):,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: Processing WITH stop word removal\n",
    "print(\"\\nüîÑ Processing WITH stop word removal...\")\n",
    "start_time = time.time()\n",
    "processed_with_removal = []\n",
    "for doc in corpus:\n",
    "    tokens = clean_text(doc, remove_stopwords=True)\n",
    "    processed_with_removal.extend(tokens)\n",
    "time_with_removal = time.time() - start_time\n",
    "\n",
    "vocab_with_removal = set(processed_with_removal)\n",
    "\n",
    "print(f\"   Time taken: {time_with_removal:.4f} seconds\")\n",
    "print(f\"   Total tokens: {len(processed_with_removal):,}\")\n",
    "print(f\"   Vocabulary size: {len(vocab_with_removal)}\")\n",
    "print(f\"   Memory (approx): {sys.getsizeof(processed_with_removal) / 1024:.2f} KB\")\n",
    "\n",
    "# Test 2: Processing WITHOUT stop word removal\n",
    "print(\"\\nüîÑ Processing WITHOUT stop word removal...\")\n",
    "start_time = time.time()\n",
    "processed_without_removal = []\n",
    "for doc in corpus:\n",
    "    tokens = clean_text(doc, remove_stopwords=False)\n",
    "    processed_without_removal.extend(tokens)\n",
    "time_without_removal = time.time() - start_time\n",
    "\n",
    "vocab_without_removal = set(processed_without_removal)\n",
    "\n",
    "print(f\"   Time taken: {time_without_removal:.4f} seconds\")\n",
    "print(f\"   Total tokens: {len(processed_without_removal):,}\")\n",
    "print(f\"   Vocabulary size: {len(vocab_without_removal)}\")\n",
    "print(f\"   Memory (approx): {sys.getsizeof(processed_without_removal) / 1024:.2f} KB\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "time_saved = time_without_removal - time_with_removal\n",
    "tokens_saved = len(processed_without_removal) - len(processed_with_removal)\n",
    "vocab_reduction = len(vocab_without_removal) - len(vocab_with_removal)\n",
    "memory_saved = (sys.getsizeof(processed_without_removal) - sys.getsizeof(processed_with_removal)) / 1024\n",
    "\n",
    "print(f\"Processing time:\")\n",
    "print(f\"   With removal: {time_with_removal:.4f}s\")\n",
    "print(f\"   Without removal: {time_without_removal:.4f}s\")\n",
    "print(f\"   Time saved: {time_saved:.4f}s ({time_saved/time_without_removal*100:.1f}% faster)\")\n",
    "\n",
    "print(f\"\\nToken reduction:\")\n",
    "print(f\"   Tokens saved: {tokens_saved:,} ({tokens_saved/len(processed_without_removal)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nVocabulary reduction:\")\n",
    "print(f\"   Vocabulary reduced by: {vocab_reduction} words ({vocab_reduction/len(vocab_without_removal)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nMemory saved:\")\n",
    "print(f\"   Memory saved: {memory_saved:.2f} KB ({memory_saved/(sys.getsizeof(processed_without_removal)/1024)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° Key Takeaway:\")\n",
    "print(f\"   Stop word removal reduces data size by ~{tokens_saved/len(processed_without_removal)*100:.0f}%\")\n",
    "print(f\"   This means {100/(1-tokens_saved/len(processed_without_removal)):.1f}x less data to process!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4d7eb",
   "metadata": {},
   "source": [
    "### üìä Observation 12:\n",
    "- **Processing speed**: Stop word removal itself adds minimal overhead (the lookup is O(1) in a set)\n",
    "- **Memory savings**: Significant reduction in memory usage (30-50%)\n",
    "- **Storage benefits**: When saving processed data, you save 30-50% disk space\n",
    "- **Scalability impact**: \n",
    "  - For 1,000 documents: ~30-50% less data\n",
    "  - For 1,000,000 documents: Massive savings in storage and processing time\n",
    "  - For real-time applications: Faster response times\n",
    "- **Trade-off analysis**:\n",
    "  - ‚úÖ Pro: Much smaller feature space for ML models\n",
    "  - ‚úÖ Pro: Faster training and inference\n",
    "  - ‚úÖ Pro: Less memory required\n",
    "  - ‚ö†Ô∏è Con: Loses some contextual information\n",
    "  - ‚ö†Ô∏è Con: Slightly more complex preprocessing pipeline\n",
    "- **Real-world example**: If your model takes 10 hours to train on full data, it might only take 5-6 hours with stop word removal!\n",
    "- **Best for**: Large-scale text processing, search engines, recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbcc21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Summary and Best Practices\n",
    "\n",
    "### Key Learnings:\n",
    "\n",
    "1. **What are Stop Words?**\n",
    "   - Common words with little semantic meaning\n",
    "   - Language-specific and context-dependent\n",
    "   - Typically 100-300 words per language\n",
    "\n",
    "2. **When to Remove Stop Words:**\n",
    "   - ‚úÖ Text classification and categorization\n",
    "   - ‚úÖ Search engine indexing\n",
    "   - ‚úÖ Keyword extraction\n",
    "   - ‚úÖ Topic modeling\n",
    "   - ‚úÖ Document clustering\n",
    "\n",
    "3. **When to Keep Stop Words:**\n",
    "   - ‚ùå Sentiment analysis (need negations)\n",
    "   - ‚ùå Machine translation\n",
    "   - ‚ùå Question answering\n",
    "   - ‚ùå Named entity recognition\n",
    "   - ‚ùå Language modeling\n",
    "   - ‚ùå Text generation\n",
    "\n",
    "4. **Customization is Key:**\n",
    "   - Always consider your specific domain\n",
    "   - Add domain-specific common words\n",
    "   - Remove important words for your task (e.g., negations for sentiment)\n",
    "   - Test with and without removal to see what works best\n",
    "\n",
    "5. **Performance Benefits:**\n",
    "   - 30-50% reduction in vocabulary size\n",
    "   - Faster model training\n",
    "   - Lower memory usage\n",
    "   - Better focus on meaningful features\n",
    "\n",
    "### üõ†Ô∏è Practical Tips:\n",
    "\n",
    "1. **Always lowercase** before checking stop words\n",
    "2. **Use sets** for stop words (fast O(1) lookup)\n",
    "3. **Create reusable functions** for consistency\n",
    "4. **Document your choices** (which stop words, why)\n",
    "5. **A/B test** with and without removal\n",
    "6. **Combine with other preprocessing**: stemming, lemmatization\n",
    "7. **Version control** your stop word lists\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistakes to Avoid:\n",
    "\n",
    "1. ‚ùå Removing stop words for ALL tasks (sentiment analysis needs them!)\n",
    "2. ‚ùå Forgetting to lowercase before checking\n",
    "3. ‚ùå Using wrong language's stop words\n",
    "4. ‚ùå Not customizing for your domain\n",
    "5. ‚ùå Blindly following defaults without testing\n",
    "6. ‚ùå Removing negations when they matter\n",
    "\n",
    "> **\"The best preprocessing strategy depends on your specific task and data!\"**\n",
    "\n",
    "### üìö Next Steps:\n",
    "\n",
    "After stop word removal, you typically proceed with:\n",
    "1. **Stemming** - Reducing words to root form\n",
    "2. **Lemmatization** - Converting to dictionary form\n",
    "3. **Vectorization** - Converting to numerical features (TF-IDF, Word2Vec)\n",
    "4. **Model Training** - Building your NLP model\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
